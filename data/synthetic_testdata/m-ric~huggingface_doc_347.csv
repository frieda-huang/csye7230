context,question,answer,source_doc,groundedness_score,groundedness_eval,relevance_score,relevance_eval,standalone_score,standalone_eval
"```
Client.predict() Usage Info
---------------------------
Named API endpoints: 1

 - predict(input_audio, api_name=""/predict"") -> value_0
    Parameters:
     - [Audio] input_audio: str (filepath or URL)
    Returns:
     - [Textbox] value_0: str (value)
```

This shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoint to make a prediction: we should call the `.predict()` method, providing a parameter `input_audio` of type `str`, which is a `filepath or URL`.

We should also provide the `api_name='/predict'` argument. Although this isn't necessary if a Gradio app has a single named endpoint, it does allow us to call different endpoints in a single app if they are available. If an app has unnamed API endpoints, these can also be displayed by running `.view_api(all_endpoints=True)`.

### Making a prediction

The simplest way to make a prediction is simply to call the `.predict()` function with the appropriate arguments:

```python
from gradio_client import Client

client = Client(""abidlabs/en2fr"")
client.predict(""Hello"")

>> Bonjour
```

If there are multiple parameters, then you should pass them as separate arguments to `.predict()`, like this:

```python
from gradio_client import Client

client = Client(""gradio/calculator"")
client.predict(4, ""add"", 5)

>> 9.0
```

For certain inputs, such as images, you should pass in the filepath or URL to the file. Likewise, for the corresponding output types, you will get a filepath or URL returned.

```python
from gradio_client import Client

client = Client(""abidlabs/whisper"")
client.predict(""https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3"")

>> ""My thought I have nobody by a beauty and will as you poured. Mr. Rochester is serve in that so don't find simpus, and devoted abode, to at might in a râ€”""
```

## Advanced Usage","What type of parameter is input_audio in the predict() function?
",str (filepath or URL),gradio-app/gradio/blob/main/client/python/README.md,5.0,"The context clearly states the type of input parameter `input_audio` is for the `.predict()` function, which is a string representing either a filepath or a URL. There is no ambiguity or uncertainty about this information.

",4.0,"This question appears to be about Hugging Face's Transformers library, specifically asking about the type of a parameter within one of its functions. The information required to answer this question would likely be found in the library's documentation or source code.

",5.0,"The question clearly refers to a specific context, such as an object or a method, but it does not require additional information about the setting or environment. It is clear what is being asked about without needing more details.

"
"å¦‚ä½•ä½¿ç”¨ 3D æ¨¡å‹ç»„ä»¶

ç›¸å…³ç©ºé—´ï¼šhttps://huggingface.co/spaces/dawood/Model3D, https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization, https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj
æ ‡ç­¾ï¼šVISION, IMAGE

## ä»‹ç»

æœºå™¨å­¦ä¹ ä¸­çš„ 3D æ¨¡å‹è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå¹¶ä¸”æ˜¯ä¸€äº›æœ€æœ‰è¶£çš„æ¼”ç¤ºå®éªŒã€‚ä½¿ç”¨ `gradio`ï¼Œæ‚¨å¯ä»¥è½»æ¾æ„å»ºæ‚¨çš„ 3D å›¾åƒæ¨¡å‹çš„æ¼”ç¤ºï¼Œå¹¶ä¸ä»»ä½•äººåˆ†äº«ã€‚Gradio 3D æ¨¡å‹ç»„ä»¶æ¥å— 3 ç§æ–‡ä»¶ç±»å‹ï¼ŒåŒ…æ‹¬ï¼š_.obj_ï¼Œ_.glb_ å’Œ _.gltf_ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨å‡ è¡Œä»£ç æ„å»ºæ‚¨çš„ 3D å›¾åƒæ¨¡å‹çš„æ¼”ç¤ºï¼›åƒä¸‹é¢è¿™ä¸ªç¤ºä¾‹ä¸€æ ·ã€‚ç‚¹å‡»ã€æ‹–æ‹½å’Œç¼©æ”¾æ¥ç©è½¬ 3D å¯¹è±¡ï¼š

<gradio-app space=""dawood/Model3D""> </gradio-app>

### å…ˆå†³æ¡ä»¶

ç¡®ä¿å·²ç»[å®‰è£…](https://gradio.app/quickstart)äº† `gradio` Python åŒ…ã€‚

## æŸ¥çœ‹ä»£ç 

è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•åˆ›å»ºä¸Šé¢çš„æœ€ç®€ç•Œé¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„æµ‹å‡½æ•°å°†åªè¿”å›åŸå§‹çš„ 3D æ¨¡å‹ç½‘æ ¼ï¼Œä½†æ‚¨å¯ä»¥æ›´æ”¹æ­¤å‡½æ•°ä»¥åœ¨æ‚¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸Šè¿è¡Œæ¨ç†ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹æ›´å¤æ‚çš„ç¤ºä¾‹ã€‚

```python
import gradio as gr

def load_mesh(mesh_file_name):
    return mesh_file_name

demo = gr.Interface(
    fn=load_mesh,
    inputs=gr.Model3D(),
    outputs=gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0],  label=""3D Model""),
    examples=[
        [""files/Bunny.obj""],
        [""files/Duck.glb""],
        [""files/Fox.gltf""],
        [""files/face.obj""],
    ],
    cache_examples=True,
)

demo.launch()
```

è®©æˆ‘ä»¬æ¥è§£æä¸Šé¢çš„ä»£ç ï¼š

`load_mesh`ï¼šè¿™æ˜¯æˆ‘ä»¬çš„â€œé¢„æµ‹â€å‡½æ•°ï¼Œä¸ºç®€å•èµ·è§ï¼Œè¯¥å‡½æ•°å°†æ¥æ”¶ 3D æ¨¡å‹ç½‘æ ¼å¹¶è¿”å›å®ƒã€‚

åˆ›å»ºç•Œé¢ï¼š

- `fn`ï¼šå½“ç”¨æˆ·ç‚¹å‡»æäº¤æ—¶ä½¿ç”¨çš„é¢„æµ‹å‡½æ•°ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒæ˜¯ `load_mesh` å‡½æ•°ã€‚
- `inputs`ï¼šåˆ›å»ºä¸€ä¸ª model3D è¾“å…¥ç»„ä»¶ã€‚è¾“å…¥æ˜¯ä¸€ä¸ªä¸Šä¼ çš„æ–‡ä»¶ï¼Œä½œä¸º{str}æ–‡ä»¶è·¯å¾„ã€‚
- `outputs`ï¼šåˆ›å»ºä¸€ä¸ª model3D è¾“å‡ºç»„ä»¶ã€‚è¾“å‡ºç»„ä»¶ä¹ŸæœŸæœ›ä¸€ä¸ªæ–‡ä»¶ä½œä¸º{str}æ–‡ä»¶è·¯å¾„ã€‚
  - `clear_color`ï¼šè¿™æ˜¯ 3D æ¨¡å‹ç”»å¸ƒçš„èƒŒæ™¯é¢œè‰²ã€‚æœŸæœ› RGBa å€¼ã€‚
  - `label`ï¼šå‡ºç°åœ¨ç»„ä»¶å·¦ä¸Šè§’çš„æ ‡ç­¾ã€‚
- `examples`ï¼š3D æ¨¡å‹æ–‡ä»¶çš„åˆ—è¡¨ã€‚3D æ¨¡å‹ç»„ä»¶å¯ä»¥æ¥å—*.obj*ï¼Œ*.glb*å’Œ*.gltf*æ–‡ä»¶ç±»å‹ã€‚
- `cache_examples`ï¼šä¿å­˜ç¤ºä¾‹çš„é¢„æµ‹è¾“å‡ºï¼Œä»¥èŠ‚çœæ¨ç†æ—¶é—´ã€‚

## æ¢ç´¢æ›´å¤æ‚çš„ Model3D æ¼”ç¤º

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ DPT æ¨¡å‹é¢„æµ‹å›¾åƒæ·±åº¦ï¼Œç„¶åä½¿ç”¨ 3D ç‚¹äº‘åˆ›å»º 3D å¯¹è±¡çš„æ¼”ç¤ºã€‚æŸ¥çœ‹[code.py](https://huggingface.co/spaces/radames/dpt-depth-estimation-3d-obj/blob/main/app.py)æ–‡ä»¶ï¼Œäº†è§£ä»£ç å’Œæ¨¡å‹é¢„æµ‹å‡½æ•°ã€‚
<gradio-app space=""radames/dpt-depth-estimation-3d-obj""> </gradio-app>

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ PIFu æ¨¡å‹å°†ç©¿ç€è¡£ç‰©çš„äººçš„å›¾åƒè½¬æ¢ä¸º 3D æ•°å­—åŒ–æ¨¡å‹çš„æ¼”ç¤ºã€‚æŸ¥çœ‹[spaces.py](https://huggingface.co/spaces/radames/PIFu-Clothed-Human-Digitization/blob/main/PIFu/spaces.py)æ–‡ä»¶ï¼Œäº†è§£ä»£ç å’Œæ¨¡å‹é¢„æµ‹å‡½æ•°ã€‚

<gradio-app space=""radames/PIFu-Clothed-Human-Digitization""> </gradio-app>

---

æå®šï¼è¿™å°±æ˜¯æ„å»º Model3D æ¨¡å‹ç•Œé¢æ‰€éœ€çš„æ‰€æœ‰ä»£ç ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯èƒ½ä¼šå‘ç°æœ‰ç”¨çš„å‚è€ƒèµ„æ–™ï¼š","ä»€ä¹ˆç±»å‹çš„æ–‡ä»¶å¯ä»¥è¢« gradio çš„ 3D æ¨¡å‹ç»„ä»¶æ¥å—?
",".obj, .glb å’Œ .gltf.",gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/how-to-use-3D-model-component.md,5.0,"æ ¹æ®ç»™å‡ºçš„æ–‡æœ¬ï¼Œé—®é¢˜â€œä»€ä¹ˆç±»å‹çš„æ–‡ä»¶å¯ä»¥è¢« Gradio çš„ 3D æ¨¡å‹ç»„ä»¶æ¥å—ï¼Ÿâ€æ¸…æ™°åœ°è¢«å›ç­”ä¸º *.obj_ã€*.glb_ å’Œ *.gltf_ æ–‡ä»¶ç±»å‹ã€‚æ–‡æœ¬æä¾›äº†è¯¦ç»†çš„ä»£ç ç¤ºä¾‹å’Œç›¸å…³ä¿¡æ¯ï¼Œä½¿å¾—è¿™ä¸ªé—®é¢˜å¾—åˆ°æ˜ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚

",5.0,"è¿™ä¸ªé—®é¢˜å¾ˆå…·ä½“ï¼Œæ¶‰åŠåˆ° Gradio ä¸­çš„ 3D æ¨¡å‹ç»„ä»¶ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…äº†è§£è¯¥ç»„ä»¶çš„ä½¿ç”¨é™åˆ¶å’Œé€‚ç”¨èŒƒå›´ã€‚é€šè¿‡å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¼€å‘è€…å¯ä»¥æ›´å¥½åœ°ç†è§£åœ¨ NLP åº”ç”¨ä¸­å¦‚ä½•å°†å…¶åº”ç”¨äº 3D æ•°æ®ï¼Œè¿™å¯¹æŸäº›ç‰¹å®šé¢†åŸŸçš„æœºå™¨å­¦ä¹ å¼€å‘æ¥è¯´æ˜¯éå¸¸æœ‰ä»·å€¼çš„ã€‚
",5.0,"This question appears to be clear and self-contained, asking about the type of files that can be accepted by Gradio's 3D model component. The mention of ""Gradio"" and its specific feature (""3D æ¨¡å‹ç»„ä»¶"") implies that the reader is familiar with the platform, but the question itself does not rely on any external context beyond this basic understanding.

"
"1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.
1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier HÃ©naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, JoÃ£o Carreira.","Who released the PEGASUS model?
",Google,huggingface/transformers/blob/main/README_es.md,5.0,"The context lists multiple models released by various organizations, but it clearly states that the PEGASUS model was released by Google with a specific paper.
",4.0,"This question is useful because it shows that the person asking has knowledge of a specific NLP model (PEGASUS) and wants to know more about its origin. Knowing who developed a particular model can be important for understanding its strengths, weaknesses, and potential applications.
",5.0,"The question is clear and unambiguous, and it does not require any additional context to understand what it's asking. It mentions a specific model (PEGASUS), which suggests that it refers to a particular object of interest in the field of natural language processing.

"
"assistant_model.to(device);
```

------------------------------------------------------------------------

\\({}^1\\) We intend to release an improved variant of Distil-Whisper with a stronger alignment in the token distribution
that will improve speculative decoding performance further. Follow the [Distil-Whisper repository](https://github.com/huggingface/distil-whisper)
for updates.

------------------------------------------------------------------------

We can define a modified function for our speculative decoding benchmark. The only difference from the previous function
is that we pass the assistant model to our call to `.generate`:

```python
def assisted_generate_with_time(model, inputs, **kwargs):
    start_time = time.time()
    outputs = model.generate(**inputs, assistant_model=assistant_model, **kwargs)
    generation_time = time.time() - start_time
    return outputs, generation_time
```

Let's run the benchmark with speculative decoding, using Distil-Whisper as the assistant to Whisper:

```python
all_time = 0
predictions = []
references = []

for sample in tqdm(dataset):
    audio = sample[""audio""]
    inputs = processor(audio[""array""], sampling_rate=audio[""sampling_rate""], return_tensors=""pt"")
    inputs = inputs.to(device=device, dtype=torch.float16)

    output, gen_time = assisted_generate_with_time(model, inputs)
    all_time += gen_time
    predictions.append(processor.batch_decode(output, skip_special_tokens=True, normalize=True)[0])
    references.append(processor.tokenizer._normalize(sample[""text""]))

print(all_time)
```

**Outputs:**
```
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:38<00:00,  1.88it/s]
32.69683289527893
```

With speculative decoding, the inference time was just 33 seconds, 2.2x faster than before! Let's verify we have the same
WER:

```python
print(wer.compute(predictions=predictions, references=references))
```
**Outputs:**
```
0.03507271171941831
```","What is the line of code that passes the assistant model to a call to `.generate`?
",assistant_model.to(device);,huggingface/blog/blob/main/whisper-speculative-decoding.md,4.0,"The context provides a code snippet that defines an `assisted_generate_with_time` function, which passes the `assistant_model` to the `.generate` method of another model. However, the specific line of code that passes the assistant model is not explicitly shown in this context, as it is part of the implementation within the `model.generate()` call. Nevertheless, based on the provided information and a general understanding of the code structure, one can infer that the relevant line of code would be something like `outputs = model.generate(**inputs, assistant_model=assistant_model, **kwargs)`. This inference assumes knowledge of the standard `generate` method usage in Hugging Face's Transformers library.

",4.0,"This question seems relevant to Hugging Face's Transformers library, which is commonly used for NLP tasks. However, it's a very specific and narrow question about how to pass an assistant model to the `.generate()` method.
",5.0,"The question asks for a specific line of code related to passing an assistant model to the `.generate` method, which suggests it's looking for a general solution or implementation detail. However, it doesn't specify any context or assumptions about the surrounding environment, such as the programming language, framework, or previous steps taken.

"
"While movement pruning does not directly optimize for memory footprint (but rather the number of non-null weights), we hypothetize that further memory compression ratios can be achieved with specific quantization aware trainings (see for instance [Q8BERT](https://arxiv.org/abs/1910.06188), [And the Bit Goes Down](https://arxiv.org/abs/1907.05686) or [Quant-Noise](https://arxiv.org/abs/2004.07320)).

## Fine-pruned models

As examples, we release two English PruneBERT checkpoints (models fine-pruned from a pre-trained `BERT` checkpoint), one on SQuAD and the other on MNLI.

- **`prunebert-base-uncased-6-finepruned-w-distil-squad`**<br/>
Pre-trained `BERT-base-uncased` fine-pruned with soft movement pruning on SQuAD v1.1. We use an additional distillation signal from `BERT-base-uncased` finetuned on SQuAD. The encoder counts 6% of total non-null weights and reaches 83.8 F1 score. The model can be accessed with: `pruned_bert = BertForQuestionAnswering.from_pretrained(""huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad"")`
- **`prunebert-base-uncased-6-finepruned-w-distil-mnli`**<br/>
Pre-trained `BERT-base-uncased` fine-pruned with soft movement pruning on MNLI. We use an additional distillation signal from `BERT-base-uncased` finetuned on MNLI. The encoder counts 6% of total non-null weights and reaches 80.7 (matched) accuracy. The model can be accessed with: `pruned_bert = BertForSequenceClassification.from_pretrained(""huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli"")`

## How to fine-prune?

### Setup

The code relies on the ğŸ¤— Transformers library. In addition to the dependencies listed in the [`examples`](https://github.com/huggingface/transformers/tree/main/examples) folder, you should install a few additional dependencies listed in the `requirements.txt` file: `pip install -r requirements.txt`.","What percentage of total non-null weights does the encoder in ""prunebert-base-uncased-6-finepruned-w-distil-squad"" count?
",6%,huggingface/transformers/blob/main/examples/research_projects/movement-pruning/README.md,5.0,"The context provides specific information about two fine-pruned models, including ""prunebert-base-uncased-6-finepruned-w-distil-squad"". It mentions that the encoder of this model counts 6% of total non-null weights. This direct statement allows for an unambiguous answer to the question.

",5.0,"This question appears to be specific and relevant to a particular model (""prunebert-base-uncased-6-finepruned-w-distil-squad"") within the Hugging Face ecosystem. The question is asking for a precise metric (percentage of total non-null weights) that can be used to evaluate or fine-tune the performance of this encoder in NLP applications, particularly those utilizing the Hugging Face library.

",4.0,"The question can be understood without any additional information or context, as it refers to a specific model variant and clearly asks for a percentage value related to its weights. However, the actual answer would depend on accessing documentation or knowledge about the model's architecture and pruning strategy.

"
"1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.
1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.
1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.
1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.
1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.","Who are the authors of the paper ""Big Bird: Transformers for Longer Sequences""?
","Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.",huggingface/transformers/blob/main/README_pt-br.md,5.0,"The question can be answered by searching for the specific paper mentioned in the BigBird-Pegasus and BigBird-RoBERTa model documentation.
",4.0,"This question appears to be about identifying authors of a specific research paper related to the Hugging Face ecosystem, which suggests that it may be relevant to machine learning developers working with NLP applications.
",5.0,"This question does not depend on any additional information because it directly refers to a specific paper with a clear title, making its meaning explicit and unambiguous.
"
"FrameworkSwitchCourse {fw} />

<!-- DISABLE-FRONTMATTER-SECTIONS -->

# End-of-chapter quiz[[end-of-chapter-quiz]]

<CourseFloatingBanner
    chapter={7}
    classNames=""absolute z-10 right-0 top-0""
/>

Let's test what you learned in this chapter!

### 1. Which of the following tasks can be framed as a token classification problem?

<Question
	choices={[
		{
			text: ""Find the grammatical components in a sentence."",
			explain: ""Correct! We can then label each word as a noun, verb, etc."",
			correct: true
		},
		{
			text: ""Find whether a sentence is grammatically correct or not."",
			explain: ""No, this is a sequence classification problem.""
		},
		{
			text: ""Find the persons mentioned in a sentence."",
			explain: ""Correct! We can label each word as person or not person."",
            correct: true
		},
        {
			text: ""Find the chunk of words in a sentence that answers a question."",
			explain: ""No, that would be a question answering problem.""
		}
	]}
/>

### 2. What part of the preprocessing for token classification differs from the other preprocessing pipelines?","In what type of problem can tasks such as finding grammatical components in a sentence or persons mentioned in a sentence be framed?
",Token classification problem.,huggingface/course/blob/main/chapters/en/chapter7/9.mdx,5.0,"This question is answerable because it is closely related to the provided context, which discusses tasks such as finding grammatical components in a sentence or persons mentioned in a sentence being framed as token classification problems. The choices given in the quiz also align with this interpretation.

",4.0,"This question appears to be related to NLP, specifically asking about the type of problem that involves identifying grammatical components and entities like people in sentences. The Hugging Face ecosystem is widely used for building NLP applications, so this question could potentially be relevant.

",5.0,"This question is clear and concise, asking about the general type of problem where specific tasks like parsing sentences for grammatical components or entities (such as people) within them can be categorized. It does not require any context beyond basic knowledge of linguistic tasks.
"
"<p><img src=""https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=700&amp;name=chest%20x-ray%20examples.png"" alt=""chest x-ray examples"" loading=""lazy"" style=""width: 700px; margin-left: auto; margin-right: auto; display: block;"" width=""700"" srcset=""https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=350&amp;name=chest%20x-ray%20examples.png 350w, https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=700&amp;name=chest%20x-ray%20examples.png 700w, https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=1050&amp;name=chest%20x-ray%20examples.png 1050w, https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=1400&amp;name=chest%20x-ray%20examples.png 1400w, https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=1750&amp;name=chest%20x-ray%20examples.png 1750w, https://www.graphcore.ai/hs-fs/hubfs/chest%20x-ray%20examples.png?width=2100&amp;name=chest%20x-ray%20examples.png 2100w"" sizes=""(max-width: 700px) 100vw, 700px""></p>
<div class=""blog-caption"" style=""max-height: 100%; max-width: 90%; margin-left: auto; margin-right: auto; line-height: 1.4;"">
<p>Eight visual examples of common thorax diseases (Credit: NIC)</p>
</div>
<h2>Setting up the environment</h2>
<p>Here are the requirements to run this walkthrough:</p>
<ul>
<li>A Jupyter Notebook server with the latest Poplar SDK and PopTorch environment enabled (see our <a href=""https://github.com/graphcore/tutorials/blob/master/tutorials/standard_tools/using_jupyter/README.md"" rel=""noopener"" target=""_blank"">guide on using IPUs from Jupyter notebooks</a>)</li>
<li>The ViT Training Notebook from the <a href=""https://github.com/graphcore/tutorials"" rel=""noopener"" target=""_blank"">Graphcore Tutorials repo</a></li>
</ul>","What are the requirements to run this walkthrough?
","A Jupyter Notebook server with the latest Poplar SDK and PopTorch environment enabled, and the ViT Training Notebook from the Graphcore Tutorials repo.",huggingface/blog/blob/main/vision-transformers.md,5.0,"The context provides explicit information about what is required to run this walkthrough, including a list of specific requirements such as having a Jupyter Notebook server with the latest Poplar SDK and PopTorch environment enabled, as well as accessing a ViT Training Notebook from the Graphcore Tutorials repo. This clarity in the context makes it easy to unambiguously answer the question about what are the requirements to run this walkthrough.

",4.0,"This question is useful because it indicates that the user wants to execute a specific walkthrough, which implies they want to use the Hugging Face ecosystem for NLP tasks. The answer would likely provide essential setup instructions or prerequisites necessary to complete the walkthrough successfully.
",5.0,"The question makes sense by itself without any additional context, as it asks for a general requirement that can be understood from the term ""walkthrough"" alone.
"
"The abstract from the paper is the following:

*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales
quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention
mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or
longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local
windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we
evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In
contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our
pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on
WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting
long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization
dataset.*

## Usage tips","What is the name of the proposed model to address the limitation of transformer-based models in processing long sequences?
",Longformer.,huggingface/transformers/blob/main/docs/source/en/model_doc/led.md,5.0,"The context clearly states that the proposed model to address the limitation of transformer-based models in processing long sequences is called ""Longformer"". This is explicitly mentioned in the first sentence, which makes it easy to determine the correct answer. Additionally, the context provides a detailed explanation of how Longformer works and its advantages over other models.
",4.0,"The question seems to be related to a specific area of NLP research, but it lacks specificity and context. However, it appears to be asking about a potential solution to a known problem in transformer-based models, which could be relevant to machine learning developers working with the Hugging Face ecosystem.
",5.0,"The question clearly refers to a specific problem (transformer-based models' limitations in processing long sequences) and asks for the name of a proposed solution, which implies that this proposed model is well-known within the context of natural language processing. Therefore, it seems reasonable to assume that someone familiar with the field would be able to understand what is being asked without additional information.

"
"|      |      |[aipicasso/cool-japan-diffusion-2-1-2-beta](https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-2-beta)                                      |701         |2       |                         |                                                                                   |[MODEL-LICENSE](https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-2-beta/blob/main/MODEL-LICENSE)               |                                                                                                    |             |
|      |      |[4bit/Llama-2-7b-Chat-GPTQ](https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ)                                                                      |668         |5       | llama2 |                                                 |[LICENSE](https://huggingface.co/4bit/Llama-2-7b-Chat-GPTQ/blob/main/LICENSE)                                           |                                                                                                    |             |
|      |      |[4bit/Llama-2-70b-chat-hf](https://huggingface.co/4bit/Llama-2-70b-chat-hf)                                                                        |661         |10      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/4bit/Llama-2-70b-chat-hf/blob/main/LICENSE.txt)                                    |                                                                                                    |             |","How many parameters does the model Llama-2-7b-Chat-GPTQ have?
",668 million,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,5.0,"The context provides information about the model Llama-2-7b-Chat-GPTQ, specifically its Hugging Face repository ID and the number of parameters it has. However, there is no explicit mention of the model's architecture or any additional parameters that might be required to answer the question unambiguously.

",4.0,"This question is useful because it allows me to identify the specific model variant being referred to (Llama-2-7b-Chat-GPTQ) and determine its parameter count, which can be an important metric for understanding a model's capacity and potential applications. However, the answer might depend on up-to-date information about the Llama-2 models, as the parameter counts of these models may not be widely published.

",5.0,"This question can be understood without any additional context, as it asks a specific attribute of a well-defined entity (the Llama-2-7b-Chat-GPTQ model). The operator needs to know what the model is, but not in which context or setting it is used.

"
"- [#6709](https://github.com/gradio-app/gradio/pull/6709) [`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88) - Remove progress animation on streaming.  Thanks [@aliabid94](https://github.com/aliabid94)!
- [#6660](https://github.com/gradio-app/gradio/pull/6660) [`5238053`](https://github.com/gradio-app/gradio/commit/523805360bbf292d9d82443b1f521528beba68bb) - Fix reload mode warning about not being able to find the app.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!
- [#6672](https://github.com/gradio-app/gradio/pull/6672) [`1234c37`](https://github.com/gradio-app/gradio/commit/1234c3732b52327a00b917af2ef75821771e2c92) - use gr.Error for audio length errors.  Thanks [@abidlabs](https://github.com/abidlabs)!
- [#6676](https://github.com/gradio-app/gradio/pull/6676) [`fe40308`](https://github.com/gradio-app/gradio/commit/fe40308894efb2c6ff18e5e328163f6641b7476c) - Rotate Images to Upright Position in preprocess.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!
- [#6487](https://github.com/gradio-app/gradio/pull/6487) [`9a5811d`](https://github.com/gradio-app/gradio/commit/9a5811df9218b622af59ba243a937a9c36ba00f9) - Fix the download button of the `gr.Gallery()` component to work.  Thanks [@whitphx](https://github.com/whitphx)!
- [#6689](https://github.com/gradio-app/gradio/pull/6689) [`c9673ca`](https://github.com/gradio-app/gradio/commit/c9673cacd6470296ee01d7717e2080986e750572) - Fix directory-only glob for FileExplorer.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!
- [#6639](https://github.com/gradio-app/gradio/pull/6639) [`9a6ff70`](https://github.com/gradio-app/gradio/commit/9a6ff704cd8429289c5376d3af5e4b8492df4773) - Fix issue with `head` param when adding more than one script tag.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!","What was the commit hash for the pull request that removed progress animation on streaming?
",6a9151d5c9432c724098da7d88a539aaaf5ffe88,gradio-app/gradio/blob/main/gradio/CHANGELOG.md,5.0,"The context provides a list of pull requests, each with its corresponding commit hash and description. To answer the question ""What was the commit hash for the pull request that removed progress animation on streaming?"", one would need to scan through the list of pull requests and find the one described as ""Remove progress animation on streaming"". Since this description is unique among all pull requests in the context, it should be possible to identify the correct pull request and retrieve its associated commit hash.

",4.0,"This question appears to be specific to the history of a particular commit or change within the Hugging Face ecosystem, likely referring to a specific issue or feature related to the Hugging Face models or transformers. The question is focused on a very specific event or technical detail (the removal of progress animation for streaming) and asks for information that might be valuable for someone investigating changes in the codebase over time.

",4.0,"The question seems to refer to a specific event or change in a codebase, but it's clear what is being asked without needing additional context. The mention of ""pull request"" and ""commit hash"" implies a development setting, but the question itself doesn't require knowledge about the project or its history beyond this.

"
"+ optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
+         optimizer, train_dataloader, eval_dataloader, lr_scheduler
+        )
```

- In case of a single model, if you have created optimizer with multiple parameter groups and called prepare with them together, then the parameter groups will be lost and the following warning is displayed:

    > FSDP Warning: When using FSDP, several parameter groups will be conflated into a single one due to nested module wrapping and parameter flattening.
    >

This is because parameter groups created before wrapping will have no meaning post wrapping due parameter flattening of nested FSDP modules into 1D arrays (which can consume many layers). For instance, below are the named parameters of FSDP model on GPU 0 (When using 2 GPUs. Around 55M (110M/2) params in 1D arrays as this will have the 1st shard of the parameters). Here, if one has applied no weight decay for [bias, LayerNorm.weight] named parameters of unwrapped BERT-Base model, it canâ€™t be applied to the below FSDP wrapped model as there are no named parameters with either of those strings and the parameters of those layers are concatenated with parameters of various other layers. More details mentioned in this [issue](https://github.com/pytorch/pytorch/issues/76501) (`The original model parameters' .grads are not set, meaning that they cannot be optimized separately (which is why we cannot support multiple parameter groups)`).

    ```
    {
    '_fsdp_wrapped_module.flat_param': torch.Size([494209]),

    '_fsdp_wrapped_module._fpw_module.bert.embeddings.word_embeddings._fsdp_wrapped_module.flat_param': torch.Size([11720448]),

    '_fsdp_wrapped_module._fpw_module.bert.encoder._fsdp_wrapped_module.flat_param': torch.Size([42527232])
    }
    ```
- In case of multiple models, it is necessary to prepare the models before creating optimizers else it will throw an error.","What happens when you pass multiple parameter groups to optimizer with FSDP enabled?
",The parameter groups will be conflated into a single one due to nested module wrapping and parameter flattening.,huggingface/blog/blob/main/pytorch-fsdp.md,5.0,"When FSDP is enabled and you pass multiple parameter groups to the optimizer, they will be conflated into a single one due to nested module wrapping and parameter flattening. This is because parameter groups created before wrapping have no meaning post-wrapping. The parameters are concatenated in a 1D array, losing their original structure.

",4.0,"This question appears to be relevant to a specific edge case in using the Hugging Face ecosystem, particularly with regards to model parallelism and distributed optimization. It suggests that the user is aware of FSDP (Fully Sharded Data Parallel) and is interested in understanding how it interacts with optimizer parameters. However, without more context or details about what exactly happens when passing multiple parameter groups to an optimizer with FSDP enabled, this question might be somewhat open-ended.

",4.0,"The question refers to a specific scenario involving passing multiple parameter groups to an optimizer, which is likely related to Federated Simulations Distributed Parallelism (FSDP) and the process of optimizing model parameters. However, it doesn't require any external context or knowledge beyond understanding FSDP and its implications on optimization processes.

"
"6.  Change the PR into a draft by clicking on ""Convert to draft"" on the
    right of the GitHub pull request web page.

In the following, whenever you have done some progress, don't forget to
commit your work and push it to your account so that it shows in the
pull request. Additionally, you should make sure to update your work
with the current main from time to time by doing:

    git fetch upstream
    git merge upstream/main

In general, all questions you might have regarding the model or your
implementation should be asked in your PR and discussed/solved in the
PR. This way, Patrick will always be notified when you are
committing new code or if you have a question. It is often very helpful
to point Patrick to your added code so that the Hugging
Face team can efficiently understand your problem or question.

To do so, you can go to the ""Files changed"" tab where you see all of
your changes, go to a line regarding which you want to ask a question,
and click on the ""+"" symbol to add a comment. Whenever a question or
problem has been solved, you can click on the ""Resolve"" button of the
created comment.

In the same way, Patrick will open comments when reviewing
your code. We recommend asking most questions on GitHub on your PR. For
some very general questions that are not very useful for the public,
feel free to ping Patrick by Slack or email.

**5. Adapt the generated models code for BigBird**

At first, we will focus only on the model itself and not care about the
tokenizer. All the relevant code should be found in the generated files
`src/transformers/models/big_bird/modeling_big_bird.py` and
`src/transformers/models/big_bird/configuration_big_bird.py`.","Where should questions regarding the model or implementation be asked?
",In your PR.,huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md,5.0,"The context clearly states that all questions regarding the model or implementation should be asked in the pull request (PR) on GitHub, with a specific emphasis on using the ""Files changed"" tab to add comments and the ""Resolve"" button when issues are resolved. This provides a clear direction on where to ask questions related to the model or implementation.
",4.0,"This question appears to be seeking information about where to ask specific types of questions within the Hugging Face ecosystem, which is likely related to community support and resources. It's a useful question for developers who are unsure about where to direct their inquiries.
",4.0,"The question does not contain any specific context or references that would limit its understanding, but it still implies a setting (e.g., a project or a community) where questions about a model or implementation are being asked.
"
"<!--
Type: model-index
Collections:
- Name: ECAResNet
  Paper:
    Title: 'ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks'
    URL: https://paperswithcode.com/paper/eca-net-efficient-channel-attention-for-deep
Models:
- Name: ecaresnet101d
  In Collection: ECAResNet
  Metadata:
    FLOPs: 10377193728
    Parameters: 44570000
    File Size: 178815067
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Efficient Channel Attention
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    - Squeeze-and-Excitation Block
    Tasks:
    - Image Classification
    Training Techniques:
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    Training Resources: 4x RTX 2080Ti GPUs
    ID: ecaresnet101d
    LR: 0.1
    Epochs: 100
    Layers: 101
    Crop Pct: '0.875'
    Batch Size: 256
    Image Size: '224'
    Weight Decay: 0.0001
    Interpolation: bicubic
  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1087
  Weights: https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45402/outputs/ECAResNet101D_281c5844.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 82.18%
      Top 5 Accuracy: 96.06%
- Name: ecaresnet101d_pruned
  In Collection: ECAResNet
  Metadata:
    FLOPs: 4463972081
    Parameters: 24880000
    File Size: 99852736
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Efficient Channel Attention
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    - Squeeze-and-Excitation Block
    Tasks:
    - Image Classification
    Training Techniques:
    - SGD with Momentum
    - Weight Decay","How many parameters does the ecaresnet101d model have?
",44570000,huggingface/pytorch-image-models/blob/main/docs/models/ecaresnet.md,5.0,"The context provides a clear and unambiguous answer to the question by directly stating the number of parameters for the ecaresnet101d model, which is ""44570000"". This information is explicitly mentioned in the Metadata section of the model's description.
",4.0,"This question is useful because it requires specific information about a particular Hugging Face model, which suggests that the person asking the question is preparing to use this model in their NLP application. Knowing the number of parameters can be essential for determining the computational resources and memory required for training or fine-tuning the model.
",5.0,"The question clearly specifies the model name ""eca-resnet-101-d"" and asks for a specific piece of information (""how many parameters"") that can be found in the model's documentation. This makes it understandable without additional context.
"
"--
title: ""Fine-Tune XLSR-Wav2Vec2 for low-resource ASR with ğŸ¤— Transformers""
thumbnail: /blog/assets/xlsr_wav2vec2.png
authors:
- user: patrickvonplaten
---

# Fine-tuning XLS-R for Multi-Lingual ASR with ğŸ¤— Transformers


<a target=""_blank"" href=""https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_XLS_R_on_Common_Voice.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>


***New (11/2021)***: *This blog post has been updated to feature XLSR\'s
successor, called [XLS-R](https://huggingface.co/models?other=xls_r)*.

**Wav2Vec2** is a pretrained model for Automatic Speech Recognition
(ASR) and was released in [September
2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)
by *Alexei Baevski, Michael Auli, and Alex Conneau*. Soon after the
superior performance of Wav2Vec2 was demonstrated on one of the most
popular English datasets for ASR, called
[LibriSpeech](https://huggingface.co/datasets/librispeech_asr),
*Facebook AI* presented a multi-lingual version of Wav2Vec2, called
[XLSR](https://arxiv.org/abs/2006.13979). XLSR stands for *cross-lingual
speech representations* and refers to model\'s ability to learn speech
representations that are useful across multiple languages.

XLSR\'s successor, simply called **XLS-R** (refering to the
[*\'\'XLM-R*](https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/)
*for Speech\'\'*), was released in [November 2021](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages) by *Arun
Babu, Changhan Wang, Andros Tjandra, et al.* XLS-R used almost **half a
million** hours of audio data in 128 languages for self-supervised
pre-training and comes in sizes ranging from 300 milion up to **two
billion** parameters. You can find the pretrained checkpoints on the ğŸ¤—
Hub:","When was Wav2Vec2 released?
",September 2020,huggingface/blog/blob/main/fine-tune-xlsr-wav2vec2.md,4.0,"The question is answerable because the context mentions the release of Wav2Vec2, providing a specific date (September 2020). However, there's no direct answer to the question in the given snippet. We would need to look up or read more to confirm that the model released in September 2020 was indeed Wav2Vec2.

",4.0,"This question is useful because it provides a specific piece of information about the release date of a widely used NLP model, Wav2Vec2. Knowing when a model was released can be helpful for understanding its development history and timeline.

",5.0,"The question clearly asks about a specific event (the release of Wav2Vec2) without referencing any context or external information, making it easily understandable with general knowledge.
"
"## What is Mixtral 8x7b?

Mixtral has a similar architecture to Mistral 7B, but comes with a twist: itâ€™s actually 8 â€œexpertâ€ models in one, thanks to a technique called Mixture of Experts (MoE). For transformers models, the way this works is by replacing some Feed-Forward layers with a sparse MoE layer. A MoE layer contains a router network to select which experts process which tokens most efficiently. In the case of Mixtral, two experts are selected for each timestep, which allows the model to decode at the speed of a 12B parameter-dense model, despite containing 4x the number of effective parameters!

For more details on MoEs, see our accompanying blog post: [hf.co/blog/moe](https://huggingface.co/blog/moe)

**Mixtral release TL;DR;**

- Release of base and Instruct versions
- Supports a context length of 32k tokens.
- Outperforms Llama 2 70B and matches or beats GPT3.5 on most benchmarks
- Speaks English, French, German, Spanish, and Italian.
- Good at coding, with 40.2% on HumanEval
- Commercially permissive with an Apache 2.0 license

So how good are the Mixtral models? Hereâ€™s an overview of the base model and its performance compared to other open models on the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) (higher scores are better):","What is the maximum context length supported by Mixtral?
",32k tokens.,huggingface/blog/blob/main/mixtral.md,4.0,"The context provides information about Mixtral's architecture, release details, and performance metrics, but it does not explicitly mention the maximum context length supported by Mixtral. However, it does provide a specific value for the context length in one of its bullet points (""Supports a context length of 32k tokens.""). This makes it likely that this is indeed the answer to the question.

",4.0,"This question seems to be quite specific to the Mixtral model, which is a part of the Hugging Face ecosystem. The question about the maximum context length supported by Mixtral can be useful for developers who are planning to use this model in their NLP applications and need to know its limitations. However, it might not be broadly applicable or relevant to most machine learning developers working with NLP and Hugging Face.
",5.0,"The question is clear and concise, asking about a specific attribute of a particular entity (Mixtral). It does not rely on any additional information or context to be understood.
"
"|      |      |[TheBloke/LLaMA-65B-GPTQ](https://huggingface.co/TheBloke/LLaMA-65B-GPTQ)                                                                          |140         |4       |llama-license            |https://huggingface.co/TheBloke/LLaMA-65B-GPTQ/blob/main/LICENSE                   |[LICENSE](https://huggingface.co/TheBloke/LLaMA-65B-GPTQ/blob/main/LICENSE)                                             |                                                                                                    |             |
|      |      |[sschet/bert-base-uncased_clinical-ner](https://huggingface.co/sschet/bert-base-uncased_clinical-ner)                                              |130         |1       |                         |                                                                                   |[LICENSE](https://huggingface.co/sschet/bert-base-uncased_clinical-ner/blob/main/LICENSE)                               |                                                                                                    |             |
|      |      |[TheBloke/MythoMax-L2-13B-GGUF](https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF)                                                              |126         |10      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/blob/main/LICENSE.txt)                               |                                                                                                    |             |","What is the license associated with the model TheBloke/LLaMA-65B-GPTQ?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,5.0,"The context explicitly mentions the license associated with TheBloke/LLaMA-65B-GPTQ as ""llama-license"", and provides a link to the LICENSE file, which suggests that the license is publicly available. Therefore, it should be possible to determine the specific terms and conditions of this license by accessing the provided link.

",5.0,"This question appears to be about obtaining licensing information for a specific Hugging Face model, which is crucial for compliance and usage purposes. It shows an understanding of the importance of knowing the underlying license of a model when working with it in production.
",5.0,"This question is clear and specific, asking about a particular model (TheBloke/LLaMA-65B-GPTQ) and its associated license. There are no ambiguous references or implied contexts that require additional information to understand the question.

"
"```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between `are` and `you` to account for that.

Since we're using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:

```py
tokenizer = AutoTokenizer.from_pretrained(""gpt2"")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(""Hello, how are  you?"")
```

it will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a `Ä ` symbol, enabling it to recover the original spaces if we decode the tokens:

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('Ä how', (6, 10)), ('Ä are', (10, 14)), ('Ä ', (14, 15)), ('Ä you', (15, 19)),
 ('?', (19, 20))]
```

Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.

For a last example, let's have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:

```py
tokenizer = AutoTokenizer.from_pretrained(""t5-small"")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(""Hello, how are  you?"")
```

```python out
[('â–Hello,', (0, 6)), ('â–how', (7, 10)), ('â–are', (11, 14)), ('â–you?', (16, 20))]
```

Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before `Hello`) and ignored the double space between `are` and `you`.","What is the GPT-2 tokenizer's special symbol for spaces?
",Ä ,huggingface/course/blob/main/chapters/en/chapter6/4.mdx,4.0,"The context provides an example of how the GPT-2 tokenizer handles spaces, where it replaces them with the special symbol 'Ä '. However, there is no explicit mention of what this symbol represents. Nevertheless, based on the information provided in the context, one can infer that the question is asking about a specific character used by the GPT-2 tokenizer to represent spaces.

",4.0,"This question is useful because it requires specific knowledge about a particular tokenizer implementation within the Hugging Face ecosystem. The answer will likely be informative and relevant to developers who work with NLP and the GPT-2 model.
",5.0,"The question clearly specifies what is being asked, namely the special symbol used by the GPT-2 tokenizer to represent spaces. It does not refer to a specific context or setting, making it easily understandable without additional information.
"
"<!--
Type: model-index
Collections:
- Name: SE ResNet
  Paper:
    Title: Squeeze-and-Excitation Networks
    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks
Models:
- Name: seresnet152d
  In Collection: SE ResNet
  Metadata:
    FLOPs: 20161904304
    Parameters: 66840000
    File Size: 268144497
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    - Squeeze-and-Excitation Block
    Tasks:
    - Image Classification
    Training Techniques:
    - Label Smoothing
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    Training Resources: 8x NVIDIA Titan X GPUs
    ID: seresnet152d
    LR: 0.6
    Epochs: 100
    Layers: 152
    Dropout: 0.2
    Crop Pct: '0.94'
    Momentum: 0.9
    Batch Size: 1024
    Image Size: '256'
    Interpolation: bicubic
  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 83.74%
      Top 5 Accuracy: 96.77%
- Name: seresnet50
  In Collection: SE ResNet
  Metadata:
    FLOPs: 5285062320
    Parameters: 28090000
    File Size: 112621903
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    - Squeeze-and-Excitation Block
    Tasks:
    - Image Classification
    Training Techniques:
    - Label Smoothing
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    Training Resources: 8x NVIDIA Titan X GPUs","What is the FLOPs count of the seresnet152d model?
",20161904304,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx,5.0,"The context clearly lists the FLOPs count for the seresnet152d model under its metadata, making it easy to determine that the FLOPs count is approximately 20.161904304 billion.
",4.0,"This question appears to be related to the computational resources required by a specific NLP model. The FLOPs (Floating-Point Operations per Second) count is an important metric for evaluating the performance and efficiency of neural network models, especially when it comes to large-scale deployment in production environments.

To answer this question accurately, one would need to have knowledge of the model's architecture, specifically the Squeeze-and-Excitation (SE) block, which is a key component of the SqueezeNet family. Additionally, familiarity with tools or libraries that can estimate FLOPs counts for given models would be beneficial.

Given the context of the Hugging Face ecosystem, this question seems to be relevant to developers working on building and deploying NLP applications, as understanding the computational resources required by their chosen models is crucial for efficient model deployment and optimization.

",5.0,"This question can be understood by itself without any additional context because it refers to a specific technical term (""FLOPs"" stands for ""Floating Point Operations Per Second"") and a well-known architecture (""seresnet152d""), which are both documented in various places. The question is asking for a numerical value, but the model name and the concept of FLOPs count are clear enough that an operator could look up the answer without needing any additional information.

"
"We expect that every model added to ğŸ¤— Transformers passes a couple of integration tests, meaning that the original
model and the reimplemented version in ğŸ¤— Transformers have to give the exact same output up to a precision of 0.001!
Since it is normal that the exact same model written in different libraries can give a slightly different output
depending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives
nearly the same output, they have to be almost identical. Therefore, you will certainly compare the intermediate
outputs of the ğŸ¤— Transformers version multiple times against the intermediate outputs of the original implementation of
*brand_new_bert* in which case an **efficient** debugging environment of the original repository is absolutely
important. Here is some advice to make your debugging environment as efficient as possible.","What is the accepted error tolerance for model output precision?
",1e-3 (0.001),huggingface/transformers/blob/main/docs/source/en/add_new_model.md,5.0,"The context clearly states that for integration tests, a model's output should match up to a precision of 0.001 (1e-3), indicating an acceptable error tolerance of 1e-3.
",4.0,"This question seems to be relevant to NLP development, but it's a bit too broad and open-ended. The concept of ""accepted error tolerance"" can vary greatly depending on the specific task, dataset, and application. However, understanding the acceptable level of precision is crucial in evaluating model performance.
",5.0,"This question can be understood on its own without referencing a specific context or document, as it asks for a general standard (error tolerance) related to model output precision. It does not specify any particular setting or require additional information from an external source.

"
"Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).

The Gymnasium library provides two things:

- An interface that allows you to **create RL environments**.
- A **collection of environments** (gym-control, atari, box2D...).

Let's look at an example, but first let's recall the RL loop.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg"" alt=""The RL process"" width=""100%"">

At each step:
- Our Agent receivesÂ a **state (S0)**Â from theÂ **Environment**Â â€” we receive the first frame of our game (Environment).
- Based on thatÂ **state (S0),**Â the Agent takes anÂ **action (A0)**Â â€” our Agent will move to the right.
- The environment transitions to aÂ **new**Â **state (S1)**Â â€” new frame.
- The environment gives someÂ **reward (R1)**Â to the Agent â€” weâ€™re not deadÂ *(Positive Reward +1)*.


With Gymnasium:

1ï¸âƒ£ We create our environment using `gymnasium.make()`

2ï¸âƒ£ We reset the environment to its initial state with `observation = env.reset()`

At each step:

3ï¸âƒ£ Get an action using our model (in our example we take a random action)

4ï¸âƒ£ Using `env.step(action)`, we perform this action in the environment and get
- `observation`: The new state (st+1)
- `reward`: The reward we get after executing the action
- `terminated`: Indicates if the episode terminated (agent reach the terminal state)
- `truncated`: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.
- `info`: A dictionary that provides additional information (depends on the environment).

For more explanations check this ğŸ‘‰ https://gymnasium.farama.org/api/env/#gymnasium.Env.step

If the episode is terminated:
- We reset the environment to its initial state with `observation = env.reset()`

**Let's look at an example!** Make sure to read the code


```python
import gymnasium as gym","What does the Gymnasium library provide.
",An interface that allows you to create RL environments and a collection of environments.,huggingface/deep-rl-class/blob/main/units/en/unit1/hands-on.mdx,5.0,"The context clearly states that Gymnasium provides two things: (1) an interface for creating RL environments and (2) a collection of environments. Therefore, when asking what Gymnasium provides, one can confidently answer that it provides both an interface for creating RL environments and a collection of pre-built environments.

",4.0,"The Gymnasium library provides an environment for reinforcement learning benchmarking, featuring various tasks and a simple API. It seems to be a useful resource for developers working with reinforcement learning in NLP applications.
",5.0,"This question refers to a specific library (Gymnasium), but it's clear that the question is asking about its general purpose or functionality. The mention of the library name provides enough context for someone familiar with software libraries to understand what the question is asking.

"
"!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# mLUKE

## Overview

The mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension
of the [LUKE model](https://arxiv.org/abs/2010.01057) trained on the basis of XLM-RoBERTa.

It is based on XLM-RoBERTa and adds entity embeddings, which helps improve performance on various downstream tasks
involving reasoning about entities such as named entity recognition, extractive question answering, relation
classification, cloze-style knowledge completion.

The abstract from the paper is the following:","Who are the authors of the mLUKE model?
","Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.",huggingface/transformers/blob/main/docs/source/en/model_doc/mluke.md,5.0,"The context provides a direct reference to the authors of the mLUKE model in the first paragraph, specifically mentioning Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka as the authors. This information is clearly and unambiguously stated, allowing for a straightforward answer.

",5.0,"The question is specific and directly related to a particular pre-trained language model (mLUKE) used in the Hugging Face ecosystem. Providing the names of the model's authors would be useful for users looking to explore or utilize this model, especially if they are interested in understanding its development context.

",5.0,"This question requires minimal context to understand what ""mLUKE"" refers to. It seems to be a reference to a specific machine learning model, and the questioner likely has knowledge about it from external sources or documentation.
"
"Once you are satisfied with your model configuration, you can save it with [`~PretrainedConfig.save_pretrained`]. Your configuration file is stored as a JSON file in the specified save directory:

```py
>>> my_config.save_pretrained(save_directory=""./your_model_save_path"")
```

To reuse the configuration file, load it with [`~PretrainedConfig.from_pretrained`]:

```py
>>> my_config = DistilBertConfig.from_pretrained(""./your_model_save_path/config.json"")
```

<Tip>

You can also save your configuration file as a dictionary or even just the difference between your custom configuration attributes and the default configuration attributes! See the [configuration](main_classes/configuration) documentation for more details.

</Tip>

## Model

The next step is to create a [model](main_classes/models). The model - also loosely referred to as the architecture - defines what each layer is doing and what operations are happening. Attributes like `num_hidden_layers` from the configuration are used to define the architecture. Every model shares the base class [`PreTrainedModel`] and a few common methods like resizing input embeddings and pruning self-attention heads. In addition, all models are also either a [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html), [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) or [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) subclass. This means models are compatible with each of their respective framework's usage.

<frameworkcontent>
<pt>
Load your custom configuration attributes into the model:

```py
>>> from transformers import DistilBertModel

>>> my_config = DistilBertConfig.from_pretrained(""./your_model_save_path/config.json"")
>>> model = DistilBertModel(my_config)
```","What is the format of the configuration file when saved with `save_pretrained`?
",JSON file.,huggingface/transformers/blob/main/docs/source/en/create_a_model.md,5.0,"The context explicitly states that the configuration file is stored as a JSON file when saved with `save_pretrained`, and provides an example of loading the configuration from this JSON file using `from_pretrained`. This makes it clear that the format of the configuration file is JSON.

",4.0,"This question is useful because it asks about a specific aspect of using the Hugging Face ecosystem, namely the format of the configuration file after saving it with `save_pretrained`. The answer to this question would provide valuable information for machine learning developers who are working with pre-trained models and want to understand how to save and load their configurations. It's likely that many developers will encounter this issue when working with Hugging Face models, making this a relatively useful question.

",5.0,"This question makes sense by itself without requiring any additional context. The phrase ""when saved with `save_pretrained`"" provides a clear condition that allows the question to be understood independently, and an operator familiar with this terminology would know exactly what is being asked.
"
"### Convert model to `transformers`
After training we want to use the model in `transformers` e.g. to evaluate it on HumanEval. You can convert it to `transformers` following [this](https://huggingface.co/nvidia/megatron-gpt2-345m) tutorial. For instance, after the training is finished you can copy the weights of the last iteration 150k and convert the `model_optim_rng.pt` file to a `pytorch_model.bin` file that is supported by `transformers`.

```bash
mkdir -p nvidia/megatron-codeparrot-small
sudo docker cp CONTAINER_ID:/workspace/Megatron-LM/experiments/codeparrot-small/iter_0150000/mp_rank_00/model_optim_rng.pt nvidia/megatron-codeparrot-small
git clone https://github.com/huggingface/transformers.git
git clone https://github.com/NVIDIA/Megatron-LM.git
export PYTHONPATH=Megatron-LM
python transformers/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py nvidia/megatron-codeparrot-small/model_optim_rng.pt
```
Be careful, you will need to replace the generated vocabulary file and merges table after the conversion, with the original ones if you plan to load the tokenizer from there.

## Further Resources
A detailed description of the project can be found in the chapter ""Training Transformers from Scratch"" in the upcoming O'Reilly book [Natural Language Processing with Transformers](https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/).

This example was provided by [Leandro von Werra](www.github.com/lvwerra).","What is the name of the tutorial to convert a model to `transformers`?
",https://huggingface.co/nvidia/megatron-gpt2-345m,huggingface/transformers/blob/main/examples/research_projects/codeparrot/README.md,5.0,"The context clearly describes the process of converting a model to `transformers` and provides a link to a tutorial on how to do it. The code snippet also shows an example of how to convert a specific model using the `convert_megatron_gpt2_checkpoint.py` script from the `transformers` library.

",4.0,"The question appears to be asking about the specific tutorial or resource available within the Hugging Face ecosystem for converting models to work with the transformers library. This suggests that the user is looking for practical information on how to integrate their existing models into the transformers framework, which could indicate a need for detailed guidance on model conversion and adaptation.
",5.0,"The question clearly specifies what it's asking about (a tutorial related to converting models to transformers), but it doesn't provide any context or background information, implying that the reader should already know what ""convert a model to transformers"" means. However, this is not necessarily an issue as long as the question is clear and concise.

"
"|      |                                                                            |[baffo32/llama-7B-sparsetest-c4-25pct-128blksz](https://huggingface.co/baffo32/llama-7B-sparsetest-c4-25pct-128blksz)|0           |0                        |llama-license                                                                                 |https://huggingface.co/baffo32/llama-7B-sparsetest-c4-25pct-128blksz/blob/main/LICENSE        |[LICENSE](https://huggingface.co/baffo32/llama-7B-sparsetest-c4-25pct-128blksz/blob/main/LICENSE)  |                                                                                                                     |                                                                                   |
|      |                                                                            |[baffo32/llama-7B-sparsetest-c4-75pct-128blksz](https://huggingface.co/baffo32/llama-7B-sparsetest-c4-75pct-128blksz)|0           |0                        |llama-license                                                                                 |https://huggingface.co/baffo32/llama-7B-sparsetest-c4-75pct-128blksz/blob/main/LICENSE        |[LICENSE](https://huggingface.co/baffo32/llama-7B-sparsetest-c4-75pct-128blksz/blob/main/LICENSE)  |                                                                                                                     |                                                                                   |","What is the name of the license used by the llama model?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides a link to the LICENSE file for each model, which indicates that the license used by the llama models is indeed called ""llama-license"". Additionally, the table shows consistency in referencing this license across both models. Therefore, it can be inferred with confidence what license is being referred to.

",4.0,"This question appears to be specific and focused on a particular aspect of the Hugging Face ecosystem, which suggests that it may be relevant to machine learning developers who are interested in the legal or technical details of using the LLaMA model.

",5.0,"This question clearly asks about a specific piece of information related to the llama model, which can be understood without any additional context.
"
"Alright, now you should be ready to take a closer look into the actual code of BigBird.
You should have understood the following aspects of BigBird by now:

- BigBird provides a new attention layer for long-range sequence modelling that can be used
  as a drop-in replacement for already existing architectures. This means that every transformer-based model architecture can replace its [Self-attention layer](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) with BigBird's self-attention layer.
- BigBird's self-attention layer is composed of three mechanisms: block sparse (local) self-attention, global self-attention, random self-attention
- BigBird's block sparse (local) self-attention is different from Longformer's local self-attention. How so? Why does that matter? => Can be deployed on TPU much easier this way
- BigBird can be implemented for both an encoder-only model **and**
  for an encoder-decoder model, which means that we can reuse lots of [code from RoBERTa](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py) and [from PEGASUS](https://github.com/huggingface/transformers/blob/main/src/transformers/models/pegasus/modeling_pegasus.py) at a later stage.


If any of the mentioned aspects above are **not** clear to you, now is a great time to talk to Patrick.

### 2. Next prepare your environment

1.  Fork the [repository](https://github.com/huggingface/transformers)
    by clicking on the 'Fork' button on the repository's page. This
    creates a copy of the code under your GitHub user account.

2.  Clone your `transformers` fork to your local disk, and add the base
    repository as a remote:

    ```bash
    git clone https://github.com/[your Github handle]/transformers.git
    cd transformers
    git remote add upstream https://github.com/huggingface/transformers.git
    ```

3.  Set up a development environment, for instance by running the
    following command:","What are the three mechanisms of BigBird's self-attention layer?
","Block sparse (local) self-attention, global self-attention, and random self-attention.",huggingface/transformers/blob/main/templates/adding_a_new_model/open_model_proposals/ADD_BIG_BIRD.md,4.0,"The context clearly states that BigBird's self-attention layer is composed of three mechanisms: block sparse (local) self-attention, global self-attention, and random self-attention. However, it does not provide a detailed explanation of each mechanism or how they work together.

",4.0,"This question appears to be very specific to the BigBird model, which is a variant of the transformer architecture. The fact that it asks about the ""three mechanisms"" implies that there might be some unique aspects of the self-attention layer in BigBird that are worth understanding for NLP applications using the Hugging Face ecosystem.

",5.0,"The question is clear and concise, asking about a specific aspect of a particular model (BigBird). It does not rely on additional information or context beyond what is provided by the name ""BigBird"" itself. However, it assumes that the reader is familiar with the concept of self-attention layers in neural networks.

"
"Take a look at the tensor values in the [`DDIMPipeline`] after two inference steps:

```python
from diffusers import DDIMPipeline
import numpy as np

model_id = ""google/ddpm-cifar10-32""

# load model and scheduler
ddim = DDIMPipeline.from_pretrained(model_id, use_safetensors=True)

# run pipeline for just two steps and return numpy tensor
image = ddim(num_inference_steps=2, output_type=""np"").images
print(np.abs(image).sum())
```

Running the code above prints one value, but if you run it again you get a different value. What is going on here?

Every time the pipeline is run, [`torch.randn`](https://pytorch.org/docs/stable/generated/torch.randn.html) uses a different random seed to create Gaussian noise which is denoised stepwise. This leads to a different result each time it is run, which is great for diffusion pipelines since it generates a different random image each time.

But if you need to reliably generate the same image, that'll depend on whether you're running the pipeline on a CPU or GPU.

### CPU

To generate reproducible results on a CPU, you'll need to use a PyTorch [`Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) and set a seed:

```python
import torch
from diffusers import DDIMPipeline
import numpy as np

model_id = ""google/ddpm-cifar10-32""

# load model and scheduler
ddim = DDIMPipeline.from_pretrained(model_id, use_safetensors=True)

# create a generator for reproducibility
generator = torch.Generator(device=""cpu"").manual_seed(0)

# run pipeline for just two steps and return numpy tensor
image = ddim(num_inference_steps=2, output_type=""np"", generator=generator).images
print(np.abs(image).sum())
```

Now when you run the code above, it always prints a value of `1491.1711` no matter what because the `Generator` object with the seed is passed to all the random functions of the pipeline.

If you run this code example on your specific hardware and PyTorch version, you should get a similar, if not the same, result.

<Tip>","What happens to the random seed used by `torch.randn` in the DDIMPipeline each time it is run?
",It uses a different random seed.,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/reproducibility.md,5.0,"The context explains that `torch.randn` uses a different random seed each time it is run, which results in different values when the pipeline is executed multiple times. It also provides an example of how to generate reproducible results on a CPU by using a PyTorch Generator with a set seed.

",4.0,"This question seems useful because understanding how the random seed affects the model's behavior and output is crucial for reproducibility and debugging. The specific context of the Hugging Face ecosystem and the DDIM pipeline suggests that the user has a clear understanding of the underlying architecture and is looking to optimize or troubleshoot their NLP application.

However, without more information about the DDIM pipeline's implementation and how it uses `torch.randn`, it's difficult to provide a definitive answer. Nonetheless, this question could be useful for sparking discussion around reproducibility and randomness in Hugging Face pipelines.

",5.0,"The question is clear about its context, mentioning a specific library (PyTorch) and a specific function (`torch.randn`). It also mentions a pipeline, which suggests that the context is likely related to the DDIM model or a similar generative model. However, the question itself does not depend on any additional information about the implementation details of the DDIMPipeline, making it self-contained.

"
"1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.
1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (from NAVER CLOVA) released with the paper [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.
1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.
1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz SuÃ¡rez*, Yoann Dupont, Laurent Romary, Ã‰ric Villemonte de la Clergerie, DjamÃ© Seddah and BenoÃ®t Sagot.
1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.","Who are the authors of the paper ""ByT5: Towards a token-free future with pre-trained byte-to-byte models""?
","Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel.",huggingface/transformers/blob/main/README_ru.md,5.0,"The context explicitly mentions that the ByT5 model was released with a paper titled ""ByT5: Towards a token-free future with pre-trained byte-to-byte models"", and lists the authors of that paper as Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. This direct information makes it easy to identify the authors of the paper.

",4.0,"This question appears to be seeking information about a specific research paper and its authors. A machine learning developer building NLP applications with the Hugging Face ecosystem might find this information useful for citing relevant work, understanding the context of certain models or techniques, or even collaborating with the authors on future projects.
",5.0,"The question refers to a specific paper, but the title of the paper provides enough context for an operator to identify the question as inquiring about the authors of that particular work.
"
"```py
import requests
from PIL import Image
import torch
from transformers import ViTImageProcessor, ViTForImageClassification
```

Next, we will download a random image of two cats and remote controls on a couch from the COCO dataset and preprocess the image to transform it to the input format expected by the model. To do this, we can conveniently use the corresponding preprocessor class (`ViTProcessor`). To initialize the model and the preprocessor, we will use one of the [Kakao Brain ViT repos](https://huggingface.co/models?search=kakaobrain/vit) on the hub. Note that initializing the preprocessor from a repository ensures that the preprocessed image is in the expected format required by that specific pretrained model.

```py
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = ViTImageProcessor.from_pretrained('kakaobrain/vit-large-patch16-384')
model = ViTForImageClassification.from_pretrained('kakaobrain/vit-large-patch16-384')
```

The rest is simple, we will forward preprocess the image and use it as input to the model to retrive the class logits. The Kakao Brain ViT image classification models are trained on ImageNet labels and output logits of shape (batch_size, 1000).
```py
# preprocess image or list of images
inputs = processor(images=image, return_tensors=""pt"")

# inference
with torch.no_grad():
    outputs = model(**inputs)

# apply SoftMax to logits to compute the probability of each class
preds = torch.nn.functional.softmax(outputs.logits, dim=-1)

# print the top 5 class predictions and their probabilities
top_class_preds = torch.argsort(preds, descending=True)[0, :5]

for c in top_class_preds:
    print(f""{model.config.id2label[c.item()]} with probability {round(preds[0, c.item()].item(), 4)}"")
```","What is the shape of the logits output by the Kakao Brain ViT image classification model?
","(batch_size, 1000)",huggingface/blog/blob/main/vit-align.md,4.0,"The context provides information on how to use the Kakao Brain ViT image classification model for inference, but it does not explicitly state what shape the logits output by the model has. However, the code snippet shows that the logits are accessed using `outputs.logits`, and it is mentioned in a comment that the model outputs ""logits of shape (batch_size, 1000)"". This suggests that the logits have a specific known shape.

",4.0,"This question is very useful because it requires knowledge of a specific model (Kakao Brain ViT) and its output format, which is a key aspect of building NLP applications with the Hugging Face ecosystem. The answer would likely involve understanding the architecture of the model and how it generates logits.
",5.0,"The question makes sense on its own, as it refers to a specific type of model (Kakao Brain ViT) and asks for information about its output. There is no implicit mention of context or additional information required to understand the question.
"
"Metric Card for SuperGLUE

## Metric description
This metric is used to compute the SuperGLUE evaluation metric associated to each of the subsets of the [SuperGLUE dataset](https://huggingface.co/datasets/super_glue).

SuperGLUE is a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard.


## How to use

There are two steps: (1) loading the SuperGLUE metric relevant to the subset of the dataset being used for evaluation; and (2) calculating the metric.

1. **Loading the relevant SuperGLUE metric** : the subsets of SuperGLUE are the following: `boolq`, `cb`, `copa`, `multirc`, `record`, `rte`, `wic`, `wsc`, `wsc.fixed`, `axb`, `axg`.

More information about the different subsets of the SuperGLUE dataset can be found on the [SuperGLUE dataset page](https://huggingface.co/datasets/super_glue) and on the [official dataset website](https://super.gluebenchmark.com/).

2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the model to score and one list of reference labels. The structure of both inputs depends on the SuperGlUE subset being used:

Format of `predictions`:
- for `record`: list of question-answer dictionaries with the following keys:
    - `idx`: index of the question as specified by the dataset
    - `prediction_text`: the predicted answer text
- for `multirc`: list of question-answer dictionaries with the following keys:
    - `idx`: index of the question-answer pair as specified by the dataset
    - `prediction`: the predicted answer label
- otherwise: list of predicted labels

Format of `references`:
- for `record`: list of question-answers dictionaries with the following keys:
    - `idx`: index of the question as specified by the dataset
    - `answers`: list of possible answers
- otherwise: list of reference labels","What are the subsets of the SuperGLUE dataset?
","boolq, cb, copa, multirc, record, rte, wic, wsc, wsc.fixed, axb, axg.",huggingface/datasets/blob/main/metrics/super_glue/README.md,5.0,"The question is clearly and directly answered by the context, which lists all the subsets of the SuperGLUE dataset in a separate section.

",4.0,"The question seems to be asking about the specific datasets within the SuperGLUE benchmark, which is a suite of natural language understanding tasks. This information would likely be useful for machine learning developers building NLP applications with the Hugging Face ecosystem, as they might want to know which datasets are included in the SuperGLUE benchmark and how they can be used.

",5.0,"The question asks about the subsets of a specific dataset, SuperGLUE, which implies some context knowledge. However, it does not require any additional information to understand what is being asked, as the term ""subsets"" is clear and the question refers to a well-defined concept in the field.

"
"<details>
<summary><b>Model Output: [click to expand]</b></summary>

```
Output:
----------------------------------------------------------------------------------------------------
Deeper neural networks are more difficult to train. We present a residual learning framework to ease
the training of networks that are substantially deeper than those used previously. We show that the
residual learning framework can be used to train deep neural networks that are significantly more
difficult to train than those used previously. We also show that the residual learning framework can
be used to train deep neural networks that are significantly more difficult to train than those used
previously.

The paper presents a new residual learning framework for deep neural networks that is based on the
concept of residuals. The residuals are the residuals of the network that are not used in the training
process. The residuals are computed by taking the residuals of the network that are used in the training
process and subtracting the residuals of the network that are not used in the training process. The
residuals are then used to train the network. The residuals are computed by taking the residuals of
the network that are used in the training process and subtracting the residuals of the network that
are not used in the training process. The residuals are then used to train the network. The residuals
are computed by taking the residuals of the network that are used in the training process and
subtracting the residuals of the
----------------------------------------------------------------------------------------------------
```
</details>


<span id='opt_greedy_example_two'/>

##### 6.2.2. Generating Text with Nucleus Sampling:

<details>
<summary><b>Code: [click to expand]</b></summary>","What is a difficulty in training deeper neural networks.
",They are more difficult to train.,huggingface/blog/blob/main/introducing-csearch.md,5.0,"The context clearly states that ""Deeper neural networks are more difficult to train"" and presents a residual learning framework as a solution to ease this difficulty. This suggests that one of the difficulties in training deeper neural networks is indeed their propensity for being harder to train, which aligns with the question.

",4.0,"This question is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it touches upon a fundamental aspect of deep learning. The ability to identify challenges in training deeper neural networks can inform decisions on model architecture and hyperparameter tuning.

",5.0,"This question is asking about a general challenge or obstacle in deep learning, which can be understood without any specific context. The question is phrased broadly and doesn't rely on any particular setting or information that would make it difficult to understand.

"
". So when should one use a decoder? Decoders, like encoders, can be used as standalone models. As they generate a numerical representation, they can also be used in a wide variety of tasks. However, the strength of a decoder lies in the way a word has access to its left context. The decoders, having only access to their left context, areÂ inherentlyÂ good at text generation: the ability to generate a word, or a sequence of words, given a known sequence of words. In NLP, this is known as Causal Language Modeling. Let's look at an example. Here's an example of how causal language modeling works: we start with an initial word, which is ""My"". We use this as input for the decoder. The model outputs a vectors of dimension 768. This vector contains information about the sequence, which is here a single word, or word. We apply a small transformation to that vector so that it maps to all the words known by the model (mapping which we'll see later, called a language modeling head). We identify that the model believes the most probable following word is ""name"". We then take that new word, and add it to the initial sequence. From ""My"", we are now at ""My name"". This is where the ""autoregressive"" aspect comes in. Auto-regressive models re-use their past outputs as inputs in the following steps. Once again, we do that the exact same operation: we cast that sequence through the decoder, and retrieve the most probable following word. In this case, it is the word ""is"". We repeat the operation until we're satisfied. Starting from a single word, we've now generated a full sentence. We decide to stop there, but we could continue for a while; GPT-2, for example, has a maximum context size of 1024. We could eventually generate up to 1024 words, and the decoder would still have some memory of the first words of the sequence! If we go back several levels higher, back to the full transformer model, we can see what we learned about the decoder part of the full transformer model","What is the maximum context size of GPT-2?

",1024,huggingface/course/blob/main/subtitles/en/raw/chapter1/06_decoders.md,5.0,"The context clearly mentions that GPT-2 has a maximum context size of 1024, which directly answers the question. There is no ambiguity or uncertainty in this statement.

",4.0,"This question appears to be a factual inquiry about the properties of a specific language model, GPT-2. To determine its usefulness for machine learning developers building NLP applications with the Hugging Face ecosystem, we need to consider whether knowledge of the maximum context size is relevant or useful in this context. The maximum context size could impact how much text can be processed by the model, but without more context, it's difficult to assess its practical implications. Nonetheless, providing accurate information about a specific model's characteristics is generally helpful for developers attempting to choose between different models.
",5.0,"The question is clear and unambiguous, asking directly about a specific characteristic (maximum context size) of a well-known model (GPT-2). There are no references to external information or contexts that would require additional knowledge beyond the name of the model.
"
"## **Decoder**

As mentioned in the *Encoder-Decoder* section, the *transformer-based*
decoder defines the conditional probability distribution of a target
sequence given the contextualized encoding sequence:

$$ p_{\theta_{dec}}(\mathbf{Y}_{1: m} | \mathbf{\overline{X}}_{1:n}), $$

which by Bayes\' rule can be decomposed into a product of conditional
distributions of the next target vector given the contextualized
encoding sequence and all previous target vectors:

$$ p_{\theta_{dec}}(\mathbf{Y}_{1:m} | \mathbf{\overline{X}}_{1:n}) = \prod_{i=1}^{m} p_{\theta_{dec}}(\mathbf{y}_i | \mathbf{Y}_{0: i-1}, \mathbf{\overline{X}}_{1:n}). $$

Let\'s first understand how the transformer-based decoder defines a
probability distribution. The transformer-based decoder is a stack of
*decoder blocks* followed by a dense layer, the \""LM head\"". The stack
of decoder blocks maps the contextualized encoding sequence
\\(\mathbf{\overline{X}}_{1:n}\\) and a target vector sequence prepended by
the \\(\text{BOS}\\) vector and cut to the last target vector, *i.e.*
\\(\mathbf{Y}_{0:i-1}\\), to an encoded sequence of target vectors
\\(\mathbf{\overline{Y}}_{0: i-1}\\). Then, the \""LM head\"" maps the encoded
sequence of target vectors \\(\mathbf{\overline{Y}}_{0: i-1}\\) to a
sequence of logit vectors
\\(\mathbf{L}_{1:n} = \mathbf{l}_1, \ldots, \mathbf{l}_n\\), whereas the
dimensionality of each logit vector \\(\mathbf{l}_i\\) corresponds to the
size of the vocabulary. This way, for each \\(i \in \{1, \ldots, n\}\\) a
probability distribution over the whole vocabulary can be obtained by
applying a softmax operation on \\(\mathbf{l}_i\\). These distributions
define the conditional distribution:

$$p_{\theta_{dec}}(\mathbf{y}_i | \mathbf{Y}_{0: i-1}, \mathbf{\overline{X}}_{1:n}), \forall i \in \{1, \ldots, n\},$$","What is the dimensionality of each logit vector in the LM head?
",The size of the vocabulary.,huggingface/blog/blob/main/encoder-decoder.md,5.0,"The context provides a detailed explanation of the transformer-based decoder and its components, including the LM head. It explicitly states that the dimensionality of each logit vector corresponds to the size of the vocabulary, which is necessary information to answer the question.

",4.0,"This question seems to be asking about a specific implementation detail of the Hugging Face transformer models, which is relevant to NLP developers using the Hugging Face ecosystem. The question is precise and focused on a particular aspect of the model architecture.

",5.0,"This question is clear and concise, asking about a specific aspect of a model component (the LM head) without referencing any external context. The terminology used is technical but still understandable with some background knowledge of deep learning models.
"
"LCM-LoRAs are available for [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), and the [SSD-1B](https://huggingface.co/segmind/SSD-1B) model. All the checkpoints can be found in this [collection](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6).

For more details about LCM-LoRA, refer to [the technical report](https://huggingface.co/papers/2311.05556).

This guide shows how to perform inference with LCM-LoRAs for
- text-to-image
- image-to-image
- combined with styled LoRAs
- ControlNet/T2I-Adapter
- inpainting
- AnimateDiff

Before going through this guide, we'll take a look at the general workflow for performing inference with LCM-LoRAs.
LCM-LoRAs are similar to other Stable Diffusion LoRAs so they can be used with any [`DiffusionPipeline`] that supports LoRAs.

- Load the task specific pipeline and model.
- Set the scheduler to [`LCMScheduler`].
- Load the LCM-LoRA weights for the model.
- Reduce the `guidance_scale` between `[1.0, 2.0]` and set the `num_inference_steps` between [4, 8].
- Perform inference with the pipeline with the usual parameters.

Let's look at how we can perform inference with LCM-LoRAs for different tasks.

First, make sure you have [peft](https://github.com/huggingface/peft) installed, for better LoRA support.

```bash
pip install -U peft
```

## Text-to-image

You'll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMScheduler`] and then load the LCM-LoRA. Together with the LCM-LoRA and the scheduler, the pipeline enables a fast inference workflow overcoming the slow iterative nature of diffusion models.

```python
import torch
from diffusers import DiffusionPipeline, LCMScheduler

pipe = DiffusionPipeline.from_pretrained(
    ""stabilityai/stable-diffusion-xl-base-1.0"",
    variant=""fp16"",
    torch_dtype=torch.float16
).to(""cuda"")","What are the available models for which LCM-LoRAs are available?
","stable-diffusion-v1-5, stable-diffusion-xl-base-1.0, and SSD-1B.",huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md,5.0,"The context explicitly lists the available models for which LCM-LoRAs are available, making it clear and unambiguous that these three specific models support LCM-LoRAs.
",4.0,"This question seems to be related to a specific model type called LCM-LoRAs, which I'm assuming stands for Localized Contextualized Memory-based Longformer Reasoning Assemblies. The question is asking about the availability of these models within the Hugging Face ecosystem. A rating of 4 indicates that this question could be useful for someone who has already worked with longformer models and is looking to explore LCM-LoRAs, but may not be as relevant to a broader audience.
",5.0,"This question can be understood without any prior knowledge of a specific context, as it only mentions a model type (LCM-LoRA) and asks for related information. The answer is likely to be a list of models that support LCM-LoRAs.

"
"## PEFT configurations

<Tip>

Learn more about the parameters you can configure for each PEFT method in their respective API reference page.

</Tip>

A configuration stores important parameters that specify how a particular PEFT method should be applied.

For example, take a look at the following [`LoraConfig`](https://huggingface.co/ybelkada/opt-350m-lora/blob/main/adapter_config.json) for applying LoRA and [`PromptEncoderConfig`](https://huggingface.co/smangrul/roberta-large-peft-p-tuning/blob/main/adapter_config.json) for applying p-tuning (these configuration files are already JSON-serialized). Whenever you load a PEFT adapter, it is a good idea to check whether it has an associated adapter_config.json file which is required.

<hfoptions id=""config"">
<hfoption id=""LoraConfig"">

```json
{
  ""base_model_name_or_path"": ""facebook/opt-350m"", #base model to apply LoRA to
  ""bias"": ""none"",
  ""fan_in_fan_out"": false,
  ""inference_mode"": true,
  ""init_lora_weights"": true,
  ""layers_pattern"": null,
  ""layers_to_transform"": null,
  ""lora_alpha"": 32,
  ""lora_dropout"": 0.05,
  ""modules_to_save"": null,
  ""peft_type"": ""LORA"", #PEFT method type
  ""r"": 16,
  ""revision"": null,
  ""target_modules"": [
    ""q_proj"", #model modules to apply LoRA to (query and value projection layers)
    ""v_proj""
  ],
  ""task_type"": ""CAUSAL_LM"" #type of task to train model on
}
```

You can create your own configuration for training by initializing a [`LoraConfig`].

```py
from peft import LoraConfig, TaskType

lora_config = LoraConfig(
    r=16,
    target_modules=[""q_proj"", ""v_proj""],
    task_type=TaskType.CAUSAL_LM,
    lora_alpha=32,
    lora_dropout=0.05
)
```

</hfoption>
<hfoption id=""PromptEncoderConfig"">","What is the default value of `lora_alpha` in LoraConfig?
",32,huggingface/peft/blob/main/docs/source/tutorial/peft_model_config.md,4.0,"The context provides an example JSON configuration file for `LoraConfig`, which includes a key-value pair for `lora_alpha` with a default value of `32`. However, the question asks about the default value of `lora_alpha` in general, and there is no explicit statement that this specific default value applies universally. The context focuses more on explaining how to create and use `LoraConfig`, rather than providing definitive information about its default values.

",5.0,"This question is useful because it asks about a specific hyperparameter in the Hugging Face ecosystem, which suggests that the person asking has some familiarity with the library but needs clarification on certain details. Knowing the default values of configuration parameters can be crucial for reproducibility and optimization of models, making this question relevant to machine learning developers building NLP applications with the Hugging Face ecosystem.

",5.0,"The question explicitly mentions ""LoraConfig"", which is a specific class, and asks about its attribute ""lora_alpha"". This suggests that the context required to understand the question is limited to knowing what LoraConfig is and what attributes it has.
"
"In our example, we will take a couple of arguments of the ResNet class that we might want to tweak. Different
configurations will then give us the different types of ResNets that are possible. We then just store those arguments,
after checking the validity of a few of them.

```python
from transformers import PretrainedConfig
from typing import List


class ResnetConfig(PretrainedConfig):
    model_type = ""resnet""

    def __init__(
        self,
        block_type=""bottleneck"",
        layers: List[int] = [3, 4, 6, 3],
        num_classes: int = 1000,
        input_channels: int = 3,
        cardinality: int = 1,
        base_width: int = 64,
        stem_width: int = 64,
        stem_type: str = """",
        avg_down: bool = False,
        **kwargs,
    ):
        if block_type not in [""basic"", ""bottleneck""]:
            raise ValueError(f""`block_type` must be 'basic' or bottleneck', got {block_type}."")
        if stem_type not in ["""", ""deep"", ""deep-tiered""]:
            raise ValueError(f""`stem_type` must be '', 'deep' or 'deep-tiered', got {stem_type}."")

        self.block_type = block_type
        self.layers = layers
        self.num_classes = num_classes
        self.input_channels = input_channels
        self.cardinality = cardinality
        self.base_width = base_width
        self.stem_width = stem_width
        self.stem_type = stem_type
        self.avg_down = avg_down
        super().__init__(**kwargs)
```

The three important things to remember when writing you own configuration are the following:
- you have to inherit from `PretrainedConfig`,
- the `__init__` of your `PretrainedConfig` must accept any kwargs,
- those `kwargs` need to be passed to the superclass `__init__`.","What is the default value for the 'num_classes' argument in the ResnetConfig class?
",1000,huggingface/transformers/blob/main/docs/source/en/custom_models.md,5.0,"The context provided is a snippet of code that defines a custom configuration class `ResnetConfig` for a ResNet model. Within this class, there is an argument named `num_classes` which has a default value of 1000. This makes it clear how to answer the question about the default value of `num_classes`.

",4.0,"This question is relatively specific to a particular implementation of a neural network architecture (ResNet) within the Hugging Face ecosystem. It shows an understanding of the architecture's configuration options and a desire to know the default value for a specific parameter.
",5.0,"The question refers to a specific class (ResnetConfig) and its argument ('num_classes'), but it does not require any external context or information beyond the class definition. An operator with access to documentation can understand what is being asked.
"
"This setup introduces some challenges. For example, although large batch sizes are usually better for performance, batch sizes in MOEs are effectively reduced as data flows through the active experts. For example, if our batched input consists of 10 tokens, **five tokens might end in one expert, and the other five tokens might end in five different experts, leading to uneven batch sizes and underutilization**. The [Making MoEs go brrr](#making-moes-go-brrr) section below will discuss other challenges and solutions.

How can we solve this? A learned gating network (G) decides which experts (E) to send a part of the input:

$$
y = \sum_{i=1}^{n} G(x)_i E_i(x)
$$

In this setup, all experts are run for all inputs - itâ€™s a weighted multiplication. But, what happens if G is 0? If thatâ€™s the case, thereâ€™s no need to compute the respective expert operations and hence we save compute. Whatâ€™s a typical gating function? In the most traditional setup, we just use a simple network with a softmax function. The network will learn which expert to send the input.

$$
G_\sigma(x) = \text{Softmax}(x \cdot W_g)
$$

Shazeerâ€™s work also explored other gating mechanisms, such as Noisy Top-k Gating. This gating approach introduces some (tunable) noise and then keeps the top k values. That is:

1. We add some noise

$$
H(x)_i = (x \cdot W_{\text{g}})_i + \text{StandardNormal()} \cdot \text{Softplus}((x \cdot W_{\text{noise}})_i)

$$

2. We only pick the top k

$$
\text{KeepTopK}(v, k)_i = \begin{cases}
v_i & \text{if } v_i \text{ is in the top } k \text{ elements of } v, \\
-\infty & \text{otherwise.}
\end{cases}
$$

3. We apply the softmax.

$$
G(x) = \text{Softmax}(\text{KeepTopK}(H(x), k))
$$","What is the name of the work by Shazeer that explored other gating mechanisms?
",Shazeer's work.,huggingface/blog/blob/main/moe.md,5.0,"The question can be answered unambiguously because it specifically mentions a work by Shazeer that explored other gating mechanisms, and the text immediately follows with a description of Noisy Top-k Gating, which is attributed to Shazeer's work. This makes it clear what work is being referred to.

",4.0,"This question appears to be about a specific research paper or concept related to NLP, but it lacks context and clarity. However, it seems like it could be useful for someone looking to understand more about different gating mechanisms in NLP models.

",4.0,"The question clearly mentions a specific author (""Shazeer"") and asks about their work, indicating that it's looking for a specific piece of information related to that person's research. However, there is no additional context or setting provided, so an operator familiar with the field could potentially understand what's being asked without needing more information.
"
"```
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary.

            Indices can be obtained using [`AlbertTokenizer`]. See [`~PreTrainedTokenizer.encode`] and
            [`~PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
```

For optional arguments or arguments with defaults we follow the following syntax: imagine we have a function with the
following signature:

```py
def my_function(x: str=None, a: float=3.14):
```

then its documentation should look like this:

```
    Args:
        x (`str`, *optional*):
            This argument controls ...
        a (`float`, *optional*, defaults to `3.14`):
            This argument is used to ...
```

Note that we always omit the ""defaults to \`None\`"" when None is the default for any argument. Also note that even
if the first line describing your argument type and its default gets long, you can't break it on several lines. You can
however write as many lines as you want in the indented description (see the example above with `input_ids`).

#### Writing a multi-line code block

Multi-line code blocks can be useful for displaying examples. They are done between two lines of three backticks as usual in Markdown:


````
```
# first line of code
# second line
# etc
```
````

#### Writing a return block

The return block should be introduced with the `Returns:` prefix, followed by a line return and an indentation.
The first line should be the type of the return, followed by a line return. No need to indent further for the elements
building the return.

Here's an example of a single value return:

```
    Returns:
        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.
```

Here's an example of a tuple return, comprising several objects:","What are input IDs?
",Indices of input sequence tokens in the vocabulary.,huggingface/diffusers/blob/main/docs/README.md,5.0,"The question ""What are input IDs?"" is clearly answerable with the given context. The context explicitly defines `input_ids` as a parameter in the function and provides a description explaining what it represents.

",5.0,"This question appears to be foundational to understanding how the Hugging Face ecosystem processes text data. Input IDs are a crucial concept in NLP models, such as those used in transformers like BERT and RoBERTa, where they serve as a numerical representation of the input text that can be fed into the model's architecture. The ability to identify input IDs is essential for tasks like tokenization, where each word or subword in the input sequence is converted into an integer ID.

",5.0,"The term ""input IDs"" refers to a concept commonly used in natural language processing (NLP) and deep learning, particularly when working with models like BERT or other transformer-based architectures. It represents the numerical encoding of input text into a format that can be processed by these models.

"
"<p class=""text-gray-700"">Technical descriptions of how ğŸ¤— Datasets classes and methods work.</p>
    </a>
  </div>
</div>","What is the subject matter of technical descriptions provided?
",Technical descriptions of how Datasets classes and methods work.,huggingface/datasets/blob/main/docs/source/index.mdx,5.0,"The question can be answered unambiguously because the context explicitly states that the technical descriptions are about how a specific class (Datasets) and its methods work, implying a focus on software development or data science.
",4.0,"This question appears to be relevant to NLP applications, particularly those that involve text classification or information extraction. Understanding the subject matter of technical descriptions can help machine learning developers build more accurate models for tasks such as document categorization or named entity recognition.

",5.0,"This question appears to be self-contained and doesn't rely on any specific context or additional information. The phrase ""technical descriptions"" suggests a general concept, and the question asks about the subject matter of these descriptions, which can be understood without prior knowledge of a particular setting.

"
"```python
import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

@dataclass
class DataCollatorCTCWithPadding:
    """"""
    Data collator that will dynamically pad the inputs received.
    Args:
        processor (:class:`~transformers.Wav2Vec2Processor`)
            The processor used for proccessing the data.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence if provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
        max_length (:obj:`int`, `optional`):
            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
        max_length_labels (:obj:`int`, `optional`):
            Maximum length of the ``labels`` returned list and optionally padding length (see above).
        pad_to_multiple_of (:obj:`int`, `optional`):
            If set will pad the sequence to a multiple of the provided value.
            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
            7.5 (Volta).
    """"""","What is the default padding strategy for returned sequences in DataCollatorCTCWithPadding?
",True or 'longest'.,huggingface/blog/blob/main/fine-tune-wav2vec2-english.md,4.0,"The context provides detailed information about the DataCollatorCTCWithPadding class, including its arguments and behavior. Specifically, it mentions that the class ""will dynamically pad the inputs received"" and describes several padding strategies. However, it does not explicitly state what the default padding strategy is.
",5.0,"This question shows a good understanding of the specific functionality within the Hugging Face ecosystem, namely the `DataCollatorCTCWithPadding` class. It demonstrates a desire to understand the behavior of this class under default settings, which is an important aspect of troubleshooting and fine-tuning NLP models for sequence-to-sequence tasks involving CTC (Connectionist Temporal Classification) losses.
",5.0,"The question refers to a specific function or class within Hugging Face's Transformers library, but it does not require any external context or information. It directly asks about a property of that function, making it understandable on its own.
"
"{/if}

### 6. What are the techniques to be aware of when batching sequences of different lengths together?

<Question
	choices={[
		{
			text: ""Truncating"",
			explain: ""Yes, truncation is a correct way of evening out sequences so that they fit in a rectangular shape. Is it the only one, though?"",
			correct: true
		},
		{
			text: ""Returning tensors"",
			explain: ""While the other techniques allow you to return rectangular tensors, returning tensors isn't helpful when batching sequences together.""
		},
		{
			text: ""Padding"",
			explain: ""Yes, padding is a correct way of evening out sequences so that they fit in a rectangular shape. Is it the only one, though?"",
			correct: true
		},
		{
			text: ""Attention masking"",
			explain: ""Absolutely! Attention masks are of prime importance when handling sequences of different lengths. That's not the only technique to be aware of, however."",
			correct: true
		}
	]}
/>

### 7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?

<Question
	choices={[
		{
			text: ""It softens the logits so that they're more reliable."",
			explain: ""No, the SoftMax function does not affect the reliability of results.""
		},
		{
			text: ""It applies a lower and upper bound so that they're understandable."",
			explain: ""Correct! The resulting values are bound between 0 and 1. That's not the only reason we use a SoftMax function, though."",
            correct: true
		},
		{
			text: ""The total sum of the output is then 1, resulting in a possible probabilistic interpretation."",
			explain: ""Correct! That's not the only reason we use a SoftMax function, though."",
            correct: true
		}
	]}
/>

### 8. What method is most of the tokenizer API centered around?","What technique is used to handle sequences of different lengths when batching them together?
","Truncation, padding, and attention masking.",huggingface/course/blob/main/chapters/en/chapter2/8.mdx,4.0,"The context discusses techniques for handling sequences of different lengths when batching them together, and lists truncation, padding, returning tensors, and attention masking as possible methods. However, it does not explicitly mention which technique is used to handle sequences of different lengths.

",4.0,"This question seems to be relevant to machine learning developers working with NLP applications, particularly those using the Hugging Face ecosystem. The technique being asked about (handling sequences of different lengths) is a common challenge in many NLP tasks that involve processing variable-length input sequences, such as text or speech data.

However, for it to be extremely useful, the question could benefit from more context or specificity regarding the Hugging Face ecosystem, such as mentioning Transformers or Tokenizers. As it stands, the answer might not directly lead to a solution specific to the Hugging Face tools but rather provide general information applicable across various NLP libraries.

",5.0,"The question refers to a common problem in machine learning, specifically when dealing with sequences (e.g., text or time series data) that have varying lengths. This issue arises during the batching process, where these sequences are grouped together for training or inference. By asking about the technique used to address this challenge, it implies knowledge of batch processing and sequence handling in machine learning.

"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# AutoPipeline

ğŸ¤— Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you're new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you're using the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the [`StableDiffusionImg2ImgPipeline`] and [`StableDiffusionInpaintPipeline`] classes respectively.

The `AutoPipeline` class is designed to simplify the variety of pipelines in ğŸ¤— Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.

<Tip>

Take a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.

</Tip>

This tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.

## Choose an AutoPipeline for your task","How many tasks does the AutoPipeline support currently?
",Three.,huggingface/diffusers/blob/main/docs/source/en/tutorials/autopipeline.md,4.0,"The context mentions that the AutoPipeline supports ""text-to-image, image-to-image, and inpainting"", but it does not provide an explicit count of tasks. However, based on the phrase ""currently, it supports"" followed by a list of three tasks, it can be inferred that these are the only tasks supported by the AutoPipeline.

",4.0,"This question is useful because it shows that the user has an interest in understanding the capabilities of the AutoPipeline, which can impact their decision on whether to use it for a specific NLP task. The answer will likely provide insight into the current limitations and potential areas for improvement of the AutoPipeline.
",5.0,"This question is clear and self-contained, asking about the current number of supported tasks by the AutoPipeline. It doesn't require any additional context or information to understand what is being asked.
"
"</div>

## What's the Hugging Face Hub?

We are helping the community work together towards the goal of advancing Machine Learning ğŸ”¥.

The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.

No single company, including the Tech Titans, will be able to â€œsolve AIâ€ by themselves â€“ the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone ğŸš€.

We encourage you to read the [Code of Conduct](https://huggingface.co/code-of-conduct) and the [Content Guidelines](https://huggingface.co/content-guidelines) to familiarize yourself with the values that we expect our community members to uphold ğŸ¤—.

## What can you find on the Hub?

The Hugging Face Hub hosts Git-based repositories, which are version-controlled buckets that can contain all your files. ğŸ’¾

On it, you'll be able to upload and discover...

- Models, _hosting the latest state-of-the-art models for NLP, vision, and audio tasks_
- Datasets, _featuring a wide variety of data for different domains and modalities_..
- Spaces, _interactive apps for demonstrating ML models directly in your browser_.

The Hub offers **versioning, commit history, diffs, branches, and over a dozen library integrations**! You can learn more about the features that all repositories share in the [**Repositories documentation**](./repositories).

## Models","How many models are hosted on the Hugging Face Hub?
",120k,huggingface/hub-docs/blob/main/docs/hub/index.md,5.0,"The context clearly states that ""The Hugging Face Hub hosts over 120k models"" which directly answers the question of how many models are hosted on the platform.

",4.0,"This question appears to be factual and straightforward, aiming to gather information about the number of models available on the Hugging Face Hub. As a machine learning developer working with the Hugging Face ecosystem, knowing this number can give an idea of the scope and scale of the community's contributions, which might be useful for understanding the potential for collaboration or finding suitable pre-trained models for specific tasks.
",5.0,"This question can be understood without any additional context, as it refers to a public database of pre-trained models. The only information needed is that there exists such a platform called the Hugging Face Hub.
"
"## Simple over easy

As PyTorch states, **explicit is better than implicit** and **simple is better than complex**. This design philosophy is reflected in multiple parts of the library:
- We follow PyTorch's API with methods like [`DiffusionPipeline.to`](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.to) to let the user handle device management.
- Raising concise error messages is preferred to silently correct erroneous input. Diffusers aims at teaching the user, rather than making the library as easy to use as possible.
- Complex model vs. scheduler logic is exposed instead of magically handled inside. Schedulers/Samplers are separated from diffusion models with minimal dependencies on each other. This forces the user to write the unrolled denoising loop. However, the separation allows for easier debugging and gives the user more control over adapting the denoising process or switching out diffusion models or schedulers.
- Separately trained components of the diffusion pipeline, *e.g.* the text encoder, the unet, and the variational autoencoder, each have their own model class. This forces the user to handle the interaction between the different model components, and the serialization format separates the model components into different files. However, this allows for easier debugging and customization. DreamBooth or Textual Inversion training
is very simple thanks to Diffusers' ability to separate single components of the diffusion pipeline.

## Tweakable, contributor-friendly over abstraction","What is PyTorch's design philosophy regarding simplicity and complexity in its API?
",Simple is better than complex.,huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md,5.0,"The context clearly states PyTorch's design philosophy as ""simple is better than complex"" and provides examples of how this philosophy is implemented in the library. It explains that explicitness is preferred over implicitness and that concise error messages are raised instead of silently correcting input errors. Additionally, it highlights how complex model vs. scheduler logic is exposed and separately trained components of the diffusion pipeline are handled by the user. This information directly addresses the question about PyTorch's design philosophy regarding simplicity and complexity in its API.

",4.0,"This question seems to be asking about a specific aspect of the PyTorch library, which is part of the Hugging Face ecosystem. The question targets the design principles behind PyTorch, specifically how it balances simplicity and complexity in its API. A high rating implies that understanding this design philosophy could provide valuable insights for developers building NLP applications with PyTorch, such as how to leverage its strengths and limitations effectively.

",5.0,"The question refers to PyTorch, a well-known deep learning framework, but does not require any additional context beyond this. It asks for a specific aspect of PyTorch's design philosophy, which is easily understood without knowing the specific document or setting.

"
"The [`M2M100Tokenizer`] depends on `sentencepiece` so be sure to install it before running the
examples. To install `sentencepiece` run `pip install sentencepiece`.

**Supervised Training**

```python
from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer

model = M2M100ForConditionalGeneration.from_pretrained(""facebook/m2m100_418M"")
tokenizer = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"", src_lang=""en"", tgt_lang=""fr"")

src_text = ""Life is like a box of chocolates.""
tgt_text = ""La vie est comme une boÃ®te de chocolat.""

model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=""pt"")

loss = model(**model_inputs).loss  # forward pass
```

**Generation**

M2M100 uses the `eos_token_id` as the `decoder_start_token_id` for generation with the target language id
being forced as the first generated token. To force the target language id as the first generated token, pass the
*forced_bos_token_id* parameter to the *generate* method. The following example shows how to translate between
Hindi to French and Chinese to English using the *facebook/m2m100_418M* checkpoint.

```python
>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

>>> hi_text = ""à¤œà¥€à¤µà¤¨ à¤à¤• à¤šà¥‰à¤•à¤²à¥‡à¤Ÿ à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥€ à¤¤à¤°à¤¹ à¤¹à¥ˆà¥¤""
>>> chinese_text = ""ç”Ÿæ´»å°±åƒä¸€ç›’å·§å…‹åŠ›ã€‚""

>>> model = M2M100ForConditionalGeneration.from_pretrained(""facebook/m2m100_418M"")
>>> tokenizer = M2M100Tokenizer.from_pretrained(""facebook/m2m100_418M"")

>>> # translate Hindi to French
>>> tokenizer.src_lang = ""hi""
>>> encoded_hi = tokenizer(hi_text, return_tensors=""pt"")
>>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(""fr""))
>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
""La vie est comme une boÃ®te de chocolat.""","What is the library required to be installed before running the examples for M2M100Tokenizer?
",sentencepiece.,huggingface/transformers/blob/main/docs/source/en/model_doc/m2m_100.md,5.0,"The question is clearly answerable with the given context because it directly asks about a specific requirement for running examples of the M2M100Tokenizer. The context explicitly states that `sentencepiece` needs to be installed before running the examples, and this is exactly what the question asks.
",4.0,"The question seems to be related to using the Hugging Face ecosystem, specifically with regards to the M2M100Tokenizer. To answer this question accurately would require knowledge of the specific libraries and dependencies required for the Hugging Face Transformers library, which includes tokenizers like M2M100Tokenizer.

However, without more context or information about what is meant by ""running the examples"", it's difficult to provide a definitive answer.

",5.0,"The question is clear and requires no additional context or background information. It mentions a specific tokenizer (M2M100Tokenizer) and asks about a requirement for running its examples, which implies that the answer can be deduced from general knowledge of how to use tokenizers.

"
"!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# XLM-RoBERTa-XL

## Overview

The XLM-RoBERTa-XL model was proposed in [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.

The abstract from the paper is the following:

*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*

This model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).","Who are the contributors of the XLM-RoBERTa-XL model?
",Soonhwan-Kwon and stefan-it.,huggingface/transformers/blob/main/docs/source/en/model_doc/xlm-roberta-xl.md,5.0,"The context provides a clear overview of the XLM-RoBERTa-XL model, including its proposal in a research paper and its performance on various tasks. Additionally, it mentions the contributors to the model as ""Soonhwan-Kwon"" and ""stefan-it"", along with a link to their GitHub profiles and the original code repository.

",4.0,"This question is useful because it seeks information about the specific contributors of a particular pre-trained language model, which can be valuable for understanding the context and credibility of the model. The answer could provide insights into the expertise and diversity of the team behind the model.
",5.0,"The question clearly specifies the model name (XLM-RoBERTa-XL) and asks for a specific piece of information about it (the contributors), which can be looked up without additional context.
"
"Most dataset configurations will take one of four forms (below you will see examples of each case):

- Case 1: The example is a pair of sentences and a label indicating how similar they are. The label can be either an integer or a float. This case applies to datasets originally prepared for Natural Language Inference (NLI), since they contain pairs of sentences with a label indicating whether they infer each other or not.
- Case 2: The example is a pair of positive (similar) sentences **without** a label. For example, pairs of paraphrases, pairs of full texts and their summaries, pairs of duplicate questions, pairs of (`query`, `response`), or pairs of (`source_language`, `target_language`). Natural Language Inference datasets can also be formatted this way by pairing entailing sentences. Having your data in this format can be great since you can use the `MultipleNegativesRankingLoss`, one of the most used loss functions for Sentence Transformers models.
- Case 3: The example is a sentence with an integer label. This data format is easily converted by loss functions into three sentences (triplets) where the first is an ""anchor"", the second a ""positive"" of the same class as the anchor, and the third a ""negative"" of a different class. Each sentence has an integer label indicating the class to which it belongs.
- Case 4: The example is a triplet (anchor, positive, negative) without classes or labels for the sentences.

As an example, in this tutorial you will train a Sentence Transformer using a dataset in the fourth case. You will then fine-tune it using the second case dataset configuration (please refer to the [Notebook Companion](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/95_Training_Sentence_Transformers.ipynb) for this blog).","What is the name of the loss function used for Sentence Transformers models?
",MultipleNegativesRankingLoss,huggingface/blog/blob/main/how-to-train-sentence-transformers.md,5.0,"The context explicitly mentions the `MultipleNegativesRankingLoss` as one of the most used loss functions for Sentence Transformers models, which is a clear indication that it is indeed the correct answer. Additionally, the text describes how datasets can be formatted in different ways to use this loss function.

",4.0,"The question appears to be about the specifics of a particular model architecture, which might be useful for developers who are directly working with Sentence Transformers. However, it's a relatively specific and narrow question that may not provide broader insights or applicability to a wide range of machine learning developers using the Hugging Face ecosystem.

",5.0,"This question clearly asks about a specific aspect of a well-defined model (Sentence Transformers), and it does not require additional context to understand what is being asked. The question specifically mentions the type of model, which implies that the answer can be deduced from general knowledge about Sentence Transformers without needing further information.
"
"## New Core ML Optimizations

Core ML is a mature framework that allows machine learning models to run efficiently on-device, taking advantage of all the compute hardware in Apple devices: the CPU, the GPU, and the Neural Engine specialized in ML tasks. On-device execution is going through a period of extraordinary interest triggered by the popularity of models such as Stable Diffusion and Large Language Models with chat interfaces. Many people want to run these models on their hardware for a variety of reasons, including convenience, privacy, and API cost savings. Naturally, many developers are exploring ways to run these models efficiently on-device and creating new apps and use cases. Core ML improvements that contribute to achieving that goal are big news for the community!

The Core ML optimization changes encompass two different (but complementary) software packages:

* The Core ML framework itself. This is the engine that runs ML models on Apple hardware and is part of the operating system. Models have to be exported in a special format supported by the framework, and this format is also referred to as â€œCore MLâ€.
* The `coremltools` conversion package. This is an [open-source Python module](https://github.com/apple/coremltools) whose mission is to convert PyTorch or Tensorflow models to the Core ML format.","What is the name of the open-source Python module used to convert PyTorch or TensorFlow models to the Core ML format?
",coremltools,huggingface/blog/blob/main/fast-diffusers-coreml.md,5.0,"The context clearly states that the 'coremltools' conversion package is an open-source Python module used to convert PyTorch or TensorFlow models to the Core ML format. This information is explicitly mentioned in the text, making it easy to determine the correct answer.
",4.0,"The question appears to be about converting machine learning models from popular deep learning frameworks (PyTorch and TensorFlow) to a specific format (Core ML), which is relevant to the Hugging Face ecosystem. However, it's worth noting that this is not an explicitly NLP-related task, as Core ML is typically used for mobile app development in general.
",5.0,"This question can be understood without any additional context, as it directly asks for a specific tool (module) used for model conversion between PyTorch/TensorFlow and Core ML formats. It mentions specific frameworks but does not imply that the reader needs to know anything about a particular document or situation.

"
"The implementation is framework agnostic, and will work for PyTorch and TensorFlow models. If we have saved this in
a file named `pair_classification.py`, we can then import it and register it like this:

```py
from pair_classification import PairClassificationPipeline
from transformers.pipelines import PIPELINE_REGISTRY
from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification

PIPELINE_REGISTRY.register_pipeline(
    ""pair-classification"",
    pipeline_class=PairClassificationPipeline,
    pt_model=AutoModelForSequenceClassification,
    tf_model=TFAutoModelForSequenceClassification,
)
```

Once this is done, we can use it with a pretrained model. For instance `sgugger/finetuned-bert-mrpc` has been
fine-tuned on the MRPC dataset, which classifies pairs of sentences as paraphrases or not.

```py
from transformers import pipeline

classifier = pipeline(""pair-classification"", model=""sgugger/finetuned-bert-mrpc"")
```

Then we can share it on the Hub by using the `save_pretrained` method in a `Repository`:

```py
from huggingface_hub import Repository

repo = Repository(""test-dynamic-pipeline"", clone_from=""{your_username}/test-dynamic-pipeline"")
classifier.save_pretrained(""test-dynamic-pipeline"")
repo.push_to_hub()
```

This will copy the file where you defined `PairClassificationPipeline` inside the folder `""test-dynamic-pipeline""`,
along with saving the model and tokenizer of the pipeline, before pushing everything into the repository
`{your_username}/test-dynamic-pipeline`. After that, anyone can use it as long as they provide the option
`trust_remote_code=True`:

```py
from transformers import pipeline

classifier = pipeline(model=""{your_username}/test-dynamic-pipeline"", trust_remote_code=True)
```

## Add the pipeline to ğŸ¤— Transformers","What are the two models that can be used with a PairClassificationPipeline?
",AutoModelForSequenceClassification and TFAutoModelForSequenceClassification.,huggingface/transformers/blob/main/docs/source/en/add_new_pipeline.md,4.0,"The context provides information on how to register a PairClassificationPipeline and use it with pre-trained models from the Hugging Face model hub, but does not explicitly mention two specific models that can be used with this pipeline. However, based on the provided code snippet where AutoModelForSequenceClassification (PyTorch) and TFAutoModelForSequenceClassification (TensorFlow) are registered as possible models for the PairClassificationPipeline, it is reasonable to infer that these are two of the possible models.

",4.0,"This question is useful because it specifically asks about a particular component of the Hugging Face ecosystem, namely the `PairClassificationPipeline`, and requests information on compatible models. A correct answer would provide insight into the available options for using this pipeline in NLP tasks. However, without knowing the two models that can be used with a PairClassificationPipeline, one cannot accurately evaluate how useful the question is.

",4.0,"The question clearly specifies that it's asking about the context of a PairClassificationPipeline, which implies that the reader should have some prior knowledge of this pipeline. However, the term itself is descriptive and suggests that the answer is not dependent on external information beyond what can be inferred from the name.

"
"**à¤¸à¥‹à¤°à¥à¤¸ à¤•à¥‹à¤¡**: <a href=""https://github.com/huggingface/huggingface_hub"" target=""_blank"">https://github.com/huggingface/huggingface_hub</a>

---

## huggingface_hub à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤®à¥‡à¤‚ à¤†à¤ªà¤•à¤¾ à¤¸à¥à¤µà¤¾à¤—à¤¤ à¤¹à¥ˆ

`huggingface_hub` à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤†à¤ªà¤•à¥‹ [à¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤¹à¤¬](https://huggingface.co/) à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¬à¤¾à¤¤à¤šà¥€à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤¦à¥‡à¤¤à¥€ à¤¹à¥ˆ, à¤œà¥‹ à¤°à¤šà¤¨à¤¾à¤•à¤¾à¤°à¥‹à¤‚ à¤”à¤° à¤¸à¤¹à¤¯à¥‹à¤—à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤“à¤ªà¤¨-à¤¸à¥‹à¤°à¥à¤¸ à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤•à¤¾ à¤²à¥‹à¤•à¤¤à¤‚à¤¤à¥à¤°à¥€à¤•à¤°à¤£ à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤à¤• à¤®à¤‚à¤š à¤¹à¥ˆà¥¤ à¤…à¤ªà¤¨à¥€ à¤ªà¤°à¤¿à¤¯à¥‹à¤œà¤¨à¤¾à¤“à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤”à¤° à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤–à¥‹à¤œà¥‡à¤‚ à¤¯à¤¾ à¤¹à¤¬ à¤ªà¤° à¤¹à¥‹à¤¸à¥à¤Ÿ à¤•à¤¿à¤ à¤—à¤ à¤¹à¤œà¤¾à¤°à¥‹à¤‚ à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤à¤ªà¥à¤¸ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤–à¥‡à¤²à¥‡à¤‚à¥¤ à¤†à¤ª à¤¸à¤®à¥à¤¦à¤¾à¤¯ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤…à¤ªà¤¨à¥‡ à¤¸à¥à¤µà¤¯à¤‚ à¤•à¥‡ à¤®à¥‰à¤¡à¤², à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤”à¤° à¤¡à¥‡à¤®à¥‹ à¤­à¥€ à¤¬à¤¨à¤¾ à¤”à¤° à¤¸à¤¾à¤à¤¾ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ `huggingface_hub` à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤ªà¤¾à¤¯à¤¥à¤¨ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤‡à¤¨ à¤¸à¤­à¥€ à¤šà¥€à¤œà¥‹à¤‚ à¤•à¥‹ à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤à¤• à¤†à¤¸à¤¾à¤¨ à¤¤à¤°à¥€à¤•à¤¾ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆà¥¤

## à¤ªà¥à¤°à¤®à¥à¤– à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤‚

- [à¤«à¤¼à¤¾à¤‡à¤²à¥‡à¤‚ à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤•à¤°à¥‡à¤‚](https://huggingface.co/docs/huggingface_hub/en/guides/download) à¤¹à¤¬ à¤¸à¥‡à¥¤
- [à¤«à¤¼à¤¾à¤‡à¤²à¥‡à¤‚ à¤…à¤ªà¤²à¥‹à¤¡ à¤•à¤°à¥‡à¤‚](https://huggingface.co/docs/huggingface_hub/en/guides/upload) à¤¹à¤¬ à¤ªà¤°à¥¤
- [à¤…à¤ªà¤¨à¥€ à¤°à¤¿à¤ªà¥‰à¤œà¤¿à¤Ÿà¤°à¥€ à¤ªà¥à¤°à¤¬à¤‚à¤§à¤¿à¤¤ à¤•à¤°à¥‡à¤‚](https://huggingface.co/docs/huggingface_hub/en/guides/repository)à¥¤
- à¤¤à¥ˆà¤¨à¤¾à¤¤ à¤®à¥‰à¤¡à¤²à¥‹à¤‚ à¤ªà¤° [à¤…à¤¨à¥à¤®à¤¾à¤¨ à¤šà¤²à¤¾à¤à¤](https://huggingface.co/docs/huggingface_hub/en/guides/inference)à¥¤
- à¤®à¥‰à¤¡à¤², à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤”à¤° à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥‡ à¤²à¤¿à¤ [à¤–à¥‹à¤œ](https://huggingface.co/docs/huggingface_hub/en/guides/search)à¥¤
- [à¤®à¥‰à¤¡à¤² à¤•à¤¾à¤°à¥à¤¡ à¤¸à¤¾à¤à¤¾ à¤•à¤°à¥‡à¤‚](https://huggingface.co/docs/huggingface_hub/en/guides/model-cards) à¤…à¤ªà¤¨à¥‡ à¤®à¥‰à¤¡à¤²à¥‹à¤‚ à¤•à¤¾ à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¥€à¤•à¤°à¤£ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤à¥¤
- [à¤¸à¤®à¥à¤¦à¤¾à¤¯ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤œà¥à¤¡à¤¼à¥‡à¤‚](https://huggingface.co/docs/huggingface_hub/en/guides/community) à¤ªà¥€à¤†à¤° à¤”à¤° à¤Ÿà¤¿à¤ªà¥à¤ªà¤£à¤¿à¤¯à¥‹à¤‚ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡à¥¤

## à¤¸à¥à¤¥à¤¾à¤ªà¤¨à¤¾

[pip](https://pypi.org/project/huggingface-hub/) à¤•à¥‡ à¤¸à¤¾à¤¥ `huggingface_hub` à¤ªà¥ˆà¤•à¥‡à¤œ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¥‡à¤‚:

```bash
pip install huggingface_hub
```

à¤¯à¤¦à¤¿ à¤†à¤ª à¤šà¤¾à¤¹à¥‡à¤‚, à¤¤à¥‹ à¤†à¤ª à¤‡à¤¸à¥‡ [conda](https://huggingface.co/docs/huggingface_hub/en/installation#install-with-conda) à¤¸à¥‡ à¤­à¥€ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤ªà¥ˆà¤•à¥‡à¤œ à¤•à¥‹ à¤¡à¤¿à¤«à¤¼à¥‰à¤²à¥à¤Ÿ à¤°à¥‚à¤ª à¤¸à¥‡ à¤¨à¥à¤¯à¥‚à¤¨à¤¤à¤® à¤°à¤–à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, `huggingface_hub` à¤•à¥à¤› à¤‰à¤ªà¤¯à¥‹à¤— à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥‹à¤—à¥€ à¤µà¥ˆà¤•à¤²à¥à¤ªà¤¿à¤• à¤¨à¤¿à¤°à¥à¤­à¤°à¤¤à¤¾ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤†à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤, à¤¯à¤¦à¤¿ à¤†à¤ª à¤…à¤¨à¥à¤®à¤¾à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤‚à¤ªà¥‚à¤°à¥à¤£ à¤…à¤¨à¥à¤­à¤µ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤šà¤²à¤¾à¤à¤:","à¤•à¥ˆà¤¸à¥‡ pip à¤•à¥‡ à¤¸à¤¾à¤¥ huggingface_hub à¤ªà¥ˆà¤•à¥‡à¤œ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¥‡à¤‚?
",pip install huggingface_hub,huggingface/huggingface_hub/blob/main/README_hi.md,5.0,"à¤¯à¤¹ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤¬à¤¹à¥à¤¤ à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤¹à¥ˆ à¤”à¤° à¤‡à¤¸à¥‡ à¤¦à¥€ à¤—à¤ˆ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤¸à¤¾à¤®à¤—à¥à¤°à¥€ à¤¸à¥‡ à¤†à¤¸à¤¾à¤¨à¥€ à¤¸à¥‡ à¤¹à¤² à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤®à¥‡à¤‚ à¤ªà¥‚à¤›à¥€ à¤—à¤ˆ à¤¬à¤¾à¤¤, à¤ªà¥ˆà¤•à¥‡à¤œ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¤¨à¤¾, à¤¸à¥€à¤§à¥‡ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤¸à¤¾à¤®à¤—à¥à¤°à¥€ à¤®à¥‡à¤‚ à¤µà¤°à¥à¤£à¤¿à¤¤ à¤¹à¥ˆ, à¤œà¤¹à¤¾à¤‚ à¤¯à¤¹ à¤¬à¤¤à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ª pip à¤•à¥‡ à¤¸à¤¾à¤¥ huggingface_hub à¤ªà¥ˆà¤•à¥‡à¤œ à¤•à¥ˆà¤¸à¥‡ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

",5.0,"à¤¯à¤¹ à¤à¤• à¤‰à¤ªà¤¯à¥‹à¤—à¥€ à¤¸à¤µà¤¾à¤² à¤¹à¥ˆ à¤œà¥‹ à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤µà¤¿à¤•à¤¾à¤¸à¤•à¥‹à¤‚ à¤•à¥‹ Hugging Face à¤‡à¤•à¥‹à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤®à¥‡à¤‚ NLP à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤—à¥‹à¤‚ à¤•à¥‡ à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤•à¥‡ à¤¸à¤‚à¤¬à¤‚à¤§ à¤®à¥‡à¤‚ à¤ªà¤¾à¤‡à¤ª à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤²à¥‡à¤¶à¤¨ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤–à¤¼à¤¾à¤¸à¤•à¤° à¤œà¤¬ à¤µà¥‡ à¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤Ÿà¥‚à¤²à¥à¤¸ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ à¤¹à¤—à¤¿à¤‚à¤— à¤«à¥‡à¤¸ à¤¹à¤¬ à¤•à¥‹ à¤…à¤ªà¤¨à¥‡ à¤ªà¥à¤°à¥‹à¤œà¥‡à¤•à¥à¤Ÿ à¤®à¥‡à¤‚ à¤à¤•à¥€à¤•à¥ƒà¤¤ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
",5.0,"The question is clear and concise, asking about installing a specific package (huggingface_hub) using a particular tool (pip). It does not rely on any additional context or information.
"
"--
title: Exact Match
emoji: ğŸ¤—
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  Returns the rate at which the input predicted strings exactly match their references, ignoring any strings input as part of the regexes_to_ignore list.
---

# Metric Card for Exact Match


## Metric Description
A given predicted string's exact match score is 1 if it is the exact same as its reference string, and is 0 otherwise.

- **Example 1**: The exact match score of prediction ""Happy Birthday!"" is 0, given its reference is ""Happy New Year!"".
- **Example 2**: The exact match score of prediction ""The Colour of Magic (1983)"" is 1, given its reference is also ""The Colour of Magic (1983)"".

The exact match score of a set of predictions is the sum of all of the individual exact match scores in the set, divided by the total number of predictions in the set.

- **Example**: The exact match score of the set {Example 1, Example 2} (above) is 0.5.


## How to Use
At minimum, this metric takes as input predictions and references:
```python
>>> from evaluate import load
>>> exact_match_metric = load(""exact_match"")
>>> results = exact_match_metric.compute(predictions=predictions, references=references)
```","What is the name of the Python module used to load the Exact Match metric?
",evaluate,huggingface/evaluate/blob/main/metrics/exact_match/README.md,4.0,"The context provides information about the Exact Match metric, including its description, examples, and usage instructions. However, it does not explicitly mention a Python module used to load the Exact Match metric. Nevertheless, based on the code snippet provided in the ""How to Use"" section, we can infer that the ""evaluate"" module is used to load the metric.
",4.0,"The question seems to be asking about a specific detail related to the Hugging Face ecosystem, which suggests that it might be useful for someone who needs to implement or understand how to use the Exact Match metric in their NLP application.
",5.0,"This question can be understood without any additional information, as it asks for a specific module that serves a particular purpose.
"
"This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/zihangdai/xlnet/).

## Usage tips

- The specific attention pattern can be controlled at training and test time using the `perm_mask` input.
- Due to the difficulty of training a fully auto-regressive model over various factorization order, XLNet is pretrained
  using only a sub-set of the output tokens as target which are selected with the `target_mapping` input.
- To use XLNet for sequential decoding (i.e. not in fully bi-directional setting), use the `perm_mask` and
  `target_mapping` inputs to control the attention span and outputs (see examples in
  *examples/pytorch/text-generation/run_generation.py*)
- XLNet is one of the few models that has no sequence length limit.
- XLNet is not a traditional autoregressive model but uses a training strategy that builds on that. It permutes the tokens in the sentence, then allows the model to use the last n tokens to predict the token n+1. Since this is all done with a mask, the sentence is actually fed in the model in the right order, but instead of masking the first n tokens for n+1, XLNet uses a mask that hides the previous tokens in some given permutation of 1,â€¦,sequence length.
- XLNet also uses the same recurrence mechanism as Transformer-XL to build long-term dependencies.

## Resources

- [Text classification task guide](../tasks/sequence_classification)
- [Token classification task guide](../tasks/token_classification)
- [Question answering task guide](../tasks/question_answering)
- [Causal language modeling task guide](../tasks/language_modeling)
- [Multiple choice task guide](../tasks/multiple_choice)

## XLNetConfig

[[autodoc]] XLNetConfig

## XLNetTokenizer

[[autodoc]] XLNetTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## XLNetTokenizerFast

[[autodoc]] XLNetTokenizerFast","What is the name of the training strategy used by XLNet that builds on traditional autoregressive models?
",Permutation-based training strategy.,huggingface/transformers/blob/main/docs/source/en/model_doc/xlnet.md,5.0,"The context provides a clear description of the training strategy used by XLNet, which builds on traditional autoregressive models. It explains how XLNet permutes the tokens in the sentence and uses a mask to hide previous tokens in some given permutation, allowing the model to use the last n tokens to predict the token n+1. This description directly addresses the question and provides enough information to determine that the training strategy used by XLNet is not traditional autoregressive but rather a modified version that builds on it.

",4.0,"The question asks about a specific detail related to XLNet, a pre-trained language model developed by Hugging Face. This indicates that the questioner has some familiarity with NLP and the Hugging Face ecosystem but is looking for more information on how XLNet works. The question is relatively narrow in scope and does not seem to be asking for general advice or troubleshooting guidance.
",5.0,"The question asks about a specific aspect of the XLNet model, which is a known pre-trained language model. It mentions a particular training strategy that builds upon traditional autoregressive models, indicating that the question requires some background knowledge about XLNet and its architecture.
"
"```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

It looks like our code tried to call `input_ids.size()`, but this clearly won't work for a Python `list`, which is just a container. How can we solve this problem? Searching for the error message on Stack Overflow gives quite a few relevant [hits](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f). Clicking on the first one displays a similar question to ours, with the answer shown in the screenshot below:

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png"" alt=""An answer from Stack Overflow."" width=""100%""/>
</div>

The answer recommends that we add `return_tensors='pt'` to the tokenizer, so let's see if that works for us:","How can I resolve an AttributeError 'list' object has no attribute 'size' in a code snippet when calling input_ids.size()?
",Adding return_tensors='pt' to the tokenizer.,huggingface/course/blob/main/chapters/en/chapter8/2.mdx,5.0,"The question is clearly and unambiguously answerable with the given context. The code snippet provided shows an AttributeError due to calling 'size()' on a list object. This error can be resolved by ensuring that 'input_ids' is not a list, but rather a tensor. According to the Stack Overflow search results, adding 'return_tensors='pt'' to the tokenizer should resolve this issue.
",4.0,"This question is about troubleshooting a specific error that may occur in NLP applications, which requires knowledge of how to handle common issues with Hugging Face's Transformers library. It is likely that someone who encounters this error will search for solutions online and come across the answer to this question.

",5.0,"This question makes sense by itself as it describes a specific issue with a code snippet and asks for a solution. The error message provided is clear, and the context (i.e., the code snippet) can be inferred from the question.

"
"</Tip>

## Text-to-image

```py
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
import torch

repo_id = ""stabilityai/stable-diffusion-2-base""
pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=""fp16"")

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to(""cuda"")

prompt = ""High quality photo of an astronaut riding a horse in space""
image = pipe(prompt, num_inference_steps=25).images[0]
image
```

## Inpainting

```py
import torch
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils import load_image, make_image_grid

img_url = ""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png""
mask_url = ""https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png""

init_image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

repo_id = ""stabilityai/stable-diffusion-2-inpainting""
pipe = DiffusionPipeline.from_pretrained(repo_id, torch_dtype=torch.float16, revision=""fp16"")

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
pipe = pipe.to(""cuda"")

prompt = ""Face of a yellow cat, high resolution, sitting on a park bench""
image = pipe(prompt=prompt, image=init_image, mask_image=mask_image, num_inference_steps=25).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```

## Super-resolution

```py
from diffusers import StableDiffusionUpscalePipeline
from diffusers.utils import load_image, make_image_grid
import torch

# load model and scheduler
model_id = ""stabilityai/stable-diffusion-x4-upscaler""
pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipeline = pipeline.to(""cuda"")","What is the name of the repository ID used to load the model for text-to-image generation?
",stabilityai/stable-diffusion-2-base,huggingface/diffusers/blob/main/docs/source/en/api/pipelines/stable_diffusion/stable_diffusion_2.md,4.0,"The context provides a clear example of loading the model for text-to-image generation using the `DiffusionPipeline` from the Diffusers library. However, there is no explicit mention of a ""repository ID"" used to load the model in this specific code snippet. Nevertheless, the context does show that the repository ID is referred to as `repo_id`, which is assigned the value `""stabilityai/stable-diffusion-2-base""` for text-to-image generation and `""stabilityai/stable-diffusion-2-inpainting""` for inpainting.

",4.0,"This question appears to be quite specific, likely referring to a particular library or framework within the Hugging Face ecosystem. It shows an understanding of the concept of loading models and might indicate that the user is looking to implement text-to-image generation capabilities in their NLP application.
",4.0,"The question explicitly asks for a specific piece of information related to loading a model, which can be identified with knowledge of typical workflow or documentation. However, it does not specify any particular context (document, project, etc.), and the term 'repository ID' is clear enough to imply a need for accessing some form of data storage.
"
"If you want to see how to load a specific model, you can click `Use in Adapter Transformers` and you will be given a working snippet that you can load it!

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-transformers_snippet.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-transformers_snippet-dark.png""/>
</div>

## Sharing your models

At the moment there is no automatic method to upload your models to the Hub, but the process to upload them is documented in the [official guide](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md#share-your-models).

All the recipes create all the needed files to upload a model to the Hub. The process usually involves the following steps:
1. Create and clone a model repository.
2. Moving files from the recipe output to the repository (model card, model filte, TensorBoard traces).
3. Push the files (`git add` + `git commit` + `git push`).

Once you do this, you can try out your model directly in the browser and share it with the rest of the community.

## Additional resources

* Asteroid [website](https://asteroid-team.github.io/).
* Asteroid [library](https://github.com/asteroid-team/asteroid).
* Integration [docs](https://github.com/asteroid-team/asteroid/blob/master/docs/source/readmes/pretrained_models.md).","How to load a specific model.
",Click 'Use in Adapter Transformers'.,huggingface/hub-docs/blob/main/docs/hub/asteroid.md,4.0,"The context only provides a link to load a specific model, but it does not explicitly explain the steps involved in loading the model. However, the image snippet appears to show a Python code that can be used to load the model.

",4.0,"This question is useful because loading a specific model is a fundamental task in NLP development with the Hugging Face ecosystem. The ability to load pre-trained models, fine-tune them, and integrate them into applications is crucial for developers working with transformer-based architectures. However, the answer largely depends on which specific model the user wants to load (e.g., BERT, RoBERTa, DistilBERT), how they plan to use it (for example, in a classification task or language translation), and whether they are working locally or on a cloud platform like Google Colab or AWS SageMaker. A detailed answer would need to account for these specifics.

",4.0,"The question is clear and concise, but it implies that there are multiple models available, so the operator needs to know which one is being referred to.
"
"- BEiT models are regular Vision Transformers, but pre-trained in a self-supervised way rather than supervised. They
  outperform both the [original model (ViT)](vit) as well as [Data-efficient Image Transformers (DeiT)](deit) when fine-tuned on ImageNet-1K and CIFAR-100. You can check out demo notebooks regarding inference as well as
  fine-tuning on custom data [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer) (you can just replace
  [`ViTFeatureExtractor`] by [`BeitImageProcessor`] and
  [`ViTForImageClassification`] by [`BeitForImageClassification`]).
- There's also a demo notebook available which showcases how to combine DALL-E's image tokenizer with BEiT for
  performing masked image modeling. You can find it [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT).
- As the BEiT models expect each image to be of the same size (resolution), one can use
  [`BeitImageProcessor`] to resize (or rescale) and normalize images for the model.
- Both the patch resolution and image resolution used during pre-training or fine-tuning are reflected in the name of
  each checkpoint. For example, `microsoft/beit-base-patch16-224` refers to a base-sized architecture with patch
  resolution of 16x16 and fine-tuning resolution of 224x224. All checkpoints can be found on the [hub](https://huggingface.co/models?search=microsoft/beit).
- The available checkpoints are either (1) pre-trained on [ImageNet-22k](http://www.image-net.org/) (a collection of
  14 million images and 22k classes) only, (2) also fine-tuned on ImageNet-22k or (3) also fine-tuned on [ImageNet-1k](http://www.image-net.org/challenges/LSVRC/2012/) (also referred to as ILSVRC 2012, a collection of 1.3 million
  images and 1,000 classes).
- BEiT uses relative position embeddings, inspired by the T5 model. During pre-training, the authors shared the","What is the patch resolution of a base-sized architecture with fine-tuning resolution of 224x224 according to BEiT checkpoints?
",16x16,huggingface/transformers/blob/main/docs/source/en/model_doc/beit.md,5.0,"The question asks for the patch resolution of a specific architecture, which is directly mentioned in the context as ""patch resolution of 16x16"" for a base-sized architecture with fine-tuning resolution of 224x224. This information is explicitly stated in the context, making it easy to answer the question unambiguously.

",4.0,"This question appears to be specific and related to the Hugging Face ecosystem, as it asks about a particular aspect (patch resolution) of a pre-trained model (BEiT). The mention of ""base-sized architecture"" and ""fine-tuning resolution"" suggests that the question is targeted at NLP developers working with large models. However, the question lacks specificity regarding the context in which this patch resolution is being used or its relevance to building an NLP application.
",4.0,"This question is clear and unambiguous, as it specifies the context (BEiT checkpoints) and the relevant parameters (base-sized architecture, fine-tuning resolution). However, the question assumes prior knowledge about the specific architecture and checkpoints being referred to. The key phrase ""according to BEiT checkpoints"" indicates that the answer depends on the specifics of the BEiT model, which may not be universally known.

"
"### `To use the type as a Parameter, please correct the detach() semantics defined by __torch_dispatch__() implementation.`

Use the latest version of `accelerate` with a command such as: `pip install -U accelerate` and the problem should be solved.

### `Parameter has no attribue .CB`

Same solution as above.

### `RuntimeError: CUDA error: an illegal memory access was encountered ... consider passing CUDA_LAUNCH_BLOCKING=1`

Run your script by pre-pending `CUDA_LAUNCH_BLOCKING=1` and you should observe an error as described in the next section.

### `CUDA illegal memory error: an illegal memory access at line...`:

Check the CUDA verisons with:
```
nvcc --version
```
and confirm it is the same version as the one detected by `bitsandbytes`. If not, run:
```
ls -l $CONDA_PREFIX/lib/libcudart.so
```
or
```
ls -l $LD_LIBRARY_PATH
```
Check if `libcudart.so` has a correct symlink that is set. Sometimes `nvcc` detects the correct CUDA version but `bitsandbytes` doesn't. You have to make sure that the symlink that is set for the file `libcudart.so` is redirected to the correct CUDA file.

Here is an example of a badly configured CUDA installation:

`nvcc --version` gives:

![Screenshot 2022-08-15 at 15.12.23.png](https://cdn-uploads.huggingface.co/production/uploads/1660569220888-62441d1d9fdefb55a0b7d12c.png)

which means that the detected CUDA version is 11.3 but `bitsandbytes` outputs:

![image.png](https://cdn-uploads.huggingface.co/production/uploads/1660569284243-62441d1d9fdefb55a0b7d12c.png)

First check:

```bash
echo $LD_LIBRARY_PATH
```

If this contains multiple paths separated by `:`. Then you have to make sure that the correct CUDA version is set. By doing:

```bash
ls -l $path/libcudart.so
```

On each path (`$path`) separated by `:`.
If not, simply run
```bash
ls -l $LD_LIBRARY_PATH/libcudart.so
```

and you can see","How to solve the issue of ""Parameter has no attribute .CB""?
",Use the latest version of `accelerate` with a command such as: `pip install -U accelerate`.,huggingface/transformers/blob/main/tests/quantization/bnb/README.md,4.0,"The question is about a specific error message ""Parameter has no attribute .CB"" and the context provides troubleshooting steps for other unrelated errors. However, one of the sections in the context mentions that using the latest version of `accelerate` with pip install may solve the issue. This section seems relevant to the question.

",4.0,"This question appears to be related to a specific error encountered while using the Hugging Face ecosystem, likely involving the Transformers library. The question is clear and concise, providing context about the issue with a parameter not having an attribute called "".CB"". This level of detail suggests that the question is well-researched and specifically targets a common problem area for developers working within the Hugging Face framework.

",5.0,"This question makes sense by itself, as it describes a specific error message (""Parameter has no attribute .CB"") that needs to be addressed. The context in which this error occurs is not specified, and an operator with general knowledge of programming principles would likely understand what the question is asking.

"
"### Step 2: Building the `xxxLayerBetterTransformer` module

Check that the identified module is not already copied from another module (by inspecting the source code in [`transformers`](https://github.com/huggingface/transformers) and checking that the class definition does not start with `# Copied from ...`) - and if not, create a class in `bettertransformer/models/encoder_model.py`.
Start with those lines:
```python
import torch
import torch.nn as nn

from ..base import BetterTransformerBaseLayer


class BertLayerBetterTransformer(BetterTransformerBaseLayer):
    def __init__(self, bert_layer, config):
...
```

Now, make sure to fill all the necessary attributes, the list of attributes are:

- `in_proj_weight`
- `in_proj_bias`
- `out_proj_weight`
- `out_proj_bias`
- `linear1_weight`
- `linear1_bias`
- `linear2_weight`
- `linear2_bias`
- `norm1_eps`
- `norm1_weight`
- `norm1_bias`
- `norm2_weight`
- `norm2_bias`
- `num_heads`
- `embed_dim`

Note that these attributes correspond to all the components that are necessary to run a Transformer Encoder module, check the figure 1 on the [""Attention Is All You Need""](https://arxiv.org/pdf/1706.03762.pdf) paper.

Once you filled all these attributes (sometimes the `query`, `key` and `value` layers needs to be ""contigufied"", check the [`modeling_encoder.py`](https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/encoder_models.py) file to understand more.)

Make sure also to add the lines:
```python
self.is_last_layer = False
self.validate_bettertransformer()
```


### Step 3: Building the forward pass

First of all, start with the line `super().forward_checker()`, this is needed so that the parent class can run all the safety checkers before.","What are the required attributes for a Transformer Encoder module?
","in_proj_weight, in_proj_bias, out_proj_weight, out_proj_bias, linear1_weight, linear1_bias, linear2_weight, linear2_bias, norm1_eps, norm1_weight, norm1_bias, norm2_weight, num_heads, embed_dim, is_last_layer",huggingface/optimum/blob/main/docs/source/bettertransformer/tutorials/contribute.mdx,4.0,"The context provides a clear instruction on building a Transformer Encoder module, specifically focusing on the `BertLayerBetterTransformer` class. It mentions the necessary attributes required for the module, which are listed in detail, and points to the relevant paper [""Attention Is All You Need""](https://arxiv.org/pdf/1706.03762.pdf) for understanding the components of a Transformer Encoder module. However, it does not explicitly mention how these attributes should be initialized or used in the forward pass.

",5.0,"The question is clear and specific about the requirements of a Transformer Encoder module, which is a crucial component of many NLP models. This makes it potentially very useful to machine learning developers building NLP applications with the Hugging Face ecosystem.
",5.0,"The question asks about the required attributes for a specific component (Transformer Encoder module) without referencing any particular context or implementation. This suggests that the question can be understood independently, assuming basic knowledge of the topic.
"
"## Using existing models

The pre-trained models on the Hub can be loaded with a single line of code

```py
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('model_name')
```

Here is an example that encodes sentences and then computes the distance between them for doing semantic search.

```py
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')

query_embedding = model.encode('How big is London')
passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',
                                  'London is known for its finacial district'])

print(""Similarity:"", util.dot_score(query_embedding, passage_embedding))
```

If you want to see how to load a specific model, you can click `Use in sentence-transformers` and you will be given a working snippet that you can load it!

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-sentence_transformers_snippet1.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-sentence_transformers_snippet1-dark.png""/>
</div>
<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-sentence_transformers_snippet2.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-sentence_transformers_snippet2-dark.png""/>
</div>

## Sharing your models

You can share your Sentence Transformers by using the `save_to_hub` method from a trained model.

```py
from sentence_transformers import SentenceTransformer

# Load or train a model
model.save_to_hub(""my_new_model"")
```","How do you load a pre-trained model on the Hub with one line of code?
",From the sentence_transformers module.,huggingface/hub-docs/blob/main/docs/hub/sentence-transformers.md,4.0,"The context provides a clear and concise example of how to load a pre-trained model on the Hub with one line of code, using the SentenceTransformers library. It also explains that if you want to see how to load a specific model, you can click ""Use in sentence-transformers"" and get a working snippet. However, it does not explicitly mention what the syntax for loading a pre-trained model is.

",5.0,"This question shows that the user is familiar with the Hugging Face ecosystem, specifically the Transformers library, which is great. The fact that they are looking for a concise solution (one line of code) suggests that they might be interested in productivity and efficiency, which is also relevant to machine learning development.
",5.0,"The question clearly specifies that it's about loading a pre-trained model from Hugging Face's Model Hub, but it doesn't require any additional context to understand what is being asked. The term ""Hub"" and the phrase ""pre-trained model"" are likely to be understood by someone familiar with the concept.

"
"You will find the 5 ported AllenAI models [here](https://huggingface.co/models?filter=allenai&tag=fsmt).

## More scripts

As each ported group of models has its own nuances, I made dedicated scripts to each one of them, so that it will be easy to re-build things in the future or to create new scripts to convert new models. You will find all the conversion, evaluation, and other scripts [here](https://github.com/huggingface/transformers/blob/129fdae04033fe4adfe013b734deaec6ec34ae2e/scripts/fsmt/).



### Model cards

One other important thing is that it's not enough to port a model and make it available to others. One needs to provide information on how to use it, nuances about hyper-parameters, sources of datasets, evaluation metrics, etc. This is all done by creating model cards, which is just a `README.md` file, that starts with some metadata that is used by [the models website](https://huggingface.co/models), followed by all the useful information that can be shared.

For example, let's take [the `facebook/wmt19-en-ru` model card](https://github.com/huggingface/transformers/tree/129fdae04033fe4adfe013b734deaec6ec34ae2e/model_cards/facebook/wmt19-en-ru/README.md). Here is its top:


```
---
language:
- en
- ru
thumbnail:
tags:
- translation
- wmt19
- facebook
license: apache-2.0
datasets:
- wmt19
metrics:
- bleu
---

# FSMT

## Model description

This is a ported version of
[...]
```

As you can see we define the languages, tags, license, datasets, and metrics. There is a full guide for writing these at [Model sharing and uploading](https://huggingface.co/transformers/model_sharing.html#add-a-model-card). The rest is the markdown document describing the model and its nuances.
You can also try out the models directly from the model pages thanks to the Inference widgets. For example for English-to-russian translation: https://huggingface.co/facebook/wmt19-en-ru?text=My+name+is+Diego+and+I+live+in+Moscow.","Where can I find the 5 ported AllenAI models?
",https://huggingface.co/models?filter=allenai&tag=fsmt.,huggingface/blog/blob/main/porting-fsmt.md,5.0,"The context clearly mentions that the 5 ported AllenAI models are available on a specific webpage, which is accessible by clicking on a link provided in the text. Additionally, the model cards section provides information on how to find and use the models, including links to their respective GitHub repositories.

",4.0,"This question appears to be specific and well-defined, indicating that the user has a particular use case or requirement (i.e., finding AllenAI models). The mention of ""ported"" also suggests that the user is looking for models that have been adapted to work with the Hugging Face ecosystem. To answer this question accurately, I would need to know more about the context and what specifically the user means by ""5 ported AllenAI models"". Assuming that there are indeed 5 such models available in the Hugging Face model hub or repository, a knowledgeable response could direct the user to those resources.

",4.0,"The question asks about a specific set of models (AllenAI) that need to be ported, implying they may have been previously discussed or mentioned. However, the term ""ported"" is clear and does not require additional context.

"
"### Contributors Shoutout:

No changes to highlight.

## 3.4

### New Features:

###### 1. Gallery Captions ğŸ–¼ï¸

You can now pass captions to images in the Gallery component. To do so you need to pass a {List} of (image, {str} caption) tuples. This is optional and the component also accepts just a list of the images.

Here's an example:

```python
import gradio as gr

images_with_captions = [
    (""https://images.unsplash.com/photo-1551969014-7d2c4cddf0b6"", ""Cheetah by David Groves""),
    (""https://images.unsplash.com/photo-1546182990-dffeafbe841d"", ""Lion by Francesco""),
    (""https://images.unsplash.com/photo-1561731216-c3a4d99437d5"", ""Tiger by Mike Marrah"")
    ]

with gr.Blocks() as demo:
    gr.Gallery(value=images_with_captions)

demo.launch()
```

<img src=""https://user-images.githubusercontent.com/9021060/192399521-7360b1a9-7ce0-443e-8e94-863a230a7dbe.gif"" alt=""gallery_captions"" width=""1000""/>

###### 2. Type Values into the Slider ğŸ”¢

You can now type values directly on the Slider component! Here's what it looks like:

![type-slider](https://user-images.githubusercontent.com/9021060/192399877-76b662a1-fede-4417-a932-fc15f0da7360.gif)

###### 3. Better Sketching and Inpainting ğŸ¨

We've made a lot of changes to our Image component so that it can support better sketching and inpainting.

Now supports:

- A standalone black-and-white sketch

```python
import gradio as gr
demo = gr.Interface(lambda x: x, gr.Sketchpad(), gr.Image())
demo.launch()
```

![bw](https://user-images.githubusercontent.com/9021060/192410264-b08632b5-7b2a-4f86-afb0-5760e7b474cf.gif)

- A standalone color sketch

```python
import gradio as gr
demo = gr.Interface(lambda x: x, gr.Paint(), gr.Image())
demo.launch()
```

![color-sketch](https://user-images.githubusercontent.com/9021060/192410500-3c8c3e64-a5fd-4df2-a991-f0a5cef93728.gif)

- An uploadable image with black-and-white or color sketching","What is the format required to pass captions to images in the Gallery component?
","A {List} of (image, {str} caption) tuples.",gradio-app/gradio/blob/main/CHANGELOG.md,5.0,"The context clearly explains how to pass captions to images in the Gallery component, specifying that it requires a list of tuples containing an image URL and its corresponding caption. This information is presented in a straightforward manner, making it easy to understand the required format.

",4.0,"This question appears to be specific to the usage of the Gallery component in a particular context, likely related to an application or framework that utilizes Hugging Face's models and components. The question asks for a format requirement, which suggests it may be seeking technical details about data input or output specifications.
The question's usefulness is limited because it:
- Does not provide enough context about the application or framework being used
- Focuses on a specific component within a broader ecosystem (Hugging Face)
- Asks for a very technical detail that might only be relevant to developers already working with this particular component and its requirements.

However, for someone who is deeply engaged in building NLP applications with the Hugging Face ecosystem, particularly those using components like Gallery, knowing the correct format for passing captions to images could be crucial. Therefore, while it's a narrow and somewhat context-dependent question, it still has value within that specific domain of expertise.

",5.0,"The question refers to a specific UI component (Gallery) but does not require additional context or information about the setting. It can be answered with general knowledge of how captions are passed to images in such components.

"
"```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(""stevhliu/my_awesome_eli5_mlm_model"")
>>> inputs = tokenizer(text, return_tensors=""pt"")
>>> mask_token_index = torch.where(inputs[""input_ids""] == tokenizer.mask_token_id)[1]
```

Pass your inputs to the model and return the `logits` of the masked token:

```py
>>> from transformers import AutoModelForMaskedLM

>>> model = AutoModelForMaskedLM.from_pretrained(""stevhliu/my_awesome_eli5_mlm_model"")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

Then return the three masked tokens with the highest probability and print them out:

```py
>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()

>>> for token in top_3_tokens:
...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))
The Milky Way is a spiral galaxy.
The Milky Way is a massive galaxy.
The Milky Way is a small galaxy.
```
</pt>
<tf>
Tokenize the text and return the `input_ids` as TensorFlow tensors. You'll also need to specify the position of the `<mask>` token:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(""stevhliu/my_awesome_eli5_mlm_model"")
>>> inputs = tokenizer(text, return_tensors=""tf"")
>>> mask_token_index = tf.where(inputs[""input_ids""] == tokenizer.mask_token_id)[0, 1]
```

Pass your inputs to the model and return the `logits` of the masked token:

```py
>>> from transformers import TFAutoModelForMaskedLM

>>> model = TFAutoModelForMaskedLM.from_pretrained(""stevhliu/my_awesome_eli5_mlm_model"")
>>> logits = model(**inputs).logits
>>> mask_token_logits = logits[0, mask_token_index, :]
```

Then return the three masked tokens with the highest probability and print them out:

```py
>>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()","What is the name of the pre-trained model used in the example code for masked language modeling?
","""stevhliu/my_awesome_eli5_mlm_model""",huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md,5.0,"The context provides a clear example of using the pre-trained model for masked language modeling. The model is loaded from the specific checkpoint ""stevhliu/my_awesome_eli5_mlm_model"", which suggests that this is the name of the pre-trained model used in the example code.

",4.0,"The question appears to be specific to a particular use case (masked language modeling) and assumes knowledge of the provided example code. Without more context, it's difficult to determine which pre-trained model is being referred to. However, assuming that the example code uses one of the popular models in the Hugging Face ecosystem, this question could still provide useful information for someone trying to reproduce or understand a similar example.

",5.0,"The question assumes some context about an example code, but it does not require any specific knowledge of the code itself or its context. It simply asks for a general piece of information that can be looked up.
"
"### Fixes

- [#5705](https://github.com/gradio-app/gradio/pull/5705) [`78e7cf516`](https://github.com/gradio-app/gradio/commit/78e7cf5163e8d205e8999428fce4c02dbdece25f) - ensure internal data has updated before dispatching `success` or `then` events. Thanks [@pngwn](https://github.com/pngwn)!
- [#5726](https://github.com/gradio-app/gradio/pull/5726) [`96c4b97c7`](https://github.com/gradio-app/gradio/commit/96c4b97c742311e90a87d8e8ee562c6ad765e9f0) - Adjust translation. Thanks [@ylhsieh](https://github.com/ylhsieh)!

## 1.6.1

### Patch Changes

- Updated dependencies [[`ee8eec1e5`](https://github.com/gradio-app/gradio/commit/ee8eec1e5e544a0127e0aa68c2522a7085b8ada5)]:
  - @gradio/markdown@0.2.2
  - @gradio/chatbot@0.4.1
  - @gradio/dataframe@0.2.4

## 1.6.0

### Features

- [#5639](https://github.com/gradio-app/gradio/pull/5639) [`e1874aff8`](https://github.com/gradio-app/gradio/commit/e1874aff814d13b23f3e59ef239cc13e18ad3fa7) - Add `gr.on` listener method. Thanks [@aliabid94](https://github.com/aliabid94)!
- [#5554](https://github.com/gradio-app/gradio/pull/5554) [`75ddeb390`](https://github.com/gradio-app/gradio/commit/75ddeb390d665d4484667390a97442081b49a423) - Accessibility Improvements. Thanks [@hannahblair](https://github.com/hannahblair)!

## 1.5.4

### Features

- [#5514](https://github.com/gradio-app/gradio/pull/5514) [`52f783175`](https://github.com/gradio-app/gradio/commit/52f7831751b432411e109bd41add4ab286023a8e) - refactor: Use package.json for version management. Thanks [@DarhkVoyd](https://github.com/DarhkVoyd)!

## 1.5.3

### Fixes","What was the commit hash that added support for `gr.on` listener method?
",e1874aff8,gradio-app/gradio/blob/main/js/app/CHANGELOG.md,4.0,"The context provides a list of commits, each with a hash and a description of the changes it made. One of these commits added support for the `gr.on` listener method, but its commit hash is not explicitly linked to the feature description.

",4.0,"This question seems to be very specific to a particular implementation detail of a library or framework, likely Transformers from the Hugging Face ecosystem. The mention of ""commit hash"" suggests that it's related to the development history of the project, and the `gr.on` listener method is likely part of the pipeline architecture in Transformers. A developer asking this question might be trying to troubleshoot an issue with their custom pipeline or understand how a specific feature was added.
",5.0,"This question refers to specific technical information (commit hash and a particular method) without requiring additional context, making it understandable by someone familiar with Git or Gradio documentation.
"
"!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# WavLM

## Overview

The WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,
Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,
Michael Zeng, Furu Wei.

The abstract from the paper is the following:","Who are the authors of WavLM model?
","Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.",huggingface/transformers/blob/main/docs/source/en/model_doc/wavlm.md,5.0,"The context provides a clear mention of the authors of the WavLM model in the ""Overview"" section, listing their names and affiliations.
",4.0,"This question seems to be asking about the origin or creators of a specific NLP model, which is relevant information that can help with understanding the context and potential biases of the model. The answer would likely involve citing the original research paper or publication where the WavLM model was introduced, which can provide valuable insights for developers building upon this model.
",5.0,"The question refers to a specific entity (the WavLM model), but it does not require additional context or information from outside sources. It can be answered based solely on knowledge about the model, such as documentation or articles about it.
"
"## è¿›è¡Œé¢„æµ‹

è¿›è¡Œé¢„æµ‹çš„æœ€ç®€å•æ–¹æ³•å°±æ˜¯ä½¿ç”¨é€‚å½“çš„å‚æ•°è°ƒç”¨`.predict()`æ–¹æ³•ï¼š

```js
import { client } from ""@gradio/client"";

const app = await client(""abidlabs/en2fr"");
const result = await app.predict(""/predict"", [""Hello""]);
```

å¦‚æœæœ‰å¤šä¸ªå‚æ•°ï¼Œæ‚¨åº”è¯¥å°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªæ•°ç»„ä¼ é€’ç»™`.predict()`ï¼Œåƒè¿™æ ·ï¼š

```js
import { client } from ""@gradio/client"";

const app = await client(""gradio/calculator"");
const result = await app.predict(""/predict"", [4, ""add"", 5]);
```

å¯¹äºæŸäº›è¾“å…¥ï¼Œä¾‹å¦‚å›¾åƒï¼Œæ‚¨åº”è¯¥æ ¹æ®æ‰€éœ€è¦çš„æ–¹ä¾¿ç¨‹åº¦ä¼ å…¥`Buffer`ã€`Blob`æˆ–`File`ã€‚åœ¨Node.jsä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Buffer`æˆ–`Blob`ï¼›åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­ï¼Œå¯ä»¥ä½¿ç”¨`Blob`æˆ–`File`ã€‚

```js
import { client } from ""@gradio/client"";

const response = await fetch(
	""https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3""
);
const audio_file = await response.blob();

const app = await client(""abidlabs/whisper"");
const result = await client.predict(""/predict"", [audio_file]);
```

## ä½¿ç”¨äº‹ä»¶

å¦‚æœæ‚¨ä½¿ç”¨çš„APIå¯ä»¥éšæ—¶é—´è¿”å›ç»“æœï¼Œæˆ–è€…æ‚¨å¸Œæœ›è®¿é—®æœ‰å…³ä½œä¸šçŠ¶æ€çš„ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨äº‹ä»¶æ¥å£è·å–æ›´å¤§çš„çµæ´»æ€§ã€‚è¿™å¯¹äºè¿­ä»£çš„æˆ–ç”Ÿæˆå™¨çš„ç«¯ç‚¹ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬ä¼šç”Ÿæˆä¸€ç³»åˆ—ç¦»æ•£çš„å“åº”å€¼ã€‚

```js
import { client } from ""@gradio/client"";

function log_result(payload) {
	const {
		data: [translation]
	} = payload;

	console.log(`ç¿»è¯‘ç»“æœä¸ºï¼š${translation}`);
}

const app = await client(""abidlabs/en2fr"");
const job = app.submit(""/predict"", [""Hello""]);

job.on(""data"", log_result);
```

## çŠ¶æ€

äº‹ä»¶æ¥å£è¿˜å¯ä»¥é€šè¿‡ç›‘å¬`""status""`äº‹ä»¶æ¥è·å–è¿è¡Œä½œä¸šçš„çŠ¶æ€ã€‚è¿™å°†è¿”å›ä¸€ä¸ªå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹å±æ€§ï¼š`status`ï¼ˆå½“å‰ä½œä¸šçš„äººç±»å¯è¯»çŠ¶æ€ï¼Œ`""pending"" | ""generating"" | ""complete"" | ""error""`ï¼‰ï¼Œ`code`ï¼ˆä½œä¸šçš„è¯¦ç»†gradio codeï¼‰ï¼Œ`position`ï¼ˆæ­¤ä½œä¸šåœ¨é˜Ÿåˆ—ä¸­çš„å½“å‰ä½ç½®ï¼‰ï¼Œ`queue_size`ï¼ˆæ€»é˜Ÿåˆ—å¤§å°ï¼‰ï¼Œ`eta`ï¼ˆä½œä¸šå®Œæˆçš„é¢„è®¡æ—¶é—´ï¼‰ï¼Œ`success`ï¼ˆè¡¨ç¤ºä½œä¸šæ˜¯å¦æˆåŠŸå®Œæˆçš„å¸ƒå°”å€¼ï¼‰å’Œ`time`ï¼ˆä½œä¸šçŠ¶æ€ç”Ÿæˆçš„æ—¶é—´ï¼Œæ˜¯ä¸€ä¸ª`Date`å¯¹è±¡ï¼‰ã€‚

```js
import { client } from ""@gradio/client"";

function log_status(status) {
	console.log(`æ­¤ä½œä¸šçš„å½“å‰çŠ¶æ€ä¸ºï¼š${JSON.stringify(status, null, 2)}`);
}

const app = await client(""abidlabs/en2fr"");
const job = app.submit(""/predict"", [""Hello""]);

job.on(""status"", log_status);
```

## å–æ¶ˆä½œä¸š

ä½œä¸šå®ä¾‹è¿˜å…·æœ‰`.cancel()`æ–¹æ³•ï¼Œç”¨äºå–æ¶ˆå·²æ’é˜Ÿä½†å°šæœªå¯åŠ¨çš„ä½œä¸šã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```js
import { client } from ""@gradio/client"";","What is the method to call for making predictions in @gradio/client?
",.predict(),gradio-app/gradio/blob/main/guides/cn/06_client-libraries/02_getting-started-with-the-js-client.md,5.0,"The context provided explicitly states the method to call for making predictions in @gradio/client, which is `.predict()`. It also mentions that if there are multiple parameters, they should be passed as an array. Additionally, it covers how to handle different types of input, such as images, and how to use the event interface to get more flexibility with API calls. The context also explains how to check the status of a job and cancel it.

",4.0,"This question appears to be seeking information on how to use the Gradio library, specifically the `@gradio/client` module, to make predictions. While it's a relatively specific and narrow question, understanding how to interact with the client module is crucial for utilizing Gradio in NLP applications that involve making predictions.

",5.0,"The question refers to a specific technical term (@gradio/client) but makes it clear that it's asking about a general method or process. Additionally, the context of Gradio is well-defined and not ambiguous, so an operator with access to documentation would understand what is being asked without needing additional information.

"
"and you can reload your tokenizer from that file with the
`Tokenizer.from_file`
`classmethod`:

<tokenizerslangcontent>
<python>
<literalinclude>
{""path"": ""../../bindings/python/tests/documentation/test_quicktour.py"",
""language"": ""python"",
""start-after"": ""START reload_tokenizer"",
""end-before"": ""END reload_tokenizer"",
""dedent"": 12}
</literalinclude>
</python>
<rust>
<literalinclude>
{""path"": ""../../tokenizers/tests/documentation.rs"",
""language"": ""rust"",
""start-after"": ""START quicktour_reload_tokenizer"",
""end-before"": ""END quicktour_reload_tokenizer"",
""dedent"": 4}
</literalinclude>
</rust>
<node>
<literalinclude>
{""path"": ""../../bindings/node/examples/documentation/quicktour.test.ts"",
""language"": ""js"",
""start-after"": ""START reload_tokenizer"",
""end-before"": ""END reload_tokenizer"",
""dedent"": 8}
</literalinclude>
</node>
</tokenizerslangcontent>

### Using the tokenizer

Now that we have trained a tokenizer, we can use it on any text we want
with the `Tokenizer.encode` method:

<tokenizerslangcontent>
<python>
<literalinclude>
{""path"": ""../../bindings/python/tests/documentation/test_quicktour.py"",
""language"": ""python"",
""start-after"": ""START encode"",
""end-before"": ""END encode"",
""dedent"": 8}
</literalinclude>
</python>
<rust>
<literalinclude>
{""path"": ""../../tokenizers/tests/documentation.rs"",
""language"": ""rust"",
""start-after"": ""START quicktour_encode"",
""end-before"": ""END quicktour_encode"",
""dedent"": 4}
</literalinclude>
</rust>
<node>
<literalinclude>
{""path"": ""../../bindings/node/examples/documentation/quicktour.test.ts"",
""language"": ""js"",
""start-after"": ""START encode"",
""end-before"": ""END encode"",
""dedent"": 8}
</literalinclude>
</node>
</tokenizerslangcontent>

This applied the full pipeline of the tokenizer on the text, returning
an `Encoding` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out [this page](pipeline).","How can you reload a trained tokenizer in Python?
",With the Tokenizer.from_file classmethod.,huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx,5.0,"The provided context explicitly mentions that a trained tokenizer can be reloaded from a file using the `Tokenizer.from_file` class method. This method is specifically shown in one of the code examples in the context, which makes it clear how to reload a trained tokenizer.
",5.0,"This question is extremely useful because it shows an understanding of how to interact with pre-trained models in the Hugging Face ecosystem, which is a key aspect of NLP development. The ability to reload a trained tokenizer is crucial for applications where model performance needs to be fine-tuned or updated without having to retrain the entire model from scratch.
",5.0,"This question does not depend on any additional information or context, as it is a clear and concise inquiry about a specific task related to tokenizers in Python.
"
"sim = einsum(""b h d i, b h d j -> b h i j"", q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)

        out = einsum(""b h i j, b h d j -> b h i d"", attn, v)
        out = rearrange(out, ""b h (x y) d -> b (h d) x y"", x=h, y=w)
        return self.to_out(out)

class LinearAttention(nn.Module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        hidden_dim = dim_head * heads
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)

        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),
                                    nn.GroupNorm(1, dim))

    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=1)
        q, k, v = map(
            lambda t: rearrange(t, ""b (h c) x y -> b h c (x y)"", h=self.heads), qkv
        )

        q = q.softmax(dim=-2)
        k = k.softmax(dim=-1)

        q = q * self.scale
        context = torch.einsum(""b h d n, b h e n -> b h d e"", k, v)

        out = torch.einsum(""b h d e, b h d n -> b h e n"", context, q)
        out = rearrange(out, ""b h c (x y) -> b (h c) x y"", h=self.heads, x=h, y=w)
        return self.to_out(out)
```

### Group normalization

The DDPM authors interleave the convolutional/attention layers of the U-Net with group normalization ([Wu et al., 2018](https://arxiv.org/abs/1803.08494)). Below, we define a `PreNorm` class, which will be used to apply groupnorm before the attention layer, as we'll see further. Note that there's been a [debate](https://tnq177.github.io/data/transformers_without_tears.pdf) about whether to apply normalization before or after attention in Transformers.

```python
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.fn = fn
        self.norm = nn.GroupNorm(1, dim)","What is the dimension used for group normalization in the `PreNorm` class?
",1,huggingface/blog/blob/main/annotated-diffusion.md,4.0,"The context describes a `PreNorm` class that applies group normalization before an attention layer. However, the specific dimension used for group normalization in the `PreNorm` class is not explicitly mentioned in the code or context. Nevertheless, we can infer from the code that group normalization is applied to the same dimension as the input data, which is `dim`. This makes it unambiguous how to determine the correct dimension.

",4.0,"This question is quite specific to the Hugging Face ecosystem, specifically asking about a particular implementation detail of the `PreNorm` class. It suggests that the person asking this question is familiar with the library and is looking for information on how to use it correctly.
The fact that they're asking about group normalization dimension suggests they want to understand how to apply this technique in their NLP model, which indicates a practical goal rather than just theoretical curiosity.

",5.0,"The question clearly asks about a specific concept within the context of a particular code or framework (likely referring to some machine learning implementation), but it does not reference any external, unspecified information. It directly refers to the `PreNorm` class and a specific operation within that class (group normalization) without implying any external context other than what would be typically understood from the question itself.
"
"1. **[KOSMOS-2](https://huggingface.co/docs/transformers/model_doc/kosmos-2)** (from Microsoft Research Asia) released with the paper [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.
1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (æ¥è‡ª Microsoft Research Asia) ä¼´éšè®ºæ–‡ [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) ç”± Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou å‘å¸ƒã€‚
1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (æ¥è‡ª Microsoft Research Asia) ä¼´éšè®ºæ–‡ [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) ç”± Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou å‘å¸ƒã€‚
1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (æ¥è‡ª Microsoft Research Asia) ä¼´éšè®ºæ–‡ [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) ç”± Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei å‘å¸ƒã€‚
1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (æ¥è‡ª Microsoft Research Asia) ä¼´éšè®ºæ–‡ [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) ç”± Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei å‘å¸ƒã€‚
1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (æ¥è‡ª AllenAI) ä¼´éšè®ºæ–‡ [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) ç”± Iz Beltagy, Matthew E. Peters, Arman Cohan å‘å¸ƒã€‚","Who are the authors of the paper ""Kosmos-2: Grounding Multimodal Large Language Models to the World""?
","Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.",huggingface/transformers/blob/main/README_zh-hans.md,5.0,"The context provides a list of papers and their corresponding authors from Microsoft Research Asia. Among the listed papers, only ""Kosmos-2: Grounding Multimodal Large Language Models to the World"" explicitly mentions its authors as Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Therefore, it is clear that one can unambiguously answer the question by referring to this specific paper in the context.

",4.0,"This question appears to be a straightforward query about the authors of a specific research paper. The Hugging Face ecosystem is deeply rooted in NLP and ML, and knowledge about recent papers and their authors can be useful for staying up-to-date with advancements in the field.
",5.0,"This question can be understood without any additional context or information about a specific document, database, or setting. It refers to a well-defined entity (a research paper) and asks for a factual piece of information (the authors' names), which should be available through standard academic sources.

"
"### 7. What is pre-tokenization for a subword tokenizer?

<Question
	choices={[
		{
			text: ""It's the step before the tokenization, where data augmentation (like random masking) is applied."",
			explain: ""No, that step is part of the preprocessing.""
		},
		{
			text: ""It's the step before the tokenization, where the desired cleanup operations are applied to the text."",
			explain: ""No, that's the normalization step.""
		},
		{
			text: ""It's the step before the tokenizer model is applied, to split the input into words."",
			explain: ""That's the correct answer!"",
			correct: true
		},
        {
			text: ""It's the step before the tokenizer model is applied, to split the input into tokens."",
			explain: ""No, splitting into tokens is the job of the tokenizer model.""
		}
	]}
/>

### 8. Select the sentences that apply to the BPE model of tokenization.","What is pre-tokenization for a subword tokenizer?
","It's the step before the tokenizer model is applied, to split the input into words.",huggingface/course/blob/main/chapters/en/chapter6/10.mdx,5.0,"The context clearly states what pre-tokenization for a subword tokenizer entails, which is the step before the tokenizer model is applied, where the input is split into words.
",5.0,"Pre-tokenization is a crucial step in text processing that involves breaking down input text into individual words or tokens before applying a subword tokenizer. This step helps to remove punctuation, handle out-of-vocabulary (OOV) words, and improve the overall performance of language models. The question seems to be looking for an explanation of what pre-tokenization is specifically for subword tokenizers in the context of NLP, which is a useful concept for machine learning developers building applications with the Hugging Face ecosystem.

",5.0,"This question is clear and concise, asking about a specific concept related to tokenizers. The term ""pre-tokenization"" provides context that is sufficient for understanding what is being asked, without relying on additional information or a particular setting.
"
"## Semantic Guidance (SEGA)

[Paper](https://arxiv.org/abs/2301.12247)

[SEGA](../api/pipelines/semantic_stable_diffusion) allows applying or removing one or more concepts from an image. The strength of the concept can also be controlled. I.e. the smile concept can be used to incrementally increase or decrease the smile of a portrait.

Similar to how classifier free guidance provides guidance via empty prompt inputs, SEGA provides guidance on conceptual prompts. Multiple of these conceptual prompts can be applied simultaneously. Each conceptual prompt can either add or remove their concept depending on if the guidance is applied positively or negatively.

Unlike Pix2Pix Zero or Attend and Excite, SEGA directly interacts with the diffusion process instead of performing any explicit gradient-based optimization.

## Self-attention Guidance (SAG)

[Paper](https://arxiv.org/abs/2210.00939)

[Self-attention Guidance](../api/pipelines/self_attention_guidance) improves the general quality of images.

SAG provides guidance from predictions not conditioned on high-frequency details to fully conditioned images. The high frequency details are extracted out of the UNet self-attention maps.

## Depth2Image

[Project](https://huggingface.co/stabilityai/stable-diffusion-2-depth)

[Depth2Image](../api/pipelines/stable_diffusion/depth2img) is fine-tuned from Stable Diffusion to better preserve semantics for text guided image variation.

It conditions on a monocular depth estimate of the original image.

## MultiDiffusion Panorama

[Paper](https://arxiv.org/abs/2302.08113)","Does SEGA directly interact with the diffusion process instead of performing any explicit gradient-based optimization?
",Yes,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlling_generation.md,5.0,"The context provides information about SEGA, specifically that it ""directly interacts with the diffusion process"" and does not perform explicit gradient-based optimization, unlike other models like Pix2Pix Zero or Attend and Excite.

",4.0,"This question appears to be relevant to developers working on NLP applications within the Hugging Face ecosystem, specifically those interested in the details of the Stable Diffusion model. The question seems to be asking for clarification on how SEGA (Segmented Evolutionary Gradient Acceleration) interacts with the diffusion process in this context, which could help them understand the underlying mechanics and potentially optimize their use of the model.
",5.0,"The question clearly refers to a specific context, likely related to a model or algorithm involving SEGA and diffusion processes. However, it does not rely on external information beyond this context; the terminology used is precise enough that an operator with access to documentation should understand what is being asked.

"
"<a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg""
       href=""./integrations"">
      <div class=""w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">
        Integrate a library
      </div><p class=""text-gray-700"">
        What does it mean to integrate a library with the Hub? And how to do it?
      </p>
    </a>

    <a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg""
       href=""./webhooks_server"">
      <div class=""w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">
        Webhooks server
      </div><p class=""text-gray-700"">
        How to create a server to receive Webhooks and deploy it as a Space?
      </p>
    </a>

  </div>
</div>","What does it mean to integrate a library with the Hub?
",It means combining a library with the Hub.,huggingface/huggingface_hub/blob/main/docs/source/en/guides/overview.md,4.0,"The context provides two clear examples of how to integrate with the Hub, but does not explicitly state what it means to integrate a library. However, given that one of the links is labeled ""Integrate a library"" and describes integration in general terms, we can infer that integrating a library means setting up a connection between the library and the Hub, allowing for data exchange or other interactions.

",4.0,"This question appears to be useful because it pertains to the Hugging Face ecosystem, specifically the concept of integrating libraries with their Hub. The Hub is a key component in the Hugging Face framework for sharing and reusing pre-trained models and datasets. Understanding how to integrate a library with the Hub can help developers leverage these shared resources more effectively.

 ",4.0,"This question assumes some prior knowledge of what the Hub refers to, but it's clear that the question is asking about a general concept related to integrating libraries. The term ""Hub"" might be unfamiliar to someone without context, but the phrase ""integrate a library"" suggests a technical or programming-related topic.

"
"The abstract from the paper is:

*We present the vector quantized diffusion (VQ-Diffusion) model for text-to-image generation. This method is based on a vector quantized variational autoencoder (VQ-VAE) whose latent space is modeled by a conditional variant of the recently developed Denoising Diffusion Probabilistic Model (DDPM). We find that this latent-space method is well-suited for text-to-image generation tasks because it not only eliminates the unidirectional bias with existing methods but also allows us to incorporate a mask-and-replace diffusion strategy to avoid the accumulation of errors, which is a serious problem with existing methods. Our experiments show that the VQ-Diffusion produces significantly better text-to-image generation results when compared with conventional autoregressive (AR) models with similar numbers of parameters. Compared with previous GAN-based text-to-image methods, our VQ-Diffusion can handle more complex scenes and improve the synthesized image quality by a large margin. Finally, we show that the image generation computation in our method can be made highly efficient by reparameterization. With traditional AR methods, the text-to-image generation time increases linearly with the output image resolution and hence is quite time consuming even for normal size images. The VQ-Diffusion allows us to achieve a better trade-off between quality and speed. Our experiments indicate that the VQ-Diffusion model with the reparameterization is fifteen times faster than traditional AR methods while achieving a better image quality.*

## VQDiffusionScheduler
[[autodoc]] VQDiffusionScheduler

## VQDiffusionSchedulerOutput
[[autodoc]] schedulers.scheduling_vq_diffusion.VQDiffusionSchedulerOutput","How many times faster is the VQ-Diffusion model with reparameterization compared to traditional AR methods?
",Fifteen.,huggingface/diffusers/blob/main/docs/source/en/api/schedulers/vq_diffusion.md,5.0,"The context provides information about the performance of the VQ-Diffusion model with reparameterization compared to traditional AR methods, stating that it is ""fifteen times faster"" while achieving better image quality. This specific comparison is mentioned in the abstract and experiments section, making it a clear and unambiguous piece of information.

",4.0,"This question appears to be evaluating a specific performance metric of a particular NLP model (VQ-Diffusion) in comparison to traditional AR methods. While it may not provide insight into how to implement or use the Hugging Face ecosystem, it could potentially inform decisions about which models to use for certain tasks, depending on performance requirements.

",5.0,"This question appears to be self-contained, as it mentions a specific model (VQ-Diffusion) and a comparison method (traditional AR methods), without referencing any external context. The term ""reparameterization"" is likely referring to a technique used in the VQ-Diffusion model, which suggests that the question is focused on the model's performance rather than an external setting or document.

"
"### Callbacks

Another option for customizing the [`Trainer`] is to use [callbacks](callbacks). Callbacks *don't change* anything in the training loop. They inspect the training loop state and then execute some action (early stopping, logging results, etc.) depending on the state. In other words, a callback can't be used to implement something like a custom loss function and you'll need to subclass and override the [`~Trainer.compute_loss`] method for that.

For example, if you want to add an early stopping callback to the training loop after 10 steps.

```py
from transformers import TrainerCallback

class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, num_steps=10):
        self.num_steps = num_steps

    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step >= self.num_steps:
            return {""should_training_stop"": True}
        else:
            return {}
```

Then pass it to the [`Trainer`]'s `callback` parameter.

```py
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset[""train""],
    eval_dataset=dataset[""test""],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callback=[EarlyStoppingCallback()],
)
```

## Logging

<Tip>

Check out the [logging](./main_classes/logging) API reference for more information about the different logging levels.

</Tip>","What is the purpose of a callback in the training loop?
",It inspects the training loop state and then executes some action depending on the state.,huggingface/transformers/blob/main/docs/source/en/trainer.md,5.0,"The context clearly explains that callbacks are used to inspect the training loop state and execute actions based on it, but they don't change anything in the training loop. It provides an example of implementing an early stopping callback, which answers what a callback is used for.

",5.0,"This question is extremely useful because understanding the concept of callbacks is crucial when building, customizing, or fine-tuning models using the Hugging Face ecosystem. Callbacks allow developers to extend the functionality of their training loops without modifying the underlying code, enabling tasks such as tracking model performance over time, implementing early stopping conditions based on validation metrics, and saving checkpoints at key intervals.

",4.0,"The question refers to a specific concept (callback) within a larger framework (training loop), which might be unfamiliar without prior knowledge. However, the term ""callback"" and its general purpose are widely used and understood across various programming contexts. Additionally, the phrase ""in the training loop"" provides a clear idea of the context in which this question is being asked.

"
"This model was contributed by [anton-l](https://huggingface.co/anton-l).

## Usage tips

- SEW-D is a speech model that accepts a float array corresponding to the raw waveform of the speech signal.
- SEWDForCTC is fine-tuned using connectionist temporal classification (CTC) so the model output has to be decoded
  using [`Wav2Vec2CTCTokenizer`].

## Resources

- [Audio classification task guide](../tasks/audio_classification)
- [Automatic speech recognition task guide](../tasks/asr)

## SEWDConfig

[[autodoc]] SEWDConfig

## SEWDModel

[[autodoc]] SEWDModel
    - forward

## SEWDForCTC

[[autodoc]] SEWDForCTC
    - forward

## SEWDForSequenceClassification

[[autodoc]] SEWDForSequenceClassification
    - forward","Who contributed this model to the Hugging Face model hub?
",anton-l,huggingface/transformers/blob/main/docs/source/en/model_doc/sew-d.md,5.0,"The context explicitly mentions that the model was ""contributed by [anton-l](https://huggingface.co/anton-l)"", which directly answers the question. There is no ambiguity or need for inference, making it clear and unambiguous.
",4.0,"This question is useful because it shows that the person asking has taken an interest in understanding the origins of a specific model, which suggests they are interested in learning more about how NLP models are developed and shared within the Hugging Face ecosystem. The answer to this question could potentially provide valuable context for anyone trying to understand or reproduce the results from this particular model.
",5.0,"This question can be understood without any additional context or information, as it directly asks for the contributor of a specific model on the Hugging Face model hub.
"
"For once, we should take a good look at the warning here. We can see
that two weights corresponding to a `""cls""` layer were not used. This
should not be a problem because we don\'t need BERT\'s CLS layer for
*sequence-to-sequence* tasks. Also, we notice that a lot of weights are
\""newly\"" or randomly initialized. When taking a closer look these
weights all correspond to the cross-attention layer, which is exactly
what we would expect after having read the theory above.

Let\'s take a closer look at the model.

```python
bert2bert
```","What type of tasks do we not need BERT's CLS layer for?
",Sequence-to-sequence tasks.,huggingface/blog/blob/main/warm-starting-encoder-decoder.md,5.0,"The context clearly states that BERT's CLS (Classification) layer is not needed for sequence-to-sequence tasks, which makes it unambiguous to determine the type of tasks where CLS is not necessary.
",4.0,"This question appears to be relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it involves understanding the role of BERT's CLS (Classification) layer in NLP tasks. The CLS layer is typically used as a pooling mechanism for sentence classification tasks such as sentiment analysis and question answering. However, there might be certain types of tasks where the CLS layer is not necessary or even counterproductive.

",5.0,"This question can be understood without any additional context, as it refers to a specific concept (BERT's CLS layer) and asks about its usage in relation to certain tasks. The answer would likely involve discussing the role of the CLS layer in BERT models and identifying the types of tasks where it is not necessary.

"
"1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.
1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.
1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.
1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.
1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, HervÃ© JÃ©gou, Matthijs Douze.
1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.","Who is one of the authors of the paper ""LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking""?
",Yupan Huang.,huggingface/transformers/blob/main/README_pt-br.md,5.0,"The context provides a list of papers and their corresponding authors. However, the paper ""LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking"" is mentioned as one of the papers in the list, but the list does not explicitly state its authors. Nevertheless, we can infer that the authors are likely to be the same as those listed under the corresponding paper.

",4.0,"This question appears to be useful because it requires knowledge about a specific research paper in the NLP field, which suggests that the person asking this question has some background in NLP or is interested in learning more about the latest advancements in the field. The Hugging Face ecosystem often deals with pre-training models for various tasks, including document AI, so understanding who contributed to papers like ""LayoutLMv3"" can be valuable information for developers working on related projects.
",5.0,"The question makes sense without additional information, as it references a specific paper and asks for a piece of information that can be looked up or recalled from general knowledge about the authors of scientific papers.

"
"<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-argilla-embed-space.png""/>
</div>

<Tip>
Argilla Datasets cannot be uploaded directly from the UI. Most Argilla users upload datasets programmatically using the argilla Python library but you can also use Argilla Data Manager, a simple Streamlit app.
</Tip>

For uploading Argilla datasets, there are two options:

1. You can use the **argilla Python library** inside Jupyter, Colab, VS Code, or other Python IDE. In this case, you will read read your source file (`csv`, `json`, etc.) and transform it into Argilla records. We recommend to read the [basics guide](https://docs.argilla.io/en/latest/guides/how_to.html).
2. You can use the **[no-code data manager app](https://huggingface.co/spaces/argilla/data-manager)** to upload a file and log it into Argilla. If you need to transform your dataset before uploading it into Argilla, we recommend the first option.

To follow a complete tutorial with Colab or Jupyter, [check this tutorial](https://docs.argilla.io/en/latest/tutorials/notebooks/training-textclassification-setfit-fewshot.html). For a quick step-by-step example using the `argilla` Python library, keep reading.

First, you need to open a Python IDE, we highly recommend using Jupyter notebooks or Colab.

Second, you need to `pip` install `datasets` and `argilla` on Colab or your local machine:

```bash
pip install datasets argilla
```

Third, you need to read the dataset using the `datasets` library. For reading other file types, check the [basics guide](https://docs.argilla.io/en/latest/guides/how_to.html).

```python
from datasets import load_dataset

dataset = load_dataset(""dvilasuero/banking_app"", split=""train"").shuffle()
```

Fourth, you need to init the `argilla` client with your Space URL and API key and upload the records into Argilla:

```python
import argilla as rg
from datasets import load_dataset","How can Argilla users upload datasets programmatically.
",using the argilla Python library.,huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-argilla.md,5.0,"The context clearly explains that Argilla users can upload datasets programmatically using either the 'argilla Python library' or 'Argilla Data Manager', a Streamlit app. It also provides step-by-step instructions on how to do so, including installing necessary libraries and writing code to read and transform the dataset before uploading it into Argilla.

",4.0,"This question appears to be specific to the Argilla library, which is a part of the Hugging Face ecosystem. The user is asking about uploading datasets programmatically, which suggests they are looking for an automated way to add data to their models. Given that Argilla is focused on making it easier to work with datasets and models in the Hugging Face ecosystem, this question seems reasonably relevant.
",5.0,"This question is clear and unambiguous, and it requires no additional context to understand what it's asking. The term ""Argilla"" suggests a specific software or platform, but the use of ""users"" and the request for a programming-based solution implies that the answer will be technical and generalizable.

"
"|      |      |[valurank/distilroberta-propaganda-2class](https://huggingface.co/valurank/distilroberta-propaganda-2class)                                        |209         |3       |                         |                                                                                   |[LICENSE](https://huggingface.co/valurank/distilroberta-propaganda-2class/blob/main/LICENSE)                            |                                                                                                    |             |
|      |      |[Neko-Institute-of-Science/LLaMA-7B-HF](https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF)                                              |207         |22      |llama-license            |https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF/blob/main/LICENSE     |[LICENSE](https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF/blob/main/LICENSE)                               |                                                                                                    |             |
|      |      |[valurank/distilroberta-bias](https://huggingface.co/valurank/distilroberta-bias)                                                                  |201         |1       |                         |                                                                                   |[LICENSE](https://huggingface.co/valurank/distilroberta-bias/blob/main/LICENSE)                                         |                                                                                                    |             |","What is the license of the model ""Neko-Institute-of-Science/LLaMA-7B-HF""?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,4.0,"The context provides information about the license of two different models, including ""Neko-Institute-of-Science/LLaMA-7B-HF"", which is explicitly mentioned as being licensed under a ""llama-license"". However, there is no additional information provided about what this license entails or where it can be found. Nonetheless, the fact that the model's license is explicitly stated makes it relatively easy to determine its license.

",4.0,"This question is useful because it asks about a specific model's licensing information, which can be crucial for developers to know before using or deploying the model in their applications. The Hugging Face ecosystem provides access to various models under different licenses, and understanding these licenses is essential for ensuring compliance with legal requirements.
",5.0,"The question clearly specifies a specific model, its organization (Neko-Institute-of-Science), and a reference to Hugging Face (HF) for context. This information is sufficient to identify the intended model license without any ambiguity.
"
"!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# XLSR-Wav2Vec2

## Overview

The XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael
Auli.

The abstract from the paper is the following:","Who proposed the XLSR-Wav2Vec2 model?
",Alexis Conneau.,huggingface/transformers/blob/main/docs/source/en/model_doc/xlsr_wav2vec2.md,5.0,"The context clearly mentions that the XLSR-Wav2Vec2 model was proposed in a specific research paper and provides the authors' names as Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. This information is readily available within the given context.

",4.0,"This question seems to be specific and targeted towards a particular NLP model, which is part of the Hugging Face ecosystem. The fact that it asks for the author/proposer of the model suggests that it might be useful for users who want to understand the background or context behind a certain model. However, without more information about the question (e.g., what XLSR-Wav2Vec2 is used for), it's difficult to evaluate its overall usefulness.
",5.0,"This question can be understood independently without any additional context, as it refers to a specific model (XLSR-Wav2Vec2) and asks for its originator. The mention of ""proposed"" is sufficient to understand that the question seeks information about the creator or author of the model.
"
"From the abstract of the XLM-V paper:

*Large multilingual language models typically rely on a single vocabulary shared across 100+ languages.
As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged.
This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R.
In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by
de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity
to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically
more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V,
a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we
tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), and
named entity recognition (WikiAnn) to low-resource tasks (Americas NLI, MasakhaNER).*

This model was contributed by [stefan-it](https://huggingface.co/stefan-it), including detailed experiments with XLM-V on downstream tasks.
The experiments repository can be found [here](https://github.com/stefan-it/xlm-v-experiments).

## Usage tips

- XLM-V is compatible with the XLM-RoBERTa model architecture, only model weights from [`fairseq`](https://github.com/facebookresearch/fairseq)
  library had to be converted.
- The `XLMTokenizer` implementation is used to load the vocab and performs tokenization.

A XLM-V (base size) model is available under the [`facebook/xlm-v-base`](https://huggingface.co/facebook/xlm-v-base) identifier.

<Tip>

XLM-V architecture is the same as XLM-RoBERTa, refer to [XLM-RoBERTa documentation](xlm-roberta) for API reference, and examples.
</Tip>","What is the size of the vocabulary in the XLM-V model?
",one million tokens.,huggingface/transformers/blob/main/docs/source/en/model_doc/xlm-v.md,5.0,"The context provides specific information about the vocabulary size of the XLM-V model, stating that it has a one million token vocabulary. Additionally, the usage tips and provided links to resources support this information. However, there is no mention of the original XLM-R model in the given context.
",4.0,"This question seems to be about querying the specific attributes or parameters of a particular Hugging Face model (XLM-V), which could potentially be useful for developers who need to integrate this model into their NLP pipeline. The size of the vocabulary is an important consideration in many NLP tasks, as it affects the number of possible inputs and outputs.

",4.0,"The question clearly mentions a specific model (XLM-V), which implies that the model's properties are being referred to. However, without any additional context or information about the specific version or configuration of the model, it is still possible to understand what is being asked: the size of its vocabulary.

"
"To get the top-5 predictions class names:
```python
# Get imagenet class mappings
url, filename = (""https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"", ""imagenet_classes.txt"")
urllib.request.urlretrieve(url, filename)
with open(""imagenet_classes.txt"", ""r"") as f:
    categories = [s.strip() for s in f.readlines()]

# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]
```

Replace the model name with the variant you want to use, e.g. `swsl_resnet18`. You can find the IDs in the model summaries at the top of this page.

To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.

## How do I finetune this model?
You can finetune any of the pre-trained models just by changing the classifier (the last layer).
```python
model = timm.create_model('swsl_resnet18', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)
```
To finetune on your own dataset, you have to write a training loop or adapt [timm's training
script](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.

## How do I train this model?

You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.

## Citation","What is the URL used to retrieve imagenet class mappings?
",https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt,huggingface/pytorch-image-models/blob/main/docs/models/swsl-resnet.md,4.0,"The context provides information on how to retrieve imagenet class mappings from a URL, which is used in conjunction with top-5 predictions. However, it does not explicitly state the URL used to retrieve imagenet class mappings; instead, it uses a hardcoded value ""https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"". This suggests that the question might be answerable but requires some inference based on the given code snippet.
",4.0,"This question is useful because it requires knowledge of a specific detail within the Hugging Face ecosystem, which suggests that the person asking has some familiarity with the tools and wants to dive deeper into its functionality. The answer should provide clarity on how to access imagenet class mappings, which could be important for tasks like image classification or object detection.
",5.0,"The question clearly references a specific task (retrieving imagenet class mappings) and a particular dataset (imagenet), but it does not rely on any additional context beyond this reference. It directly asks for a URL, which is a well-defined piece of information that can be looked up independently.

"
"```bash
huggingface-cli login
```

Now we can start training!

```bash
accelerate launch train_dreambooth_lora.py \
  --pretrained_model_name_or_path=$MODEL_NAME  \
  --instance_data_dir=$INSTANCE_DIR \
  --output_dir=$OUTPUT_DIR \
  --instance_prompt=""a photo of sks dog"" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=1 \
  --checkpointing_steps=100 \
  --learning_rate=1e-4 \
  --report_to=""wandb"" \
  --lr_scheduler=""constant"" \
  --lr_warmup_steps=0 \
  --max_train_steps=500 \
  --validation_prompt=""A photo of sks dog in a bucket"" \
  --validation_epochs=50 \
  --seed=""0"" \
  --push_to_hub
```

**___Note: When using LoRA we can use a much higher learning rate compared to vanilla dreambooth. Here we
use *1e-4* instead of the usual *2e-6*.___**

The final LoRA embedding weights have been uploaded to [patrickvonplaten/lora_dreambooth_dog_example](https://huggingface.co/patrickvonplaten/lora_dreambooth_dog_example). **___Note: [The final weights](https://huggingface.co/patrickvonplaten/lora/blob/main/pytorch_attn_procs.bin) are only 3 MB in size which is orders of magnitudes smaller than the original model.**

The training results are summarized [here](https://api.wandb.ai/report/patrickvonplaten/xm6cd5q5).
You can use the `Step` slider to see how the model learned the features of our subject while the model trained.

Optionally, we can also train additional LoRA layers for the text encoder. Specify the `--train_text_encoder` argument above for that. If you're interested to know more about how we
enable this support, check out this [PR](https://github.com/huggingface/diffusers/pull/2918).

With the default hyperparameters from the above, the training seems to go in a positive direction. Check out [this panel](https://wandb.ai/sayakpaul/dreambooth-lora/reports/test-23-04-17-17-00-13---Vmlldzo0MDkwNjMy). The trained LoRA layers are available [here](https://huggingface.co/sayakpaul/dreambooth).


### Inference","What is the learning rate used in this DreamBooth Lora example?
",1e-4,huggingface/diffusers/blob/main/examples/dreambooth/README.md,5.0,"The context provides explicit information about the learning rate used in the DreamBooth Lora example, specifically stating that ""Here we use *1e-4* instead of the usual *2e-6*"". This makes it clear and unambiguous that the learning rate used is 1e-4.

",4.0,"The question is relatively specific and asks about a particular aspect of a pre-existing example, which suggests that it may be asking for a detail that is not necessarily documented or clearly mentioned. However, being able to identify the learning rate used in an example can be important for reproduction and modification of the model.
",5.0,"The question makes sense without any additional context, and it refers to a specific technical term (""DreamBooth Lora""), but it's still clear what information is being requested.
"
"The abstract from the paper is the following:

*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document
Understanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a
large number of digital documents where the layout information is not fixed and needs to be interactively and
dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this
paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as
HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the
pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding
tasks. The pre-trained model and code will be publicly available.*

This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/markuplm).

## Usage tips

- In addition to `input_ids`, [`~MarkupLMModel.forward`] expects 2 additional inputs, namely `xpath_tags_seq` and `xpath_subs_seq`.
These are the XPATH tags and subscripts respectively for each token in the input sequence.
- One can use [`MarkupLMProcessor`] to prepare all data for the model. Refer to the [usage guide](#usage-markuplmprocessor) for more info.

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg""
alt=""drawing"" width=""600""/>

<small> MarkupLM architecture. Taken from the <a href=""https://arxiv.org/abs/2110.08518"">original paper.</a> </small>

## Usage: MarkupLMProcessor","What are the additional inputs expected by ~MarkupLMModel.forward besides input_ids?
",xpath_tags_seq and xpath_subs_seq.,huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md,5.0,"The context clearly states that besides input_ids, ~MarkupLMModel.forward expects two additional inputs: xpath_tags_seq and xpath_subs_seq. This information is explicitly mentioned in the usage tips section of the documentation.

",5.0,"The question seems to be asking about a specific detail of the MarkupLMModel, which suggests that the user has some familiarity with Hugging Face's Transformers library and its ecosystem. This makes me think that they are likely looking for information on how to use the model effectively or troubleshoot an issue related to the input format.

The fact that they are specifically asking about additional inputs besides input_ids implies that they have already done some research and understand the basics of working with Hugging Face models. Therefore, their question is not a simple ""how-to"" query but rather a more nuanced inquiry that requires knowledge of the underlying model architecture.

",4.0,"The question clearly refers to a specific class (~MarkupLMModel) within the context of the Transformers library, but it still makes sense without any external knowledge or information about the specific model's implementation. It assumes some basic understanding of how forward methods typically work in PyTorch and what input_ids are, but it does not require any additional context beyond that.
"
"```python
accelerator.backward(loss)
```

This last line adds the necessary steps for the backward pass (mostly for mixed precision but other integrations will require some custom behavior here).

### What about evaluation?

Evaluation can either be run normally on all processes, or if you just want it to run on the main process, you can use the handy test:

```python
if accelerator.is_main_process():
    # Evaluation loop
```

But you can also very easily run a distributed evaluation using Accelerate, here is what you would need to add to your evaluation loop:

```diff
+ eval_dataloader = accelerator.prepare(eval_dataloader)
  predictions, labels = [], []
  for source, targets in eval_dataloader:
      with torch.no_grad():
          output = model(source)

-     predictions.append(output.cpu().numpy())
-     labels.append(targets.cpu().numpy())
+     predictions.append(accelerator.gather(output).cpu().numpy())
+     labels.append(accelerator.gather(targets).cpu().numpy())

  predictions = np.concatenate(predictions)
  labels = np.concatenate(labels)

+ predictions = predictions[:len(eval_dataloader.dataset)]
+ labels = label[:len(eval_dataloader.dataset)]

  metric_compute(predictions, labels)
```

Like for the training, you need to add one line to prepare your evaluation dataloader. Then you can just use `accelerator.gather` to gather across processes the tensors of predictions and labels. The last line to add truncates the predictions and labels to the number of examples in your dataset because the prepared evaluation dataloader will return a few more elements to make sure batches all have the same size on each process.

### One launcher to rule them all","What is added by the line `accelerator.backward(loss)`?
",The necessary steps for the backward pass.,huggingface/blog/blob/main/accelerate-library.md,4.0,"This question is about understanding what the line `accelerator.backward(loss)` does in the context of mixed precision training. The provided code snippet gives some hints, but it's not entirely clear from this snippet alone how exactly the backward pass works.

",4.0,"This question seems to be about understanding the role of a specific line of code in the Hugging Face ecosystem, specifically related to acceleration and backpropagation. The answer would likely involve explaining how this line contributes to training a model with accelerators like GPUs or TPUs.
",5.0,"The question is clear and can be understood without any additional context. It specifically asks about the functionality of a particular line of code, making it self-contained.
"
"|      |                                                                            |[4bit/llama-13b-4bit-hf](https://huggingface.co/4bit/llama-13b-4bit-hf)|5           |2                        |llama-license                                                                                 |https://huggingface.co/4bit/llama-13b-4bit-hf/blob/main/LICENSE                               |[LICENSE](https://huggingface.co/4bit/llama-13b-4bit-hf/blob/main/LICENSE)                         |                                                                                                                     |                                                                                   |
|      |                                                                            |[gsaivinay/Llama-2-7b-Chat-GPTQ](https://huggingface.co/gsaivinay/Llama-2-7b-Chat-GPTQ)|5           |1                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE](https://huggingface.co/gsaivinay/Llama-2-7b-Chat-GPTQ/blob/main/LICENSE)                 |                                                                                                                     |                                                                                   |","What is the name of the license associated with the model ""4bit/llama-13b-4bit-hf""?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provided is a table listing models and their associated licenses. It explicitly mentions the license for the model ""4bit/llama-13b-4bit-hf"" as ""llama-license"", along with a link to the actual LICENSE file hosted on Hugging Face. This clear association between the model name and its license allows us to easily identify the answer.
",4.0,"This question appears to be related to checking the licensing information for a specific Hugging Face model, which is an important aspect of responsible AI development. The question seems to be well-defined and relevant to NLP developers who want to use or build upon existing models.

",4.0,"The question refers to a specific model, which implies that it might be necessary to know more about this particular model or its documentation to fully understand what is being asked. However, since the question explicitly mentions the name of the model and asks for information that can be looked up independently (the associated license), it still makes sense without requiring additional context beyond general knowledge.

"
"# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}

The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in [Chapter 1](/course/chapter1), this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it's imperative to be able to share and reuse models that have already been trained.

Loading a Transformer model that is already trained is simple â€” we can do this using the `from_pretrained()` method:

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained(""bert-base-cased"")
```

As you saw earlier, we could replace `BertModel` with the equivalent `AutoModel` class. We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained(""bert-base-cased"")
```

As you saw earlier, we could replace `TFBertModel` with the equivalent `TFAutoModel` class. We'll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).

{/if}","How can you load a pre-trained Transformer model?
",You can use the `from_pretrained()` method.,huggingface/course/blob/main/chapters/en/chapter2/3.mdx,5.0,"The context provides an example of loading a pre-trained Transformer model using the `from_pretrained()` method from the transformers library. It also explains how to use the equivalent `AutoModel` or `TFAutoModel` classes for checkpoint-agnostic code, which can work with different architectures as long as they are trained for similar tasks.

",5.0,"This question is very useful because loading pre-trained models is a crucial step in many NLP tasks, and understanding how to do so with the Hugging Face ecosystem is essential for machine learning developers working on these projects. The answer will likely involve using the `transformers` library, which is a key part of the Hugging Face ecosystem.

",5.0,"This question is clear and straightforward, requiring only general knowledge of Transformers and their usage. It does not reference any specific context or information outside of standard documentation for loading pre-trained models.
"
"<Tip>

It is a good idea to include the `bad_words_ids` in the call to `generate` to avoid errors arising when increasing
the `max_new_tokens`: the model will want to generate a new `<image>` or `<fake_token_around_image>` token when there
is no image being generated by the model.
You can set it on-the-fly as in this guide, or store in the `GenerationConfig` as described in the [Text generation strategies](../generation_strategies) guide.
</Tip>

## Prompted image captioning

You can extend image captioning by providing a text prompt, which the model will continue given the image. Let's take
another image to illustrate:

<div class=""flex justify-center"">
     <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-prompted-im-captioning.jpg"" alt=""Image of the Eiffel Tower at night""/>
</div>

Photo by [Denys Nevozhai](https://unsplash.com/@dnevozhai).

Textual and image prompts can be passed to the model's processor as a single list to create appropriate inputs.

```py
>>> prompt = [
...     ""https://images.unsplash.com/photo-1543349689-9a4d426bee8e?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3501&q=80"",
...     ""This is an image of "",
... ]

>>> inputs = processor(prompt, return_tensors=""pt"").to(""cuda"")
>>> bad_words_ids = processor.tokenizer([""<image>"", ""<fake_token_around_image>""], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> print(generated_text[0])
This is an image of the Eiffel Tower in Paris, France.
```

## Few-shot prompting","What is the purpose of including `bad_words_ids` in the call to `generate`?
",To avoid errors arising when increasing `max_new_tokens`.,huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md,5.0,"The context explains that including `bad_words_ids` in the call to `generate` helps avoid errors when increasing `max_new_tokens`, specifically when generating new tokens like `<image>` or `<fake_token_around_image>`. This is because without `bad_words_ids`, these special tokens would be generated instead of the intended text. The context also provides an example code snippet showing how `bad_words_ids` can be used to prevent this issue.
",4.0,"The question shows a clear understanding of the Hugging Face ecosystem, specifically the use of the `generate` method. It also indicates that the person asking the question has some knowledge about how to configure the model's behavior, as they are aware of the existence of the `bad_words_ids` parameter. This suggests that the question could be from someone who is familiar with NLP development and wants to refine their understanding of a specific aspect of the ecosystem.
",5.0,"The question clearly refers to a specific API or function (Gradio's `generate` method) and its parameters, but it does not require any additional context beyond what is typically available from documentation. This suggests that an operator with access to Gradio's documentation can understand the purpose of `bad_words_ids` without needing more information.

"
"The abstract from the paper is the following:

*Open-vocabulary object detection has benefited greatly from pretrained vision-language models, but is still limited by the amount of available detection training data. While detection training data can be expanded by using Web image-text pairs as weak supervision, this has not been done at scales comparable to image-level pretraining. Here, we scale up detection data with self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. Major challenges in scaling self-training are the choice of label space, pseudo-annotation filtering, and training efficiency. We present the OWLv2 model and OWL-ST self-training recipe, which address these challenges. OWLv2 surpasses the performance of previous state-of-the-art open-vocabulary detectors already at comparable training scales (~10M examples). However, with OWL-ST, we can scale to over 1B examples, yielding further large improvement: With an L/14 architecture, OWL-ST improves AP on LVIS rare classes, for which the model has seen no human box annotations, from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale training for open-world localization, similar to what has been seen for image classification and language modelling.*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/owlv2_overview.png""
alt=""drawing"" width=""600""/>

<small> OWLv2 high-level overview. Taken from the <a href=""https://arxiv.org/abs/2306.09683"">original paper</a>. </small>

This model was contributed by [nielsr](https://huggingface.co/nielsr).
The original code can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit).

## Usage example","What is the relative improvement in AP on LVIS rare classes achieved by OWL-ST?
",43%,huggingface/transformers/blob/main/docs/source/en/model_doc/owlv2.md,5.0,"The question asks for the relative improvement in AP on LVIS rare classes achieved by OWL-ST, which is explicitly stated in the context as a 43% relative improvement. There are no ambiguous or unclear statements that could lead to multiple interpretations of this value.

",4.0,"This question appears to be specific to a particular task or benchmark (LVIS rare classes) and a specific model (OWL-ST). It seems to be related to object detection, which is a part of NLP (Natural Language Processing), specifically in the context of visual language understanding. To provide an accurate answer, one would need to understand the specifics of the LVIS dataset, OWL-ST model, and how they relate to each other in terms of AP (Average Precision) on rare classes.

",4.0,"This question is asking about a specific metric (relative improvement in Average Precision, or AP) for a particular setting (LVIS rare classes and the OWL-ST method). The terms ""AP"", ""LVIS"", and ""OWL-ST"" are technical and likely require additional context to understand. However, they are all related to object detection and computer vision, which is a specific field with well-defined terminology.

"
"Search

You can now easily search anything on the Hub with **Full-text search**. We index model cards, dataset cards, and Spaces app.py files.

Go directly to https://huggingface.co/search or, using the search bar at the top of https://huggingface.co, you can select ""Try Full-text search"" to help find what you seek on the Hub across models, datasets, and Spaces:

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/fulltextsearch1.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/fulltextsearch2.png""/>
</div>

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/AlbertFTS1.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/AlbertFTS2.png""/>
</div>

## Filter with ease

By default, models, datasets, & spaces are being searched when a user enters a query. If one prefers, one can filter to search only models, datasets, or spaces.

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/Filter%20search%201.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/Filter%20search%202.png""/>
</div>

Moreover, one can copy & share the URL from one's browser's address bar, which should contain the filter information as URL query. For example, when one searches for a query `llama` with a filter to show `Spaces` only, one gets URL https://huggingface.co/search/full-text?q=llama&type=space","What is the URL to access full-text search on Hugging Face Hub?
",https://huggingface.co/search,huggingface/hub-docs/blob/main/docs/hub/search.md,5.0,"The context clearly provides two possible URLs to access the full-text search on Hugging Face Hub: https://huggingface.co/search and https://huggingface.co. It also mentions that one can copy and share the URL from their browser's address bar, which contains the filter information as a URL query. This makes it easy for someone to find the correct URL based on the provided context.
",4.0,"This question shows an interest in utilizing a specific feature of the Hugging Face ecosystem, which suggests that the user is likely familiar with the platform and its capabilities. The question's specificity also indicates that the user may be trying to troubleshoot or optimize their workflow.
",5.0,"The question asks for a specific URL related to Hugging Face Hub, but it does not require any additional context or information about the surrounding environment. It can be understood and answered independently.
"
"Use ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] to apply the transformations on the fly:

```py
food[""train""].set_transform(preprocess_train)
food[""test""].set_transform(preprocess_val)
```

As a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike other data collators in ğŸ¤— Transformers, the
`DefaultDataCollator` does not apply additional preprocessing, such as padding.

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors=""tf"")
```
</tf>
</frameworkcontent>

## Evaluate

Including a metric during training is often helpful for evaluating your model's performance. You can quickly load an
evaluation method with the ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load
the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):

```py
>>> import evaluate

>>> accuracy = evaluate.load(""accuracy"")
```

Then create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions, labels = eval_pred
...     predictions = np.argmax(predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=labels)
```

Your `compute_metrics` function is ready to go now, and you'll return to it when you set up your training.

## Train

<frameworkcontent>
<pt>
<Tip>

If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial [here](../training#train-with-pytorch-trainer)!

</Tip>

You're ready to start training your model now! Load ViT with [`AutoModelForImageClassification`]. Specify the number of labels along with the number of expected labels, and the label mappings:","How many arguments does the `compute_metrics` function take?
",1.,huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md,4.0,"The question is asking about the `compute_metrics` function, which is defined in a specific code snippet. However, upon reviewing the context provided, I notice that the definition of the `compute_metrics` function is not explicitly shown, but rather its usage and explanation are given. Nevertheless, based on the information presented, it appears to take two arguments: `eval_pred`, which contains predictions and labels.

",4.0,"This question appears to be very specific and targeted towards a particular function within the Hugging Face ecosystem. The `compute_metrics` function is likely used for computing metrics in various NLP tasks, such as text classification or token classification. By asking how many arguments this function takes, the person asking the question may be trying to understand how to implement custom metrics for their specific use case or integrate with other parts of the Hugging Face library.

",5.0,"The question makes sense without any additional context. It's a clear and straightforward inquiry about the number of arguments taken by a specific function (`compute_metrics`). There are no implicit references to a particular setting or context.
"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# LMSDiscreteScheduler

`LMSDiscreteScheduler` is a linear multistep scheduler for discrete beta schedules. The scheduler is ported from and created by [Katherine Crowson](https://github.com/crowsonkb/), and the original implementation can be found at [crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181).

## LMSDiscreteScheduler
[[autodoc]] LMSDiscreteScheduler

## LMSDiscreteSchedulerOutput
[[autodoc]] schedulers.scheduling_lms_discrete.LMSDiscreteSchedulerOutput","Who created the original implementation of LMSDiscreteScheduler?
",Katherine Crowson,huggingface/diffusers/blob/main/docs/source/en/api/schedulers/lms_discrete.md,5.0,"The context explicitly states that the original implementation of LMSDiscreteScheduler was ported from and created by Katherine Crowson, providing a direct answer to the question. Additionally, it links to the specific GitHub repository and file where the original implementation can be found.
",4.0,"This question appears to be specific and technical, likely targeted at individuals familiar with the Hugging Face ecosystem and the LMSDiscreteScheduler. It requires knowledge about the history or development of this specific component. The answer might be found in a paper, documentation, or an interview with the creator.
",5.0,"The question does not reference any specific context or setting, and it mentions a particular technical term (LMSDiscreteScheduler) that can be looked up independently. It directly asks for information about a specific entity (the creator), which implies that the answer should be readily available without additional information.
"
"text_encoder = T5EncoderModel.from_pretrained(
    ""DeepFloyd/IF-I-XL-v1.0"", subfolder=""text_encoder"", device_map=""auto"", load_in_8bit=True, variant=""8bit""
)

from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(
    ""DeepFloyd/IF-I-XL-v1.0"",
    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder
    unet=None,
    device_map=""auto"",
)

prompt_embeds, negative_embeds = pipe.encode_prompt(""<prompt>"")
```

For CPU RAM constrained machines like Google Colab free tier where we can't load all model components to the CPU at once, we can manually only load the pipeline with
the text encoder or UNet when the respective model components are needed.

```py
from diffusers import IFPipeline, IFSuperResolutionPipeline
import torch
import gc
from transformers import T5EncoderModel
from diffusers.utils import pt_to_pil, make_image_grid

text_encoder = T5EncoderModel.from_pretrained(
    ""DeepFloyd/IF-I-XL-v1.0"", subfolder=""text_encoder"", device_map=""auto"", load_in_8bit=True, variant=""8bit""
)

# text to image
pipe = DiffusionPipeline.from_pretrained(
    ""DeepFloyd/IF-I-XL-v1.0"",
    text_encoder=text_encoder,  # pass the previously instantiated 8bit text encoder
    unet=None,
    device_map=""auto"",
)

prompt = 'a photo of a kangaroo wearing an orange hoodie and blue sunglasses standing in front of the eiffel tower holding a sign that says ""very deep learning""'
prompt_embeds, negative_embeds = pipe.encode_prompt(prompt)

# Remove the pipeline so we can re-load the pipeline with the unet
del text_encoder
del pipe
gc.collect()
torch.cuda.empty_cache()

pipe = IFPipeline.from_pretrained(
    ""DeepFloyd/IF-I-XL-v1.0"", text_encoder=None, variant=""fp16"", torch_dtype=torch.float16, device_map=""auto""
)

generator = torch.Generator().manual_seed(0)
stage_1_output = pipe(
    prompt_embeds=prompt_embeds,
    negative_prompt_embeds=negative_embeds,
    output_type=""pt"",
    generator=generator,
).images","What is the model name used to instantiate a T5EncoderModel?
",DeepFloyd/IF-I-XL-v1.0,huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md,4.0,"The context provides a clear example of how to instantiate the T5EncoderModel, specifically loading it in 8-bit precision. However, it does not explicitly mention the model name used for instantiation. Nevertheless, based on the given code snippet and the typical naming conventions for models, one can infer that the model being referred to is ""DeepFloyd/IF-I-XL-v1.0"". Yet, this inference relies heavily on prior knowledge about the specific model architecture and its naming convention.

",4.0,"This question is useful because it directly asks about a specific component of the Hugging Face ecosystem, which is T5EncoderModel. The answer will likely be a straightforward model name, such as ""t5-base"" or ""t5-large"", which can help developers get started with using this model in their NLP applications.
",5.0,"This question assumes knowledge of the T5 encoder model, which is a specific type of transformer-based architecture. However, it does not require any additional information about a particular context or setting. It can be understood by someone familiar with the T5 model and its components.

"
"You can also offload the model to the CPU to save even more memory:

```diff
+ pipeline.enable_xformers_memory_efficient_attention()
+ pipeline.enable_model_cpu_offload()
```

To speed-up your inference code even more, use [`torch_compile`](../optimization/torch2.0#torchcompile). You should wrap `torch.compile` around the most intensive component in the pipeline which is typically the UNet:

```py
pipeline.unet = torch.compile(pipeline.unet, mode=""reduce-overhead"", fullgraph=True)
```

Learn more in the [Reduce memory usage](../optimization/memory) and [Torch 2.0](../optimization/torch2.0) guides.","What is the name of the most intensive component typically wrapped with `torch.compile`?
",UNet.,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md,5.0,"The context explicitly states that the most intensive component typically wrapped with `torch.compile` is the UNet, which is a specific type of neural network architecture.
",4.0,"The question is clear and focused, but it assumes a certain level of background knowledge about PyTorch and the Hugging Face ecosystem. However, for someone familiar with these technologies, this question can be helpful in understanding the typical use case of `torch.compile`. Additionally, providing the answer (""forward pass"") will likely clarify any ambiguity.

",4.0,"The question does not specify any context or setting, and it clearly asks about a specific concept related to PyTorch. However, the term ""most intensive"" implies that there might be multiple components, which could make the question ambiguous without additional information. Nevertheless, an operator familiar with PyTorch should be able to understand what is being asked.

"
"- Fix forwarding for guides after SEO renaming by [@aliabd](https://github.com/aliabd) in [PR 3017](https://github.com/gradio-app/gradio/pull/3017)
- Switch all pages on the website to use latest stable gradio by [@aliabd](https://github.com/aliabd) in [PR 3016](https://github.com/gradio-app/gradio/pull/3016)
- Fix bug related to deprecated parameters in `huggingface_hub` for the HuggingFaceDatasetSaver in [PR 3025](https://github.com/gradio-app/gradio/pull/3025)
- Added better support for symlinks in the way absolute paths are resolved by [@abidlabs](https://github.com/abidlabs) in [PR 3037](https://github.com/gradio-app/gradio/pull/3037)
- Fix several minor frontend bugs (loading animation, examples as gallery) frontend [@aliabid94](https://github.com/3026) in [PR 2961](https://github.com/gradio-app/gradio/pull/3026).
- Fixes bug that the chatbot sample code does not work with certain input value by [@petrov826](https://github.com/petrov826) in [PR 3039](https://github.com/gradio-app/gradio/pull/3039).
- Fix shadows for form element and ensure focus styles more visible in dark mode [@pngwn](https://github.com/pngwn) in [PR 3042](https://github.com/gradio-app/gradio/pull/3042).
- Fixed bug where the Checkbox and Dropdown change events were not triggered in response to other component changes by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3045](https://github.com/gradio-app/gradio/pull/3045)
- Fix bug where the queue was not properly restarted after launching a `closed` app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3022](https://github.com/gradio-app/gradio/pull/3022)
- Adding missing embedded components on docs by [@aliabd](https://github.com/aliabd) in [PR 3027](https://github.com/gradio-app/gradio/pull/3027)","Who fixed the bug related to deprecated parameters in `huggingface_hub` for the HuggingFaceDatasetSaver?
",[@aliabd](https://github.com/aliabd),gradio-app/gradio/blob/main/gradio/CHANGELOG.md,5.0,"The context provided includes a list of PRs (pull requests), but only one of them specifically mentions fixing a bug related to deprecated parameters in `huggingface_hub` for the HuggingFaceDatasetSaver. This makes it clear and unambiguous which PR is responsible for this fix.
",5.0,"This question seems specific and technical, targeting a particular issue with the `huggingface_hub` library. It indicates that the person asking the question has a clear understanding of their problem and is looking for information about a specific fix or patch related to HuggingFaceDatasetSaver's usage with deprecated parameters in `huggingface_hub`. This kind of detailed inquiry can be very useful for developers who are using these libraries, as it suggests that there might be a solution available that they haven't yet discovered.

",5.0,"The question asks about a specific event or issue (a bug fix) that occurred within a particular framework or library (Hugging Face's `huggingface_hub`). However, it does not require additional context beyond the mention of the library and its component (`HuggingFaceDatasetSaver`), making it self-contained.

"
"The abstract from the paper is the following:

*Transformers have emerged as a preferred model for many tasks in natural language processing and vision. Recent efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified sparsity patterns, low-rank basis expansions and combinations thereof. In this paper, we revisit classical Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value in this setting remains underexplored thus far. We show that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest. We undertake an extensive set of experiments and demonstrate that this multi-resolution scheme outperforms most efficient self-attention proposals and is favorable for both short and long sequences. Code is available at https://github.com/mlpen/mra-attention.*

This model was contributed by [novice03](https://huggingface.co/novice03).
The original code can be found [here](https://github.com/mlpen/mra-attention).

## MraConfig

[[autodoc]] MraConfig

## MraModel

[[autodoc]] MraModel
    - forward

## MraForMaskedLM

[[autodoc]] MraForMaskedLM
    - forward

## MraForSequenceClassification

[[autodoc]] MraForSequenceClassification
    - forward

## MraForMultipleChoice

[[autodoc]] MraForMultipleChoice
    - forward

## MraForTokenClassification

[[autodoc]] MraForTokenClassification
    - forward

## MraForQuestionAnswering

[[autodoc]] MraForQuestionAnswering
    - forward","Where is the code for the Transformer model available?
",https://github.com/mlpen/mra-attention.,huggingface/transformers/blob/main/docs/source/en/model_doc/mra.md,5.0,"The context clearly mentions that the code for the Transformer model is available at https://github.com/mlpen/mra-attention, providing a direct link to the repository. Additionally, it is mentioned that this model was contributed by novice03 and the original code can be found at the same repository URL. This information makes it easy to find the code for the Transformer model.
",5.0,"This question is extremely useful because it directly relates to one of the most popular models in the Hugging Face ecosystem, which is widely used by NLP developers. The answer will likely point to a publicly available repository or library that contains the implementation of the Transformer model, which can be a valuable resource for those looking to use or build upon this architecture.

",5.0,"The question refers to a specific type of neural network architecture, but it does not require any additional context or information about the setting where the code is being asked for. It's clear and self-contained.
"
"1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research ã‹ã‚‰) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡ [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf)
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ã‹ã‚‰) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡ [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google ã‹ã‚‰) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google ã‹ã‚‰) Jason Phang, Yao Zhao, and Peter J. Liu ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)
1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (Deepmind ã‹ã‚‰) Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier HÃ©naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, JoÃ£o Carreira ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795)
1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)** (ADEPT ã‹ã‚‰) Erich Elsen, Augustus Odena, Maxwell Nye, SaÄŸnak TaÅŸÄ±rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani. ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡ [blog post](https://www.adept.ai/blog/persimmon-8b)","Who are the authors of the research paper ""A Time Series is Worth 64 Words: Long-term Forecasting with Transformers""?
","Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.",huggingface/transformers/blob/main/README_ja.md,5.0,"The context provides information about several research papers and their authors, but it does not explicitly mention the authors of the paper ""A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"". However, one of the points in the context mentions a specific paper ""A Time Series is Worth 64 Words: Long-term Forecasting with Transformers"" and its title matches the question. Additionally, it mentions that this paper was published by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.

",4.0,"This question is useful because it directly references a specific research paper and asks for information that can be easily verified by checking the publication details. The answer would likely involve searching online or accessing academic databases to find the authors listed in the paper.

",5.0,"The question can be understood without any additional context, as it refers to a specific and well-known research paper. The title of the paper is included in the question, which makes it clear what paper is being referred to.
"
"Post-processors

<tokenizerslangcontent>
<python>
## BertProcessing

[[autodoc]] tokenizers.processors.BertProcessing

## ByteLevel

[[autodoc]] tokenizers.processors.ByteLevel

## RobertaProcessing

[[autodoc]] tokenizers.processors.RobertaProcessing

## TemplateProcessing

[[autodoc]] tokenizers.processors.TemplateProcessing
</python>
<rust>
The Rust API Reference is available directly on the [Docs.rs](https://docs.rs/tokenizers/latest/tokenizers/) website.
</rust>
<node>
The node API has not been documented yet.
</node>
</tokenizerslangcontent>","What are the post-processors supported by the tokenizers library?
","BertProcessing, ByteLevel, RobertaProcessing, TemplateProcessing",huggingface/tokenizers/blob/main/docs/source-doc-builder/api/post-processors.mdx,4.0,"The context provided does list several post-processors supported by the tokenizers library, including BertProcessing, ByteLevel, RobertaProcessing, and TemplateProcessing. However, it does not explicitly mention any post-processors that are part of the tokenizers library itself, but rather seems to be listing different processing techniques.

",5.0,"This question is useful because it allows users to understand the capabilities of the tokenizers library, which is a crucial component of the Hugging Face ecosystem. Knowing the supported post-processors can help developers decide whether to use this library for their specific NLP tasks and how to configure it accordingly.

",5.0,"The question is clear and concise, asking about a specific feature of the tokenizers library. It does not rely on any external context or information, making it understandable without additional knowledge.
"
". To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.","Which of the two indices corresponds to the positive label in the SoftMax output?
",Index 1,huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md,5.0,"The context clearly explains the post-processing step of a machine learning pipeline, including the application of a SoftMax layer to convert logits into probabilities, and specifies which index corresponds to the positive label in the output. It mentions that the id2label field of the model config determines this correspondence, and explicitly states that the second index (1) corresponds to the positive label.

",4.0,"This question seems relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it touches on a specific aspect of how models like Transformers produce outputs. However, it is somewhat narrow and assumes a basic understanding of Softmax activation functions and their use in classification tasks.

",5.0,"This question is clear and self-contained, requiring no additional context beyond basic knowledge of machine learning concepts such as SoftMax outputs. The terminology used (SoftMax output and positive label) is specific but not obscure, making it understandable without external references.
"
"|      |                                                                            |[ehartford/samantha-falcon-7b](https://huggingface.co/ehartford/samantha-falcon-7b)|29          |22                       |                                                                                              |                                                                                              |[LICENSE.txt](https://huggingface.co/ehartford/samantha-falcon-7b/blob/main/LICENSE.txt)           |                                                                                                                     |                                                                                   |
|      |                                                                            |[tawfikgh/llama2-ggml](https://huggingface.co/tawfikgh/llama2-ggml)|29          |0                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE](https://huggingface.co/tawfikgh/llama2-ggml/blob/main/LICENSE)                           |                                                                                                                     |                                                                                   |","What is the license of the model llama-2-community-license?
",Community License,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides information about two models, Samantha Falcon and Llama-2 Community License. It also shows a link to the license of the Llama-2 model, which directly answers the question.

",4.0,"This question is useful because it asks about a specific model within the Hugging Face ecosystem, indicating that the user is interested in understanding the licensing terms associated with this particular model. The answer would provide valuable information for developers who want to use or distribute the llama-2-community-license model in their applications.

",4.0,"The question asks about the specific license of a particular model, which suggests that some context or information about the model (such as its name) is already assumed to be known. However, the license itself is a well-defined concept and can be looked up independently without requiring further information.

"
"In this blog post, we demonstrate how Speculative Decoding can be employed to reduce the
inference time of Whisper by a **factor of 2**, while mathematically ensuring exactly the **same outputs** are achieved
from the model. As a result, this method provides a perfect drop-in replacement for existing Whisper pipelines, since it
provides free 2x speed-up while maintaining the same accuracy. For a more streamlined version of the blog post
with fewer explanations but all the code, see the accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/speculative_decoding.ipynb).

## Speculative Decoding

Speculative Decoding was proposed in [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
by Yaniv Leviathan et. al. from Google. It works on the premise that a faster, **assistant model** very often generates the same tokens as a larger **main model**.

First, the assistant model auto-regressively generates a sequence of \\( N \\) *candidate tokens*, \\( \hat{\boldsymbol{y}}_{1:N} \\).
In the diagram below, the assistant model generates a sequence of 5 candidate tokens: `The quick brown sock jumps`.

<figure class=""image table text-center m-0 w-full"">
    <video
        style=""max-width: 90%; margin: auto;""
        controls playsinline
        src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/whisper-speculative-decoding/split_1.mp4""
    ></video>
</figure>

While these candidate tokens are generated quickly, they may differ from those predicted by the main model. Therefore,
in the second step, the candidate tokens are passed to the main model to be ""verified"". The main model takes the
candidate tokens as input and performs a **single forward pass**. The outputs of the main model are the ""correct""
token for each step in the token sequence \\( \boldsymbol{y}_{1:N} \\).","How often does a faster assistant model generate the same tokens as a larger main model?
",very often.,huggingface/blog/blob/main/whisper-speculative-decoding.md,4.0,"The question seems to be answerable based on the context, which mentions that a faster assistant model ""very often generates the same tokens as a larger main model"". However, it doesn't provide explicit information about how frequently this occurs. Still, we can infer that it happens quite often due to the mention of ""very often"" in the context.

",4.0,"This question seems to be related to the performance and consistency of transformer-based models, which are commonly used in NLP applications with the Hugging Face ecosystem. The question appears to be asking about the trade-offs between model size (larger vs faster assistant) and token generation accuracy, which is a relevant consideration for machine learning developers building NLP applications.

",5.0,"The question makes sense without any additional context, and it's clear what it's asking about. It refers to two models (faster assistant and larger main) and asks for a comparison of their token generation.

"
"### Generated output is too short/long

If not specified in the [`~generation.GenerationConfig`] file, `generate` returns up to 20 tokens by default. We highly recommend manually setting `max_new_tokens` in your `generate` call to control the maximum number of new tokens it can return. Keep in mind LLMs (more precisely, [decoder-only models](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)) also return the input prompt as part of the output.


```py
>>> model_inputs = tokenizer([""A sequence of numbers: 1, 2""], return_tensors=""pt"").to(""cuda"")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### Incorrect generation mode

By default, and unless specified in the [`~generation.GenerationConfig`] file, `generate` selects the most likely token at each iteration (greedy decoding). Depending on your task, this may be undesirable; creative tasks like chatbots or writing an essay benefit from sampling. On the other hand, input-grounded tasks like audio transcription or translation benefit from greedy decoding. Enable sampling with `do_sample=True`, and you can learn more about this topic in this [blog post](https://huggingface.co/blog/how-to-generate).

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer([""I am a cat.""], return_tensors=""pt"").to(""cuda"")","What is the default maximum number of tokens returned by the `generate` function?
",20,huggingface/transformers/blob/main/docs/source/en/llm_tutorial.md,5.0,"The context explicitly states that if not specified in the `~generation.GenerationConfig` file, the `generate` function returns up to 20 tokens by default. This information directly answers the question.

",4.0,"This question seems to be about understanding a specific detail related to the Hugging Face ecosystem, specifically the `generate` function. The fact that it's asking for a ""default"" value suggests that it might be a common or useful piece of information for developers working with this library.
",5.0,"The question refers to a specific function (`generate`) within a larger framework (likely an AI or NLP context), but the meaning and purpose of the function are clear from its name, making it understandable without additional information.
"
"| [Table Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/table-transformer) | [arxiv](https://arxiv.org/abs/2110.00061) | [MIT](https://github.com/microsoft/table-transformer/blob/main/LICENSE) | [huggingface](https://huggingface.co/models?other=table-transformer) |
| [LiLT](https://huggingface.co/docs/transformers/main/en/model_doc/lilt) | [arxiv](https://arxiv.org/abs/2202.13669) | [MIT](https://github.com/jpWang/LiLT/blob/main/LICENSE) | [huggingface](https://huggingface.co/models?other=lilt) |","What is the license for the Table Transformer model?
",MIT,huggingface/blog/blob/main/document-ai.md,5.0,"The context provides a direct link to the license of the Table Transformer model, which is located on GitHub under the MIT license. This link explicitly states that the model is licensed under the MIT license, making it clear and unambiguous.
",4.0,"This question appears to be useful because it requires knowledge of the specific model's licensing details, which can be important for developers who want to use the model in their applications. The Hugging Face ecosystem provides a wide range of models with varying licenses, and understanding these licenses is crucial for compliance and future development.
",4.0,"The question clearly refers to the Table Transformer model, which implies that it should be understood within a specific context or knowledge domain. However, since the question is about the licensing information of the model, which is typically publicly available and not specific to any particular setting or environment, the context-independence is relatively high.

"
"{#if fw === 'pt'}

<Tip>

ğŸ’¡ If the output directory you are using exists, it needs to be a local clone of the repository you want to push to (so set a new name if you get an error when defining your `Trainer`).

</Tip>

Finally, we just pass everything to the `Trainer` class and launch the training:

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()
```

{:else}

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir=""bert-finetuned-squad"", tokenizer=tokenizer)

# We're going to do validation afterwards, so no validation mid-training
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)
```

{/if}

Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. The whole training takes a while (a little over an hour on a Titan RTX), so you can grab a coffee or reread some of the parts of the course that you've found more challenging while it proceeds. Also note that as soon as the first epoch is finished, you will see some weights uploaded to the Hub and you can start playing with your model on its page.

{#if fw === 'pt'}

Once the training is complete, we can finally evaluate our model (and pray we didn't spend all that compute time on nothing). The `predict()` method of the `Trainer` will return a tuple where the first elements will be the predictions of the model (here a pair with the start and end logits). We send this to our `compute_metrics()` function:

```python
predictions, _, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[""validation""])
```

{:else}","What happens when the first epoch is finished during training?
",You will see some weights uploaded to the Hub.,huggingface/course/blob/main/chapters/en/chapter7/7.mdx,5.0,"The context describes the process of training a model using the `Trainer` class from the Transformers library. It mentions that each time the model is saved (every epoch), it is uploaded to the Hub in the background. Specifically, as soon as the first epoch is finished, some weights are uploaded to the Hub. This indicates that the completion of the first epoch triggers an action.

",4.0,"This question is useful because understanding what happens at the end of each epoch is crucial for optimizing model performance. In the Hugging Face ecosystem, users may want to know whether their model will be automatically saved or if additional steps are required to evaluate and fine-tune it after an epoch completes.

",5.0,"This question is clear without any additional context, as it refers to a specific point in time (the end of the first epoch) and asks about the consequences or outcomes that occur. The concept of an epoch is a well-defined term in machine learning, making it easy for someone with basic knowledge to understand what is being asked.

"
"```python
from huggingface_hub import notebook_login

notebook_login()
```

If you aren't working in a notebook, just type the following line in your terminal:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

Once this is done, we can define our `TrainingArguments`. As we said when we defined our function to compute the metric, we won't be able to have a regular evaluation loop because of the signature of the `compute_metrics()` function. We could write our own subclass of `Trainer` to do this (an approach you can find in the [question answering example script](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)), but that's a bit too long for this section. Instead, we will only evaluate the model at the end of training here and show you how to do a regular evaluation in ""A custom training loop"" below.

This is really where the `Trainer` API shows its limits and the ğŸ¤— Accelerate library shines: customizing the class to a specific use case can be painful, but tweaking a fully exposed training loop is easy.

Let's take a look at our `TrainingArguments`:

```python
from transformers import TrainingArguments

args = TrainingArguments(
    ""bert-finetuned-squad"",
    evaluation_strategy=""no"",
    save_strategy=""epoch"",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)
```

We've seen most of these before: we set some hyperparameters (like the learning rate, the number of epochs we train for, and some weight decay) and indicate that we want to save the model at the end of every epoch, skip evaluation, and upload our results to the Model Hub. We also enable mixed-precision training with `fp16=True`, as it can speed up the training nicely on a recent GPU.

{:else}

Now that's done, we can create our TF Datasets. We can use the simple default data collator this time:

```python
from transformers import DefaultDataCollator","What is the command to log in if you are not working in a notebook?
",huggingface-cli login,huggingface/course/blob/main/chapters/en/chapter7/7.mdx,5.0,"The context provides a clear alternative command to log in if you are not working in a notebook, which is ""huggingface-cli login"". Additionally, it explicitly states that if you aren't working in a notebook, this command should be used instead of the original notebook_login() function. This makes it easy for someone reading the code to understand what to do when they're not in a notebook environment.
",4.0,"This question seems basic, but it's essential for users who might be new to the Hugging Face ecosystem. The answer is related to the `transformers` library and its usage outside of a Jupyter Notebook environment. However, a more precise understanding of the context would help in providing a more accurate rating.

",5.0,"This question is context-independent because it asks about a general action that can be performed regardless of the specific setting. The phrase ""if you are not working in a notebook"" clarifies that this question is asking about an alternative scenario, but the action itself (logging in) remains the same.
"
"### Dynamic padding[[dynamic-padding]]

<Youtube id=""7q5NyFT8REg""/>

{#if fw === 'pt'}
The function that is responsible for putting together samples inside a batch is called a *collate function*. It's an argument you can pass when you build a `DataLoader`, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won't be possible in our case since the inputs we have won't all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding.

{:else}

The function that is responsible for putting together samples inside a batch is called a *collate function*. The default collator is a function that will just convert your samples to tf.Tensor and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won't be possible in our case since the inputs we have won't all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you're training on a TPU it can cause problems â€” TPUs prefer fixed shapes, even when that requires extra padding.

{/if}","What problem can training on a TPU cause when dynamic padding is used?
",TPUs prefer fixed shapes.,huggingface/course/blob/main/chapters/en/chapter3/2.mdx,5.0,"The context clearly explains the issue with dynamic padding when training on a TPU. It mentions that TPUs prefer fixed shapes and that dynamic padding may cause problems due to its variable output shape.
",4.0,"This question appears to be specific and relevant to the Hugging Face ecosystem, as it mentions TPUs (Tensor Processing Units) and dynamic padding. Training on a TPU is a common task in NLP development, especially with large models like BERT or RoBERTa that are optimized for parallelization on accelerators. Dynamic padding can introduce variable-length sequences in a batch, which might lead to difficulties in training when using TPUs, such as reduced memory efficiency due to the need to use larger tensor shapes or more complex data loading processes.

",4.0,"This question requires knowledge of a specific situation where using a TPU (Tensor Processing Unit) and dynamic padding might lead to a particular problem. The mention of ""training"" suggests a context of machine learning model development, which may not be immediately clear without some background in the field.

"
"```bash
accelerate launch train_dreambooth.py \
  --train_text_encoder
```

## Training script

DreamBooth comes with its own dataset classes:

- [`DreamBoothDataset`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L604): preprocesses the images and class images, and tokenizes the prompts for training
- [`PromptDataset`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L738): generates the prompt embeddings to generate the class images

If you enabled [prior preservation loss](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L842), the class images are generated here:

```py
sample_dataset = PromptDataset(args.class_prompt, num_new_images)
sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)

sample_dataloader = accelerator.prepare(sample_dataloader)
pipeline.to(accelerator.device)

for example in tqdm(
    sample_dataloader, desc=""Generating class images"", disable=not accelerator.is_local_main_process
):
    images = pipeline(example[""prompt""]).images
```

Next is the [`main()`](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L799) function which handles setting up the dataset for training and the training loop itself. The script loads the [tokenizer](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L898), [scheduler and models](https://github.com/huggingface/diffusers/blob/072e00897a7cf4302c347a63ec917b4b8add16d4/examples/dreambooth/train_dreambooth.py#L912C1-L912C1):","What command is used to launch the DreamBooth training script.
",accelerate launch train_dreambooth.py.,huggingface/diffusers/blob/main/docs/source/en/training/dreambooth.md,4.0,"The context provided shows a code snippet that launches the DreamBooth training script using the `accelerate launch` command with the argument `train_dreambooth.py`. This suggests that `train_dreambooth.py` is the Python file containing the script to be launched. However, the question asks for the command used to launch the DreamBooth training script, which could potentially refer to any part of the code or arguments passed to it.

",4.0,"This question seems useful because it's asking about a specific command related to the DreamBooth training script, which is part of the Hugging Face ecosystem. Knowing this command can save developers time and effort in getting started with DreamBooth. However, the usefulness heavily depends on whether the person asking has already explored the documentation or community resources where this information might be readily available.

",4.0,"The question can be understood without any additional context or information, as it clearly asks about a specific command related to launching a script. However, it may require some knowledge of the DreamBooth framework or its installation process.

"
"To get the top-5 predictions class names:

```py
>>> # Get imagenet class mappings
>>> url, filename = (""https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"", ""imagenet_classes.txt"")
>>> urllib.request.urlretrieve(url, filename)
>>> with open(""imagenet_classes.txt"", ""r"") as f:
...     categories = [s.strip() for s in f.readlines()]

>>> # Print top categories per image
>>> top5_prob, top5_catid = torch.topk(probabilities, 5)
>>> for i in range(top5_prob.size(0)):
...     print(categories[top5_catid[i]], top5_prob[i].item())
>>> # prints class names and probabilities like:
>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]
```

Replace the model name with the variant you want to use, e.g. `gluon_inception_v3`. You can find the IDs in the model summaries at the top of this page.

To extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.

## How do I finetune this model?

You can finetune any of the pre-trained models just by changing the classifier (the last layer).

```py
>>> model = timm.create_model('gluon_inception_v3', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)
```
To finetune on your own dataset, you have to write a training loop or adapt [timm's training
script](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.

## How do I train this model?

You can follow the [timm recipe scripts](../scripts) for training a new model afresh.

## Citation","What is the command to retrieve imagenet class mappings?
","urlretrieve(url, filename)",huggingface/pytorch-image-models/blob/main/hfdocs/source/models/gloun-inception-v3.mdx,4.0,"The context provided seems to focus on using pre-trained models and fine-tuning them, but it doesn't explicitly mention how to retrieve imagenet class mappings. However, a snippet of code shows how to get the imagenet class mappings by downloading and reading the ""imagenet_classes.txt"" file.

",4.0,"The question appears to be related to Hugging Face's Transformers library, which provides access to pre-trained models and their associated datasets. In this case, the question is asking about retrieving the ImageNet class mappings, which are likely used for image classification tasks. A developer with experience in NLP and the Hugging Face ecosystem might find this information useful when working with image classification models.

",5.0,"This question can be understood by itself without any additional context, as it directly asks for a specific piece of information related to Imagenet class mappings. The term ""command"" suggests that the answer might be a terminal or API instruction, but this is not crucial for understanding the question.

"
"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# ğŸ¤— Hub å®¢æˆ·ç«¯åº“

é€šè¿‡`huggingface_hub` åº“ï¼Œæ‚¨å¯ä»¥ä¸é¢å‘æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œåä½œè€…çš„å¹³å° [Hugging Face Hub](https://huggingface.co/)è¿›è¡Œäº¤äº’ï¼Œæ‰¾åˆ°é€‚ç”¨äºæ‚¨æ‰€åœ¨é¡¹ç›®çš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†ï¼Œä½“éªŒåœ¨å¹³å°æ‰˜ç®¡çš„æ•°ç™¾ä¸ªæœºå™¨å­¦ä¹ åº”ç”¨ï¼Œè¿˜å¯ä»¥åˆ›å»ºæˆ–åˆ†äº«è‡ªå·±çš„æ¨¡å‹å’Œæ•°æ®é›†å¹¶äºç¤¾åŒºå…±äº«ã€‚ä»¥ä¸Šæ‰€æœ‰éƒ½å¯ä»¥ç”¨Pythonåœ¨`huggingface_hub` åº“ä¸­è½»æ¾å®ç°ã€‚

é˜…è¯»[å¿«é€Ÿå…¥é—¨æŒ‡å—](å¿«é€Ÿå…¥é—¨æŒ‡å—)ä»¥å¼€å§‹ä½¿ç”¨huggingface_hubåº“ã€‚æ‚¨å°†å­¦ä¹ å¦‚ä½•ä»Hubä¸‹è½½æ–‡ä»¶,åˆ›å»ºå­˜å‚¨åº“ä»¥åŠå°†æ–‡ä»¶ä¸Šä¼ åˆ°Hubã€‚ç»§ç»­é˜…è¯»ä»¥äº†è§£æ›´å¤šå…³äºå¦‚ä½•åœ¨ğŸ¤—Hubä¸Šç®¡ç†æ‚¨çš„å­˜å‚¨åº“,å¦‚ä½•å‚ä¸è®¨è®ºæˆ–è€…ç”šè‡³å¦‚ä½•è®¿é—®æ¨ç†APIçš„ä¿¡æ¯ã€‚

<div class=""mt-10"">
  <div class=""w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5"">

    `<a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"" href=""./guides/overview"">`
      `<div class=""w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">`How-to guides`</div>`
      `<p class=""text-gray-700"">`Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems.`</p>`
    `</a>`

    `<a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"" href=""./package_reference/overview"">`
      `<div class=""w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">`Reference`</div>`
      `<p class=""text-gray-700"">`Exhaustive and technical description of huggingface_hub classes and methods.`</p>`
    `</a>`","What is the name of the library that allows interaction with Hugging Face Hub?
",huggingface_hub,huggingface/huggingface_hub/blob/main/docs/source/cn/index.md,5.0,"The context provides a clear description of the Hugging Face Hub client library, stating that it is called ""huggingface_hub"" and can be used to interact with the Hugging Face Hub platform. This information makes it unambiguous how to answer the question about the name of the library.

",5.0,"This question is useful because it gets at a fundamental aspect of the Hugging Face ecosystem, which is the ability to interact with models and datasets hosted on their platform. A clear answer would provide valuable context for developers looking to leverage this capability.
",5.0,"This question is clear and concise, requiring only general knowledge of the topic to understand what is being asked. It does not rely on specific context or information external to the question itself.
"
"Thus, results for language pairs containing uncovered languages are unreliable, as per the [COMET website](https://github.com/Unbabel/COMET)

Also, calculating the COMET metric involves downloading the model from which features are obtained -- the default model, `wmt20-comet-da`, takes over 1.79GB of storage space and downloading it can take a significant amount of time depending on the speed of your internet connection. If this is an issue, choose a smaller model; for instance `wmt21-cometinho-da` is 344MB.

## Citation

```bibtex
@inproceedings{rei-EtAl:2020:WMT,
   author    = {Rei, Ricardo  and  Stewart, Craig  and  Farinha, Ana C  and  Lavie, Alon},
   title     = {Unbabel's Participation in the WMT20 Metrics Shared Task},
   booktitle      = {Proceedings of the Fifth Conference on Machine Translation},
   month          = {November},
   year           = {2020},
   address        = {Online},
   publisher      = {Association for Computational Linguistics},
   pages     = {909--918},
}
```

```bibtex
@inproceedings{rei-etal-2020-comet,
   title = ""{COMET}: A Neural Framework for {MT} Evaluation"",
   author = ""Rei, Ricardo  and
      Stewart, Craig  and
      Farinha, Ana C  and
      Lavie, Alon"",
   booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"",
   month = nov,
   year = ""2020"",
   address = ""Online"",
   publisher = ""Association for Computational Linguistics"",
   url = ""https://www.aclweb.org/anthology/2020.emnlp-main.213"",
   pages = ""2685--2702"",

```

## Further References

- [COMET website](https://unbabel.github.io/COMET/html/index.html)
- [Hugging Face Tasks - Machine Translation](https://huggingface.co/tasks/translation)","What is the storage space required by the default COMET model `wmt20-comet-da`?
",1.79GB,huggingface/datasets/blob/main/metrics/comet/README.md,4.0,"The context mentions the storage space required by the default COMET model `wmt20-comet-da` to be over 1.79GB, but it does not provide a specific answer to the question. However, it provides enough information for one to infer that this is the case.

",4.0,"This question appears to be specific to a particular model and its characteristics, which is a good indicator of how useful it would be for machine learning developers building NLP applications with the Hugging Face ecosystem. The fact that the question asks about storage space specifically implies that the developer is considering deployment or inference scenarios, where model size can be a critical factor.

However, without more context (e.g., what kind of application or scenario), it's difficult to gauge exactly how relevant this information would be for a given developer. Nevertheless, the question seems well-defined and focused on a particular aspect of the model, which suggests that it could be helpful in certain contexts.

",5.0,"This question can be understood without any additional context, as it specifies a particular model name and asks for a specific piece of information (storage space) that should be directly related to that model.
"
"## ğŸ¦œ Additional LLMs ğŸ¦œ

In addition to Meta's 70 billion Llama 2 model, we have prepared template spaces for the following LLMs and deployment options:

- [gpt-3.5-turbo](https://huggingface.co/spaces/gradio-discord-bots/gpt-35-turbo), powered by openai. Required OpenAI key.
- [falcon-7b-instruct](https://huggingface.co/spaces/gradio-discord-bots/falcon-7b-instruct) powered by Hugging Face Inference Endpoints.
- [Llama-2-13b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/Llama-2-13b-chat-hf) powered by Hugging Face Inference Endpoints.
- [Llama-2-13b-chat-hf](https://huggingface.co/spaces/gradio-discord-bots/llama-2-13b-chat-transformers) powered by Hugging Face transformers.

To deploy any of these models to discord, simply follow the instructions in the linked space for that model.

## Deploying non-chat gradio apps to discord

As mentioned above, you don't need a `gr.ChatInterface` if you want to deploy your gradio app to discord. All that's needed is an api route that takes in a single string and outputs a single string.

The following code will deploy a space that translates english to german as a discord bot.

```python
import gradio_client as grc
client = grc.Client(""freddyaboulton/english-to-german"")
client.deploy_discord(api_names=['german'])
```

## Conclusion

That's it for this guide! We're really excited about this feature. Tag [@Gradio](https://twitter.com/Gradio) on twitter and show us how your discord community interacts with your discord bots.","Which company powers the gpt-3.5-turbo LLM.
",OpenAI.,gradio-app/gradio/blob/main/guides/04_chatbots/03_creating-a-discord-bot-from-a-gradio-app.md,5.0,"The context clearly mentions that the gpt-3.5-turbo LLM is ""powered by openai"", which directly answers the question about which company powers it. There is no ambiguity or uncertainty in this information, making it easy to determine the answer.
",4.0,"This question is relevant to NLP developers working with large language models, as it asks about a specific model variant and its underlying technology provider. The answer, OpenAI, is also directly related to the Hugging Face ecosystem since GPT-3.5-turbo can be fine-tuned and used in various applications through Hugging Face's Transformers library.

",5.0,"The question references a specific Large Language Model (LLM) by name, but does not rely on additional context or information to understand what is being asked.
"
"#### 2.2. Feature requests

A world-class feature request addresses the following points:

1. Motivation first:
* Is it related to a problem/frustration with the library? If so, please explain
why. Providing a code snippet that demonstrates the problem is best.
* Is it related to something you would need for a project? We'd love to hear
about it!
* Is it something you worked on and think could benefit the community?
Awesome! Tell us what problem it solved for you.
2. Write a *full paragraph* describing the feature;
3. Provide a **code snippet** that demonstrates its future use;
4. In case this is related to a paper, please attach a link;
5. Attach any additional information (drawings, screenshots, etc.) you think may help.

You can open a feature request [here](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feature_request.md&title=).

#### 2.3 Feedback

Feedback about the library design and why it is good or not good helps the core maintainers immensely to build a user-friendly library. To understand the philosophy behind the current design philosophy, please have a look [here](https://huggingface.co/docs/diffusers/conceptual/philosophy). If you feel like a certain design choice does not fit with the current design philosophy, please explain why and how it should be changed. If a certain design choice follows the design philosophy too much, hence restricting use cases, explain why and how it should be changed.
If a certain design choice is very useful for you, please also leave a note as this is great feedback for future design decisions.

You can open an issue about feedback [here](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feedback.md&title=).

#### 2.4 Technical questions","Where can I open a feature request for the Diffusers library?
",https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feature_request.md&title=,huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md,5.0,"The context provides a clear explanation of how to submit a feature request for the Diffusers library, including a link to the GitHub issue tracker with a specific template for feature requests. This makes it easy for someone to follow and open a new feature request.

",4.0,"This question shows that the user is interested in contributing to or providing feedback on an existing library within the Hugging Face ecosystem. The question about opening a feature request suggests they are familiar with how these libraries work and want to help improve them. A feature request for Diffusers could potentially lead to new features, bug fixes, or improvements that can be beneficial to other users.

",5.0,"The question can be understood without any additional context as it mentions a specific library (Diffusers) and asks about a general action (opening a feature request). The presence of ""library"" suggests that the speaker is familiar with software development and has some understanding of how open-source projects work. Therefore, it's clear what is being asked: where can I submit a suggestion for new features in Diffusers.
"
"`class_labels` and `boxes` key where each value of the batch corresponds to the expected label and number of bounding boxes of each individual image.
- For automatic speech recognition models, ([`Wav2Vec2ForCTC`]), the model expects a tensor of dimension `(batch_size,
  target_length)` with each value corresponding to the expected label of each individual token.

<Tip>","What is the expected shape of the input tensor for automatic speech recognition models?
","(batch_size, target_length)",huggingface/transformers/blob/main/docs/source/en/glossary.md,5.0,"The context explicitly mentions that for automatic speech recognition models (specifically Wav2Vec2ForCTC), the input tensor is expected to have a dimensionality of `(batch_size, target_length)`, where each value corresponds to the expected label of each individual token. This clearly implies that the shape of the input tensor should be two-dimensional with a specific batch size and sequence length. Furthermore, it directly addresses the question about the expected shape for this type of model.

",4.0,"This question appears to be relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as they often work with various types of models and data formats. The specific domain of automatic speech recognition (ASR) might not be directly related to NLP, but the underlying concepts and requirements for input tensor shape are applicable across multiple domains in deep learning.

",5.0,"This question is clear and self-contained, requiring no additional context or information to understand what is being asked. The term ""automatic speech recognition models"" could potentially be unclear without background knowledge in this field, but it is a well-defined technical concept that can be looked up.

"
"1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (Google AI ã‹ã‚‰) Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (Google AI ã‹ã‚‰) Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸãƒ¬ãƒã‚¸ãƒˆãƒªãƒ¼ [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)
1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (Microsoft Research ã‹ã‚‰) Brandon Smock, Rohith Pesala, Robin Abraham ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061)
1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (Google AI ã‹ã‚‰) Jonathan Herzig, PaweÅ‚ Krzysztof Nowak, Thomas MÃ¼ller, Francesco Piccinno and Julian Martin Eisenschlos ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349)
1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (Microsoft Research ã‹ã‚‰) Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653)
1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)**  (HuggingFace ã‹ã‚‰).","Who are the authors of the research paper ""Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer""?
",Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu,huggingface/transformers/blob/main/README_ja.md,5.0,"The context explicitly mentions the authors of a specific research paper, ""Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"", which is attributed to Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu. This information provides a clear answer to the question.
",4.0,"This question appears to be a specific factual inquiry about a particular research paper. The Hugging Face ecosystem often relies on and builds upon existing research, so knowing the authors of influential papers like this one can be useful for understanding the context and history behind certain techniques or models.
",5.0,"The question clearly refers to a specific research paper, but it does not require any additional context or information about the setting in which the paper was written. It can be understood on its own merit.
"
"Metric Card for Precision


## Metric Description

Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:
Precision = TP / (TP + FP)
where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).


## How to Use

At minimum, precision takes as input a list of predicted labels, `predictions`, and a list of output labels, `references`.

```python
>>> precision_metric = datasets.load_metric(""precision"")
>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'precision': 1.0}
```","What is the formula for computing Precision?
",Precision = TP / (TP + FP),huggingface/datasets/blob/main/metrics/precision/README.md,5.0,"The context provides a clear and concise definition of Precision as a metric, including its formula (Precision = TP / (TP + FP)) and an example usage in Python code. This makes it easy to understand how to compute Precision with the given context.
",5.0,"This question is about the fundamental concept of precision in information retrieval and machine learning evaluation metrics. While it's a basic question, understanding the formula for precision is crucial for developers working with NLP applications, especially those using Hugging Face models to fine-tune and evaluate their performance on various tasks.

",5.0,"The question asks about the formula for computing Precision, which is a well-defined concept in the field of information retrieval and machine learning. It does not require any specific context or knowledge of a particular setting to be understood.
"
"Sharing and Loading Models From the Hugging Face Hub

The `timm` library has a built-in integration with the Hugging Face Hub, making it easy to share and load models from the ğŸ¤— Hub.

In this short guide, we'll see how to:
  1. Share a `timm` model on the Hub
  2. How to load that model back from the Hub

## Authenticating

First, you'll need to make sure you have the `huggingface_hub` package installed.

```bash
pip install huggingface_hub
```

Then, you'll need to authenticate yourself. You can do this by running the following command:

```bash
huggingface-cli login
```

Or, if you're using a notebook, you can use the `notebook_login` helper:

```py
>>> from huggingface_hub import notebook_login
>>> notebook_login()
```

## Sharing a Model

```py
>>> import timm
>>> model = timm.create_model('resnet18', pretrained=True, num_classes=4)
```

Here is where you would normally train or fine-tune the model. We'll skip that for the sake of this tutorial.

Let's pretend we've now fine-tuned the model. The next step would be to push it to the Hub! We can do this with the `timm.models.hub.push_to_hf_hub` function.

```py
>>> model_cfg = dict(labels=['a', 'b', 'c', 'd'])
>>> timm.models.hub.push_to_hf_hub(model, 'resnet18-random', model_config=model_cfg)
```

Running the above would push the model to `<your-username>/resnet18-random` on the Hub. You can now share this model with your friends, or use it in your own code!

## Loading a Model

Loading a model from the Hub is as simple as calling `timm.create_model` with the `pretrained` argument set to the name of the model you want to load. In this case, we'll use [`nateraw/resnet18-random`](https://huggingface.co/nateraw/resnet18-random), which is the model we just pushed to the Hub.

```py
>>> model_reloaded = timm.create_model('hf_hub:nateraw/resnet18-random', pretrained=True)
```","What command do you need to run in your terminal or notebook to authenticate yourself for sharing and loading models from the Hugging Face Hub?
",huggingface-cli login,huggingface/pytorch-image-models/blob/main/hfdocs/source/hf_hub.mdx,5.0,"The question asks for a command to authenticate oneself on the Hugging Face Hub, and the context provides exactly that information. It lists two methods of authentication: running `huggingface-cli login` in the terminal or using the `notebook_login` helper function in a notebook.

",4.0,"This question shows a clear understanding of the problem, and it's directly related to using the Hugging Face ecosystem. However, it assumes prior knowledge of the command to be executed in the terminal or notebook for authentication purposes.

",5.0,"This question requires no additional information beyond what is given. The concept of authentication, running commands in a terminal or notebook, and the Hugging Face Hub are all clearly defined.
"
"The data comprises a keyword, a location and the text of the tweet. For the sake of simplicity, we select the `text` feature as the only input to the LLM.

At this stage, we prepared the train, validation, and test sets in the HuggingFace format expected by the pre-trained LLMs. The next step is to define the tokenized dataset for training using the appropriate tokenizer to transform the `text` feature into two Tensors of sequence of token ids and attention masks. As each model has its specific tokenizer, we will need to define three different datasets.


We start by defining the RoBERTa dataloader:

-  Load the tokenizer:
```python
from transformers import AutoTokenizer
roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)
```
**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a result, the first word of the sentence is encoded differently if it is not preceded by a white space. To ensure the first word includes a space, we set `add_prefix_space=True`. Also, to maintain consistent pre-processing for all three models, we set the parameter to 'True' for Llama 2 and Mistral 7b.

- Define the preprocessing function for converting one row of the dataframe:
```python
def roberta_preprocessing_function(examples):
    return roberta_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)
```

By applying the preprocessing function to the first example of our training dataset, we have the tokenized inputs (`input_ids`) and the attention mask:
```python
roberta_preprocessing_function(data['train'][0])
```
```
{'input_ids': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

- Now, let's  apply the preprocessing function to the entire dataset:","What is the name of the tokenizer that treats spaces as part of the token?
",RoBERTa,huggingface/blog/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md,5.0,"The context mentions ""As each model has its specific tokenizer"" and then loads a RoBERTa tokenizer with `add_prefix_space=True`, indicating that this setting allows the tokenizer to treat spaces as part of the token. Furthermore, the code snippet for the preprocessing function using the RoBERTa tokenizer also sets `add_prefix_space=True`. This suggests that the RoBERTa tokenizer is indeed the one being used to treat spaces as part of the token.

",4.0,"This question is useful because it directly targets a specific functionality of Hugging Face's NLP ecosystem, specifically regarding tokenization. The correct answer to this question would likely be 'Byte-Pair Encoding (BPE) with WordPiece' or more accurately 'WordPieceTokenizer', which treats spaces as part of the token. However, without further context, such as whether the question pertains to a particular library like Transformers from Hugging Face, it's hard to provide an exact answer.

",5.0,"The question clearly specifies what property the tokenizer should have, which is treating spaces as part of the token. This information is sufficient for an operator with knowledge of text processing to identify the correct tokenizer.
"
"<PipelineTag pipeline=""text-classification""/>


- [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).


- [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification).

- [`FlaxAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).
- Check the [Text classification task guide](../tasks/sequence_classification) on how to use the model.


<PipelineTag pipeline=""token-classification""/>


- [`AlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification).


- [`TFAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).



- [`FlaxAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).
- [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ğŸ¤— Hugging Face Course.
- Check the [Token classification task guide](../tasks/token_classification) on how to use the model.

<PipelineTag pipeline=""fill-mask""/>","Is FlaxAlbertForSequenceClassification supported by an example script and a notebook?
",Yes,huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md,5.0,"The context clearly mentions that FlaxAlbertForSequenceClassification is supported by an example script and a notebook, specifically providing links to both.
",4.0,"The question seems to be about verifying the availability of specific resources (example script and notebook) for a particular model (FlaxAlbertForSequenceClassification) in the Hugging Face ecosystem. To assess its usefulness, I would consider how often developers may encounter this situation and require confirmation on the existence of these resources.
",5.0,"The question can be understood without any additional context, as it mentions specific technical terms (FlaxAlbertForSequenceClassification) and asks about their support in two clear environments (example script and notebook).

"
"|          [RoBERTa-PreLayerNorm](model_doc/roberta-prelayernorm)          |       âœ…        |         âœ…         |      âœ…      |
|                      [RoCBert](model_doc/roc_bert)                       |       âœ…        |         âŒ         |      âŒ      |
|                      [RoFormer](model_doc/roformer)                      |       âœ…        |         âœ…         |      âœ…      |
|                          [RWKV](model_doc/rwkv)                          |       âœ…        |         âŒ         |      âŒ      |
|                           [SAM](model_doc/sam)                           |       âœ…        |         âœ…         |      âŒ      |
|                  [SeamlessM4T](model_doc/seamless_m4t)                   |       âœ…        |         âŒ         |      âŒ      |
|                [SeamlessM4Tv2](model_doc/seamless_m4t_v2)                |       âœ…        |         âŒ         |      âŒ      |
|                     [SegFormer](model_doc/segformer)                     |       âœ…        |         âœ…         |      âŒ      |
|                           [SEW](model_doc/sew)                           |       âœ…        |         âŒ         |      âŒ      |
|                         [SEW-D](model_doc/sew-d)                         |       âœ…        |         âŒ         |      âŒ      |
|        [Speech Encoder decoder](model_doc/speech-encoder-decoder)        |       âœ…        |         âŒ         |      âœ…      |
|                 [Speech2Text](model_doc/speech_to_text)                  |       âœ…        |         âœ…         |      âŒ      |
|                      [SpeechT5](model_doc/speecht5)                      |       âœ…        |         âŒ         |      âŒ      |
|                      [Splinter](model_doc/splinter)                      |       âœ…        |         âŒ         |      âŒ      |
|                   [SqueezeBERT](model_doc/squeezebert)                   |       âœ…        |         âŒ         |      âŒ      |","Is RoFormer a model that supports incremental learning?
",Yes,huggingface/transformers/blob/main/docs/source/en/index.md,5.0,"The context clearly lists RoFormer under the column that indicates incremental learning is supported, marked with a ""âœ…"" symbol. This suggests that RoFormer does support incremental learning.
",4.0,"The question seems to be specific to a particular NLP model (RoFormer) and its capabilities (incremental learning). While it's related to the Hugging Face ecosystem, it doesn't directly involve building NLP applications with their tools. However, understanding which models support incremental learning is crucial for certain real-world use cases, such as continuous learning or online training.
",5.0,"This question can be understood without any additional context because it clearly asks about the capabilities of RoFormer, which is a specific type of model. The phrase ""incremental learning"" is also well-defined in the field of machine learning.
"
"## Deployment on Vertex AI

[This resource](https://cloud.google.com/vertex-ai/docs/general/general-concepts) shows some relevant concepts on Vertex AI.


```python
from google.cloud.aiplatform import gapic as aip
```


```python
# Deployment hardware
DEPLOY_COMPUTE = ""n1-standard-8""
DEPLOY_GPU = aip.AcceleratorType.NVIDIA_TESLA_T4
PROJECT_ID = ""GCP-PROJECT-ID""
```


```python
# Initialize clients.
API_ENDPOINT = f""{REGION}-aiplatform.googleapis.com""
PARENT = f""projects/{PROJECT_ID}/locations/{REGION}""

client_options = {""api_endpoint"": API_ENDPOINT}
model_service_client = aip.ModelServiceClient(client_options=client_options)
endpoint_service_client = aip.EndpointServiceClient(client_options=client_options)
prediction_service_client = aip.PredictionServiceClient(client_options=client_options)
```


```python
# Upload the model to Vertex AI.
tf28_gpu_model_dict = {
    ""display_name"": ""ViT Base TF2.8 GPU model"",
    ""artifact_uri"": f""{GCS_BUCKET}/{LOCAL_MODEL_DIR}"",
    ""container_spec"": {
        ""image_uri"": ""us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-8:latest"",
    },
}
tf28_gpu_model = (
    model_service_client.upload_model(parent=PARENT, model=tf28_gpu_model_dict)
    .result(timeout=180)
    .model
)
tf28_gpu_model
```


```python
# Create an Endpoint for the model.
tf28_gpu_endpoint_dict = {
    ""display_name"": ""ViT Base TF2.8 GPU endpoint"",
}
tf28_gpu_endpoint = (
    endpoint_service_client.create_endpoint(
        parent=PARENT, endpoint=tf28_gpu_endpoint_dict
    )
    .result(timeout=300)
    .name
)
tf28_gpu_endpoint
```


```python
# Deploy the Endpoint.
tf28_gpu_deployed_model_dict = {
    ""model"": tf28_gpu_model,
    ""display_name"": ""ViT Base TF2.8 GPU deployed model"",
    ""dedicated_resources"": {
        ""min_replica_count"": 1,
        ""max_replica_count"": 1,
        ""machine_spec"": {
            ""machine_type"": DEPLOY_COMPUTE,
            ""accelerator_type"": DEPLOY_GPU,
            ""accelerator_count"": 1,
        },
    },
}","What is the machine type for deploying a model on Vertex AI?
",n1-standard-8,huggingface/blog/blob/main/notebooks/112_vertex_ai_vision.ipynb,4.0,"The context provides information on deploying a model on Vertex AI, specifically using the `create_endpoint` and `deploy_model` methods. It also shows how to specify the machine type for deployment, which is set to ""n1-standard-8"" in this case. However, the context does not explicitly state what the machine type should be used for deployment, making it somewhat ambiguous.

",4.0,"This question is relevant to developers working with the Hugging Face ecosystem, specifically those interested in deploying their models using Google Cloud's Vertex AI. However, the question itself is quite specific and assumes prior knowledge about Vertex AI, which might limit its usefulness as a standalone query.
",4.0,"This question can be understood without additional context as it mentions specific terminology (""machine type"", ""deploying a model"" and ""Vertex AI"") that provides enough information to infer what is being asked. However, there might be multiple machine types available in the Vertex AI context.
"
"2. If in a python notebook, you can use `notebook_login`.

```py
from huggingface_hub import notebook_login

notebook_login()
```

Then, you can share your models by calling the `save_to_hub` method from the trained model. By default, the model will be uploaded to your account. Still, you can upload to an organization by passing it in the `organization` parameter. `save_to_hub` automatically generates a model card, an inference widget, example code snippets, and more details. You can automatically add to the Hubâ€™s model card a list of datasets you used to train the model with the argument `train_datasets`:

```py
model.save_to_hub(
    ""distilroberta-base-sentence-transformer"",
    organization= # Add your username
    train_datasets=[""embedding-data/QQP_triplets""],
    )
```

In the [Notebook Companion](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/95_Training_Sentence_Transformers.ipynb) I fine-tuned this same model using the [embedding-data/sentence-compression](https://huggingface.co/datasets/embedding-data/sentence-compression) dataset and the [`MultipleNegativesRankingLoss`](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss) loss.

## What are the limits of Sentence Transformers?

Sentence Transformers models work much better than the simple Transformers models for semantic search. However, where do the Sentence Transformers models not work well? If your task is classification, then using sentence embeddings is the wrong approach. In that case, the ğŸ¤— [Transformers library](https://huggingface.co/docs/transformers/tasks/sequence_classification) would be a better choice.

## Extra Resources","What parameter can you use with `save_to_hub` to upload to an organization?
",organization,huggingface/blog/blob/main/how-to-train-sentence-transformers.md,4.0,"The context provides information on how to upload a model to the Hub using `save_to_hub`, including the possibility of uploading to an organization. However, it does not explicitly mention any parameters that can be used with `save_to_hub` to achieve this. Nonetheless, based on the code snippet provided in the context, we can infer that the `organization` parameter is indeed used to upload to an organization.

",4.0,"The question is useful because it shows that the user has some basic knowledge of the Hugging Face Hub and the `save_to_hub` method, but needs help with a specific parameter. This indicates that they are likely interested in fine-tuning their skills in working with the Hugging Face ecosystem.
",4.0,"The question makes sense without any additional context, as it references a specific function (`save_to_hub`) and asks about a parameter that is commonly used in such functions. However, the mention of an ""organization"" suggests that this might not be a universal parameter, but rather one specific to Hugging Face's Hub.
"
"|      |                                                                            |[Enoch/llama-65b-hf](https://huggingface.co/Enoch/llama-65b-hf)|1630        |3                        |llama-license                                                                                 |https://huggingface.co/Enoch/llama-65b-hf/blob/main/LICENSE                                   |[LICENSE](https://huggingface.co/Enoch/llama-65b-hf/blob/main/LICENSE)                             |                                                                                                                     |                                                                                   |
|      |                                                                            |[luodian/llama-7b-hf](https://huggingface.co/luodian/llama-7b-hf)|1136        |14                       |llama-license                                                                                 |https://huggingface.co/luodian/llama-7b-hf/blob/main/LICENSE                                  |[LICENSE](https://huggingface.co/luodian/llama-7b-hf/blob/main/LICENSE)                            |                                                                                                                     |                                                                                   |","What is the size of the model ""Enoch/llama-65b-hf""?
",1630,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides a table listing different models, including ""Enoch/llama-65b-hf"". The third column of the table indicates the size of each model in terms of the number of parameters. Therefore, one can easily identify and answer the question by looking up the corresponding value for ""Enoch/llama-65b-hf"" in this column.

",4.0,"This question seems to be about retrieving specific information regarding a pre-trained language model, namely its size. Since this type of information can be crucial for planning computational resources and memory allocation in NLP applications that utilize the Hugging Face ecosystem, understanding model sizes is an important aspect of model deployment.

",5.0,"This question can be understood by itself because it specifies a unique model name, which implies that the answer should be directly related to this specific model. The context required for understanding is limited to the knowledge of what information is typically associated with a model's specification.
"
"```bash
huggingface-cli login
```

Once you've logged in, you can push your tokenizer by executing the following command:

```py
tokenizer.push_to_hub(""code-search-net-tokenizer"")
```

This will create a new repository in your namespace with the name `code-search-net-tokenizer`, containing the tokenizer file. You can then load the tokenizer from anywhere with the `from_pretrained()` method:

```py
# Replace ""huggingface-course"" below with your actual namespace to use your own tokenizer
tokenizer = AutoTokenizer.from_pretrained(""huggingface-course/code-search-net-tokenizer"")
```

You're now all set for training a language model from scratch and fine-tuning it on your task at hand! We'll get to that in [Chapter 7](/course/chapter7), but first, in the rest of this chapter we'll take a closer look at fast tokenizers and explore in detail what actually happens when we call the method `train_new_from_iterator()`.","How do you push a tokenizer to the Hugging Face model hub?
","By executing `tokenizer.push_to_hub(""code-search-net-tokenizer"")`.",huggingface/course/blob/main/chapters/en/chapter6/2.mdx,5.0,"The context provides a clear step-by-step guide on how to push a tokenizer to the Hugging Face model hub, including logging into the CLI, pushing the tokenizer using the `push_to_hub()` method, and loading it from anywhere with the `from_pretrained()` method. Additionally, it mentions a specific command to execute (`tokenizer.push_to_hub(""code-search-net-tokenizer"")`) which makes it easy to understand what action needs to be taken.

",5.0,"This question is extremely useful because it directly addresses a common task that developers may encounter when working with the Hugging Face ecosystem, specifically pushing their custom tokenizers to the model hub for others to use. It indicates that the developer has created or modified a tokenizer and wants to make it accessible through the Hub.
",4.0,"The question clearly refers to a specific action that can be performed on a tokenizer, which is a common concept in natural language processing. However, the mention of ""Hugging Face model hub"" implies some prior knowledge about the context, such as the existence of a Hugging Face account and familiarity with their API.

"
"The abstract from the paper is the following:

*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.*

This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The Authors' code can be
found [here](https://github.com/google/trax/tree/master/trax/models/reformer).

## Usage tips

- Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).
- Use Axial position encoding (see below for more details). Itâ€™s a mechanism to avoid having a huge positional encoding matrix (when the sequence length is very big) by factorizing it into smaller matrices.
- Replace traditional attention by LSH (local-sensitive hashing) attention (see below for more details). Itâ€™s a technique to avoid computing the full product query-key in the attention layers.
- Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them for results inside a given layer (less efficient than storing them but saves memory).
- Compute the feedforward operations by chunks and not on the whole batch.","What is the time complexity of dot-product attention in the Transformer model?
",O(L^2),huggingface/transformers/blob/main/docs/source/en/model_doc/reformer.md,5.0,"The context clearly mentions that the time complexity of dot-product attention is O(L^2), where L is the length of the sequence. It also highlights that replacing dot-product attention with locality-sensitive hashing (LSH) attention reduces the time complexity to O(Llog(L)). Therefore, the question can be answered unambiguously based on this information.

",4.0,"The question asks about the time complexity of a specific component (dot-product attention) within the Transformer model, which is a fundamental aspect of transformer architectures. This information can be crucial for developers building large-scale NLP applications with the Hugging Face ecosystem, particularly when considering the computational resources and efficiency required for these models. Knowing the time complexity helps in understanding the scalability of the model and making informed decisions about hardware or optimizations.
",5.0,"The question clearly states that it's referring to a specific concept within a well-known model, but it doesn't require any additional information or context beyond basic knowledge of the Transformer architecture. The question is self-contained and can be understood by anyone familiar with the topic.

"
"The output is then a list of `Encoding`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.

To process a batch of sentences pairs, pass two lists to the
`Tokenizer.encode_batch` method: the
list of sentences A and the list of sentences B:

<tokenizerslangcontent>
<python>
<literalinclude>
{""path"": ""../../bindings/python/tests/documentation/test_quicktour.py"",
""language"": ""python"",
""start-after"": ""START encode_batch_pair"",
""end-before"": ""END encode_batch_pair"",
""dedent"": 8}
</literalinclude>
</python>
<rust>
<literalinclude>
{""path"": ""../../tokenizers/tests/documentation.rs"",
""language"": ""rust"",
""start-after"": ""START quicktour_encode_batch_pair"",
""end-before"": ""END quicktour_encode_batch_pair"",
""dedent"": 4}
</literalinclude>
</rust>
<node>
<literalinclude>
{""path"": ""../../bindings/node/examples/documentation/quicktour.test.ts"",
""language"": ""js"",
""start-after"": ""START encode_batch_pair"",
""end-before"": ""END encode_batch_pair"",
""dedent"": 8}
</literalinclude>
</node>
</tokenizerslangcontent>

When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`Tokenizer.enable_padding`, with the
`pad_token` and its ID (which we can
double-check the id for the padding token with
`Tokenizer.token_to_id` like before):","How many lists do you need to pass to `Tokenizer.encode_batch` method?
",2,huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx,5.0,"The context clearly states that to process a batch of sentences pairs, you need to pass two lists to the `Tokenizer.encode_batch` method: the list of sentences A and the list of sentences B. This implies that one needs to pass at least two lists to the method.
",4.0,"The question appears to be asking about a specific detail of the Hugging Face Tokenizer, which suggests that it may be looking for an implementation or usage-specific answer. However, without more context, it's hard to determine whether this is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem. Still, understanding how to properly utilize the `Tokenizer.encode_batch` method is important for many NLP tasks.
",4.0,"The question clearly refers to a specific method, `Tokenizer.encode_batch`, which implies that it is part of an API or library. However, without knowing the context of the API or library, the question still makes sense as it simply asks for a numerical value, making it relatively self-contained.

"
"In few-shot prompting, we provide examples in the prompt giving the model more context to improve the performance.
The examples condition the model to generate the output following the patterns in the examples.

Here's an example:

```python
>>> torch.manual_seed(0) # doctest: +IGNORE_RESULT
>>> prompt = """"""Text: The first human went into space and orbited the Earth on April 12, 1961.
... Date: 04/12/1961
... Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.
... Date:""""""

>>> sequences = pipe(
...     prompt,
...     max_new_tokens=8,
...     do_sample=True,
...     top_k=10,
... )

>>> for seq in sequences:
...     print(f""Result: {seq['generated_text']}"")
Result: Text: The first human went into space and orbited the Earth on April 12, 1961.
Date: 04/12/1961
Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.
Date: 09/28/1960
```

In the above code snippet we used a single example to demonstrate the desired output to the model, so this can be called a
""one-shot"" prompting. However, depending on the task complexity you may need to use more than one example.

Limitations of the few-shot prompting technique:
- While LLMs can pick up on the patterns in the examples, these technique doesn't work well on complex reasoning tasks
- Few-shot prompting requires creating lengthy prompts. Prompts with large number of tokens can increase computation and latency. There's also a limit to the length of the prompts.
- Sometimes when given a number of examples, models can learn patterns that you didn't intend them to learn, e.g. that the third movie review is always negative.

### Chain-of-thought","What are some limitations of the few-shot prompting technique?
","The limitations include that it doesn't work well on complex reasoning tasks, requires creating lengthy prompts, and can lead to unintended pattern learning.",huggingface/transformers/blob/main/docs/source/en/tasks/prompting.md,5.0,"The question about limitations of the few-shot prompting technique is directly addressed in the context provided, which explicitly mentions three limitations: poor performance on complex reasoning tasks, lengthy prompts that increase computation and latency, and unintended pattern learning from examples. This context provides a clear and unambiguous answer to the question.
",4.0,"This question is useful because it highlights a potential limitation of a specific NLP technique, which can help developers identify areas for improvement or alternative approaches. However, it does not specify the context (e.g., Hugging Face ecosystem) explicitly.

",5.0,"This question is clear and concise, asking for specific information about a particular technique (few-shot prompting). The context is implicit, but it's clear that the question refers to the topic of natural language processing or machine learning. There are no ambiguous references or settings that require additional information to understand.
"
"Stable Diffusion text-to-image fine-tuning

The `train_text_to_image.py` script shows how to fine-tune stable diffusion model on your own dataset.

___Note___:

___This script is experimental. The script fine-tunes the whole model and often times the model overfits and runs into issues like catastrophic forgetting. It's recommended to try different hyperparamters to get the best result on your dataset.___


## Running locally with PyTorch
### Installing the dependencies

Before running the scripts, make sure to install the library's training dependencies:

**Important**

To make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:
```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

Then cd in the example folder  and run
```bash
pip install -r requirements.txt
```

And initialize an [ğŸ¤—Accelerate](https://github.com/huggingface/accelerate/) environment with:

```bash
accelerate config
```

### Pokemon example

You need to accept the model license before downloading or using the weights. In this example we'll use model version `v1-4`, so you'll need to visit [its card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree.

You have to be a registered user in ğŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).

Run the following command to authenticate your token

```bash
huggingface-cli login
```

If you have already cloned the repo, then you won't need to go through these steps.

<br>","How do I install the library's training dependencies for stable diffusion text-to-image fine-tuning?
","Execute `git clone https://github.com/huggingface/diffusers` and `pip install .`. Then, cd into the example folder and run `pip install -r requirements.txt`.",huggingface/diffusers/blob/main/examples/research_projects/onnxruntime/text_to_image/README.md,5.0,"The context provides detailed instructions on how to install the library's training dependencies for stable diffusion text-to-image fine-tuning. It specifically mentions running a script in a new virtual environment, installing dependencies using pip, and initializing an Accelerate environment. Additionally, it provides links to relevant resources, such as the example scripts repository and the Hugging Face Hub documentation. The context also explains the importance of installing from source to ensure compatibility with the latest versions of the example scripts.

",4.0,"This question appears to be specific and relevant to a particular use case involving stable diffusion, which is a component of the Hugging Face ecosystem. The query seems clear in its intent, asking for instructions on how to install necessary dependencies for a specific task within NLP (fine-tuning). Given that the Hugging Face Transformers library supports fine-tuning models like Stable Diffusion and often requires additional dependencies for such tasks, this question could be quite useful for developers trying to undertake similar projects.

",5.0,"The question asks a straightforward procedural query that can be understood independently of any specific context, as it mentions a well-known library (stable diffusion) and a common task (text-to-image fine-tuning), but does not reference any particular setting or document.
"
"You have now finished the coding part, congratulation! ğŸ‰ You are Awesome! ğŸ˜

**12. Upload the models to the model hub**

In this final part, you should convert and upload all checkpoints to the model hub and add a model card for each
uploaded model checkpoint. You can get familiar with the hub functionalities by reading our [Model sharing and uploading Page](model_sharing). You should work alongside the Hugging Face team here to decide on a fitting name for each
checkpoint and to get the required access rights to be able to upload the model under the author's organization of
*brand_new_bert*. The `push_to_hub` method, present in all models in `transformers`, is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:

```python
brand_new_bert.push_to_hub(""brand_new_bert"")
# Uncomment the following line to push to an organization.
# brand_new_bert.push_to_hub(""<organization>/brand_new_bert"")
```

It is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the
specific characteristics of this particular checkpoint, *e.g.* On which dataset was the checkpoint
pretrained/fine-tuned on? On what down-stream task should the model be used? And also include some code on how to
correctly use the model.

**13. (Optional) Add notebook**

It is very helpful to add a notebook that showcases in-detail how *brand_new_bert* can be used for inference and/or
fine-tuned on a downstream task. This is not mandatory to merge your PR, but very useful for the community.

**14. Submit your finished PR**

You're done programming now and can move to the last step, which is getting your PR merged into main. Usually, the
Hugging Face team should have helped you already at this point, but it is worth taking some time to give your finished
PR a nice description and eventually add comments to your code, if you want to point out certain design choices to your
reviewer.

### Share your work!!","What method is present in all models in transformers for pushing the checkpoint to the hub?
",The `push_to_hub` method.,huggingface/transformers/blob/main/docs/source/en/add_new_model.md,5.0,"The context describes the process of uploading models to the model hub using the `push_to_hub` method in transformers, providing a specific snippet of code for the `brand_new_bert` model. This indicates that the `push_to_hub` method is indeed present in all models in transformers.

",5.0,"This question is specific, clear, and well-defined, indicating a good understanding of the Hugging Face ecosystem. The transformer library is a key component of the Hugging Face framework, and knowing which method pushes checkpoints to the hub is essential for deploying models on the Hub. A user with this question likely wants to understand how to share their trained models publicly or access pre-trained models from the Hub.
",4.0,"The question refers specifically to a context of using Hugging Face's Transformers library, which suggests that it requires some prior knowledge or understanding of the library's architecture and usage. However, the phrase ""in all models"" implies a generalization across various transformers models, making it somewhat self-contained within the scope of Transformers.
"
"### Output Values
The output is a dictionary with one entry for each rouge type in the input list `rouge_types`. If `use_aggregator=False`, each dictionary entry is a list of scores, with one score for each sentence. E.g. if `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=False`, the output is:

```python
{'rouge1': [0.6666666666666666, 1.0], 'rouge2': [0.0, 1.0]}
```

If `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=True`, the output is of the following format:
```python
{'rouge1': 1.0, 'rouge2': 1.0}
```

The ROUGE values are in the range of 0 to 1.


#### Values from Popular Papers


### Examples
An example without aggregation:
```python
>>> rouge = evaluate.load('rouge')
>>> predictions = [""hello goodbye"", ""ankh morpork""]
>>> references = [""goodbye"", ""general kenobi""]
>>> results = rouge.compute(predictions=predictions,
...                         references=references,
...                         use_aggregator=False)
>>> print(list(results.keys()))
['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
>>> print(results[""rouge1""])
[0.5, 0.0]
```

The same example, but with aggregation:
```python
>>> rouge = evaluate.load('rouge')
>>> predictions = [""hello goodbye"", ""ankh morpork""]
>>> references = [""goodbye"", ""general kenobi""]
>>> results = rouge.compute(predictions=predictions,
...                         references=references,
...                         use_aggregator=True)
>>> print(list(results.keys()))
['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
>>> print(results[""rouge1""])
0.25
```","What is the format of ROUGE values returned by the compute function when use_aggregator=True?
",The output is a dictionary with ROUGE values in the range of 0 to 1.,huggingface/evaluate/blob/main/metrics/rouge/README.md,5.0,"The context explicitly states the output format of the `compute` function when `use_aggregator=True`, which is a dictionary with ROUGE values in the range of 0 to 1, where each value represents an aggregator score for all sentences.

",5.0,"This question appears to be quite specific and relevant to machine learning developers using Hugging Face's tools for NLP applications, particularly those working with metrics like ROUGE. The mention of `use_aggregator=True` suggests that the developer is familiar with certain settings or options within the toolset and is seeking detailed information on how the output would be formatted in a specific case.
",5.0,"The question clearly specifies what is being asked (the format of ROUGE values) and provides context about how it's being computed, including the specific parameter setting. However, it does not provide any information about the expected output or its relation to a specific task or dataset.

"
"Below is a basic example usage of how to inject LoRA adapters into the submodule `linear` of the module `DummyModel`.
```python
import torch
from peft import inject_adapter_in_model, LoraConfig


class DummyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = torch.nn.Embedding(10, 10)
        self.linear = torch.nn.Linear(10, 10)
        self.lm_head = torch.nn.Linear(10, 10)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        x = self.linear(x)
        x = self.lm_head(x)
        return x


lora_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias=""none"",
    target_modules=[""linear""],
)

model = DummyModel()
model = inject_adapter_in_model(lora_config, model)

dummy_inputs = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])
dummy_outputs = model(dummy_inputs)
```

If you print the model, you will notice that the adapters have been correctly injected into the model

```bash
DummyModel(
  (embedding): Embedding(10, 10)
  (linear): Linear(
    in_features=10, out_features=10, bias=True
    (lora_dropout): ModuleDict(
      (default): Dropout(p=0.1, inplace=False)
    )
    (lora_A): ModuleDict(
      (default): Linear(in_features=10, out_features=64, bias=False)
    )
    (lora_B): ModuleDict(
      (default): Linear(in_features=64, out_features=10, bias=False)
    )
    (lora_embedding_A): ParameterDict()
    (lora_embedding_B): ParameterDict()
  )
  (lm_head): Linear(in_features=10, out_features=10, bias=True)
)
```
Note that it should be up to users to properly take care of saving the adapters (in case they want to save adapters only), as `model.state_dict()` will return the full state dict of the model.
In case you want to extract the adapters state dict you can use the `get_peft_model_state_dict` method:

```python
from peft import get_peft_model_state_dict

peft_state_dict = get_peft_model_state_dict(model)
print(peft_state_dict)
```

## Pros and cons","What is the value of lora_alpha in the LoraConfig object?
",16,huggingface/peft/blob/main/docs/source/developer_guides/low_level_api.md,4.0,"The context provides a clear example of how to use the LoraConfig object, specifically how to inject LoRA adapters into a model. It also explains the properties of the LoraConfig object, including lora_alpha, which is set to 16 in this example. However, there is no explicit statement that defines the value of lora_alpha or its purpose in the context.
",4.0,"This question appears to be related to a specific configuration parameter in the Hugging Face ecosystem, likely within the context of the Long Short-Term Memory (LSTM) model or a similar architecture. The value of `lora_alpha` is crucial for controlling the learning rate of adapter layers in models like BERT or its variants when using LoRA (Low-Rank Adaptation). To accurately answer this question, one would need to refer to the documentation for LoraConfig within the Hugging Face Transformers library, which typically includes parameters such as `lora_alpha`, among others.

",5.0,"The question refers to specific technical terms like ""LoraConfig"" and ""lora_alpha"", but it does not imply any particular context or setting, making it clear what object and parameter are being asked about.

"
"Notebook Magic ç°åœ¨æ˜¯ä½œè€…æ„å»º Gradio æ¼”ç¤ºçš„é¦–é€‰æ–¹å¼ã€‚æ— è®ºæ‚¨å¦‚ä½•ç¼–å†™ Python ä»£ç ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›è¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½ä¸ºæ‚¨æä¾›æ›´å¥½çš„ Gradio å¼€å‘ä½“éªŒã€‚

---

## ä¸‹ä¸€æ­¥

æ—¢ç„¶æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ Gradio å¿«é€Ÿå¼€å‘ï¼Œè¯·å¼€å§‹æ„å»ºè‡ªå·±çš„åº”ç”¨ç¨‹åºå§ï¼

å¦‚æœä½ æ­£åœ¨å¯»æ‰¾çµæ„Ÿï¼Œè¯·å°è¯•æµè§ˆå…¶ä»–äººç”¨ Gradio æ„å»ºçš„æ¼”ç¤ºï¼Œ[æµè§ˆ Hugging Face Spaces](http://hf.space/) ğŸ¤—","What is Notebook Magic used for?
",It is the preferred way to build Gradio demonstrations.,gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/developing-faster-with-reload-mode.md,4.0,"The context mentions Notebook Magic as the preferred way to build Gradio demonstrations, implying that it is used for creating interactive visualizations or applications. However, there is no explicit statement about its specific use case or purpose beyond being a development tool.

",4.0,"This question appears to be related to Jupyter Notebooks, which are often used with the Hugging Face ecosystem. However, Notebook Magic is a feature of IPython/Jupyter that allows users to use special syntax (like %magic commands) to perform various tasks in their notebooks, such as running shell commands or setting environment variables. While it may not be directly related to NLP development, understanding Notebook Magic can still be helpful for developers who want to automate tasks or create interactive tutorials within Jupyter Notebooks.

",5.0,"The question refers to a specific concept within the Jupyter Notebooks ecosystem, but it doesn't rely on any additional information from the context to be understood. It's clear that Notebook Magic is related to Jupyter Notebooks, and the question aims to understand its purpose.

"
"|      |                                                                            |[TheBloke/Zarafusionex-1.1-L2-7B-GPTQ](https://huggingface.co/TheBloke/Zarafusionex-1.1-L2-7B-GPTQ)|187         |4                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/Zarafusionex-1.1-L2-7B-GPTQ/blob/main/LICENSE.txt)   |                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/llama2_7b_chat_uncensored-GGML](https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GGML)|179         |89                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GGML/blob/main/LICENSE.txt)|                                                                                                                     |                                                                                   |","What is the name of the license for the LLaMA 2 model?
",llama-2-community-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides a clear reference to the license name, ""llama-2-community-license"", and also links to the license document hosted on huggingface.co. Additionally, there is a direct link to the AI Meta website that hosts the LLaMA license information. This suggests that all necessary information for answering the question is readily available within the given context.
",4.0,"The question is specific to a particular NLP model (LLaMA 2) and its licensing terms, which are essential knowledge for developers working with this model in the Hugging Face ecosystem. This type of question can help clarify any legal or regulatory concerns related to using pre-trained models in commercial applications.

",5.0,"This question clearly refers to a specific model, LLaMA 2, but it does not require any additional context to understand what is being asked. The question can be answered independently by looking up the information about the LLaMA 2 model and its associated license.

"
"### Testing and Infrastructure Changes:

No changes to highlight.

### Breaking Changes:

- `gr.HuggingFaceDatasetSaver` behavior changed internally. The `flagging/` folder is not a `.git/` folder anymore when using it. `organization` parameter is now ignored in favor of passing a full dataset id as `dataset_name` (e.g. `""username/my-dataset""`).
- New lines (`\n`) are not automatically converted to `<br>` in `gr.Markdown()` or `gr.Chatbot()`. For multiple new lines, a developer must add multiple `<br>` tags.

### Full Changelog:

- Safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of git pull/push by [@Wauplin](https://github.com/Wauplin) in [PR 3973](https://github.com/gradio-app/gradio/pull/3973)

### Contributors Shoutout:

No changes to highlight.

## 3.28.1

### New Features:

- Add a ""clear mask"" button to `gr.Image` sketch modes, by [@space-nuko](https://github.com/space-nuko) in [PR 3615](https://github.com/gradio-app/gradio/pull/3615)

### Bug Fixes:

- Fix dropdown default value not appearing by [@aliabid94](https://github.com/aliabid94) in [PR 3996](https://github.com/gradio-app/gradio/pull/3996).
- Fix faded coloring of output textboxes in iOS / Safari by [@aliabid94](https://github.com/aliabid94) in [PR 3993](https://github.com/gradio-app/gradio/pull/3993)

### Documentation Changes:

No changes to highlight.

### Testing and Infrastructure Changes:

- CI: Simplified Python CI workflow by [@akx](https://github.com/akx) in [PR 3982](https://github.com/gradio-app/gradio/pull/3982)
- Upgrade pyright to 1.1.305 by [@akx](https://github.com/akx) in [PR 4042](https://github.com/gradio-app/gradio/pull/4042)
- More Ruff rules are enabled and lint errors fixed by [@akx](https://github.com/akx) in [PR 4038](https://github.com/gradio-app/gradio/pull/4038)

### Breaking Changes:

No changes to highlight.

### Full Changelog:

No changes to highlight.

### Contributors Shoutout:

No changes to highlight.

## 3.28.0

### Bug Fixes:","Who contributed the safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of git pull/push?
",@Wauplin,gradio-app/gradio/blob/main/CHANGELOG.md,5.0,"The question is clearly answerable with the context because it directly references a specific change made by a contributor in the ""Full Changelog"" section of the release notes, which mentions the safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of git pull/push.

",4.0,"This question appears to be asking about a specific contributor or commit related to a particular change in the Hugging Face dataset saver. The mention of ""safer version"" and ""HTTP methods"" suggests that it's asking for information on how a security vulnerability was addressed, which could be valuable for developers looking to understand the history and evolution of the codebase.
",5.0,"This question refers to a specific contribution made by someone, likely related to a particular project or codebase. It mentions the name of a class from Gradio and Hugging Face's libraries (gr.HuggingFaceDatasetSaver), which suggests that the context is programming or software development. However, it does not rely on any external information about a document, setting, or previous conversation.

"
"!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Summarization

[[open-in-colab]]

<Youtube id=""yHnr5Dk2zCI""/>

Summarization creates a shorter version of a document or an article that captures all the important information. Along with translation, it is another example of a task that can be formulated as a sequence-to-sequence task. Summarization can be:

- Extractive: extract the most relevant information from a document.
- Abstractive: generate new text that captures the most relevant information.

This guide will show you how to:

1. Finetune [T5](https://huggingface.co/t5-small) on the California state bill subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset for abstractive summarization.
2. Use your finetuned model for inference.

<Tip>
The task illustrated in this tutorial is supported by the following model architectures:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->","What is abstractive summarization?
",generate new text that captures the most relevant information.,huggingface/transformers/blob/main/docs/source/en/tasks/summarization.md,5.0,"The context provides a clear definition of abstractive summarization as ""generate new text that captures the most relevant information"". It also distinguishes it from extractive summarization, which extracts relevant information from a document. Additionally, the context mentions a specific guide on how to finetune a model for abstractive summarization using the T5 architecture and the BillSum dataset. This suggests that the question about abstractive summarization can be answered unambiguously with the given context.

",5.0,"This question demonstrates a clear understanding of a specific NLP task, which is essential for machine learning developers working with the Hugging Face ecosystem. Abstractive summarization is a complex task that requires models to generate summaries that condense and rephrase the original text, making it a crucial aspect of many NLP applications.

",5.0,"The term ""abstractive summarization"" refers to a method of text summarization that involves generating a condensed version of the original content by identifying the most important information and rephrasing it in a more concise manner. This type of summarization is ""abstractive"" because it goes beyond simply extracting keywords or phrases from the original text, instead using language generation capabilities to create new, summarized text.

"
"For example, create PyTorch tensors by setting `type=""torch""`:

```py
>>> import torch
>>> dataset.set_format(type=""torch"", columns=[""input_ids"", ""token_type_ids"", ""attention_mask"", ""label""])
```

The [`~Dataset.with_format`] function also changes the format of a column, except it returns a new [`Dataset`] object:

```py
>>> dataset = dataset.with_format(type=""torch"", columns=[""input_ids"", ""token_type_ids"", ""attention_mask"", ""label""])
```

<Tip>

ğŸ¤— Datasets also provides support for other common data formats such as NumPy, Pandas, and JAX. Check out the [Using Datasets with TensorFlow](https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset) guide for more details on how to efficiently create a TensorFlow dataset.

</Tip>

If you need to reset the dataset to its original format, use the [`~Dataset.reset_format`] function:

```py
>>> dataset.format
{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}
>>> dataset.reset_format()
>>> dataset.format
{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```

### Format transform

The [`~Dataset.set_transform`] function applies a custom formatting transform on-the-fly. This function replaces any previously specified format. For example, you can use this function to tokenize and pad tokens on-the-fly. Tokenization is only applied when examples are accessed:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
>>> def encode(batch):
...     return tokenizer(batch[""sentence1""], padding=""longest"", truncation=True, max_length=512, return_tensors=""pt"")
>>> dataset.set_transform(encode)
>>> dataset.format
{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}
```","What is the name of the function that applies a custom formatting transform on-the-fly in the Hugging Face Datasets library?
",set_transform,huggingface/datasets/blob/main/docs/source/process.mdx,5.0,"The context provides a clear explanation of how to apply a custom formatting transform on-the-fly in the Hugging Face Datasets library using the `set_transform` function. It also shows an example code snippet demonstrating how to use this function with a specific tokenizer.

",4.0,"This question is specific to the Hugging Face Datasets library, which is part of the larger Hugging Face ecosystem used for NLP applications. The question appears to be about understanding the functionality of a particular library, and specifically asks for a detail that might be relevant when working with datasets in this context.
",4.0,"The question clearly references a specific library (Hugging Face Datasets) and asks about a particular functionality within it, suggesting that some background knowledge or context might be necessary to fully understand the question. However, the key concepts mentioned are well-defined in the context of data science and machine learning, making the question fairly self-explanatory.
"
"<Tip>

Guess mode does not have any impact on prompt conditioning and you can still provide a prompt if you want.

</Tip>

Set `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) to set the `guidance_scale` value between 3.0 and 5.0.

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
from PIL import Image
import cv2

controlnet = ControlNetModel.from_pretrained(""lllyasviel/sd-controlnet-canny"", use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", controlnet=controlnet, use_safetensors=True).to(""cuda"")

original_image = load_image(""https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png"")

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe("""", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<div class=""flex gap-4"">
  <div>
    <img class=""rounded-xl"" src=""https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">regular mode with prompt</figcaption>
  </div>
  <div>
    <img class=""rounded-xl"" src=""https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">guess mode without prompt</figcaption>
  </div>
</div>

## ControlNet with Stable Diffusion XL","What is the recommended guidance scale value for guess mode?
",between 3.0 and 5.0,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md,4.0,"The context mentions that the guidance scale value for guess mode is recommended to be set between 3.0 and 5.0, but it does not specify what this recommendation means in terms of the optimal or default guidance scale value for guess mode. However, it does provide a specific example where `guidance_scale=3.0` is used when setting `guess_mode=True`, which suggests that a guidance scale value within this recommended range can be used.

",4.0,"This question is likely to be very useful because it directly addresses a specific aspect of the Hugging Face ecosystem, namely the Guess Mode feature. The fact that it asks for a ""recommended guidance scale value"" implies that the respondent has some understanding of the feature and its parameters, which makes the question more targeted and relevant.

However, without knowing the exact context or the type of model being used, the answer might not be universally applicable to all NLP applications with the Hugging Face ecosystem. Nevertheless, it is still a very specific and focused question that can help clarify a particular aspect of working with the Hugging Face library.

",5.0,"The question does not require any additional context to understand what it's asking. It mentions a specific concept (""guess mode"") and asks about a particular value (""recommended guidance scale""), which implies that this value can be looked up or accessed in documentation.
"
"### Pre-requisites

First follow these steps to install Flash Attention V2:  Dao-AILab/flash-attention: Fast and memory-efficient exact attention (github.com). Install the latest nightlies of PyTorch with CUDA â‰¥11.8. Install the remaining requirements as per DHS-LLM-Workshop/code_assistant/training/requirements.txt. Here, we will be installing ğŸ¤— Accelerate and ğŸ¤— Transformers from the main branch.

## Fine-Tuning

### Addressing Challenge 1
PRs [huggingface/transformers#25107](https://github.com/huggingface/transformers/pull/25107)Â andÂ [huggingface/accelerate#1777](https://github.com/huggingface/accelerate/pull/1777) solve the first challenge and requires no code changes from user side.  It does the following:

1. Create the model with no weights on all ranks (using the `meta` device).
2. Load the state dict only on rank==0 and set the model weights with that state dict on rank 0
3. For all other ranks, doÂ `torch.empty(*param.size(), dtype=dtype)`Â for every parameter onÂ `meta`Â device
4. So, rank==0 will have loaded the model with correct state dict while all other ranks will have random weights.
5. SetÂ `sync_module_states=True`Â so that FSDP object takes care of broadcasting them to all the ranks before training starts.

Below is the output snippet on a 7B model on 2 GPUs measuring the memory consumed and model parameters at various stages. We can observe that during loading the pre-trained model rank 0 & rank 1 have CPU total peak memory of `32744 MB` and `1506 MB` , respectively. Therefore, only rank 0 is loading the pre-trained model leading to efficient usage of CPU RAM. The whole logs at be found [here](https://gist.github.com/pacman100/2fbda8eb4526443a73c1455de43e20f9)","What is the total peak memory on CPU for rank 1 during loading of the pre-trained model?
",1506 MB,huggingface/blog/blob/main/ram-efficient-pytorch-fsdp.md,5.0,"The context provides specific information about the memory consumption on different ranks during the loading of a pre-trained model, mentioning that rank 0 and rank 1 have CPU total peak memories of 32744 MB and 1506 MB, respectively. This information directly answers the question regarding the total peak memory on CPU for rank 1.

",4.0,"This question seems to be related to debugging or optimization issues with a specific Hugging Face model, likely using the `transformers` library. The answer would require knowledge of the model's architecture and how memory is allocated during the loading process. A total peak memory value for rank 1 during loading could help identify potential memory bottlenecks.

",5.0,"This question appears to be self-contained, assuming knowledge of machine learning concepts such as pre-trained models and CPU memory usage. The term ""peak memory"" suggests a specific metric being asked about, and the mention of ""rank 1"" implies a particular context or parameter setting is being referred to.

"
"Currently, only `MANUAL_SEED_FN`, `EMPTY_CACHE_FN` and `DEVICE_COUNT_FN` are supported for device-specific dispatch.


### Distributed training

`pytest` can't deal with distributed training directly. If this is attempted - the sub-processes don't do the right
thing and end up thinking they are `pytest` and start running the test suite in loops. It works, however, if one
spawns a normal process that then spawns off multiple workers and manages the IO pipes.

Here are some tests that use it:

- [test_trainer_distributed.py](https://github.com/huggingface/transformers/tree/main/tests/trainer/test_trainer_distributed.py)
- [test_deepspeed.py](https://github.com/huggingface/transformers/tree/main/tests/deepspeed/test_deepspeed.py)

To jump right into the execution point, search for the `execute_subprocess_async` call in those tests.

You will need at least 2 GPUs to see these tests in action:

```bash
CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/test_trainer_distributed.py
```

### Output capture

During test execution any output sent to `stdout` and `stderr` is captured. If a test or a setup method fails, its
according captured output will usually be shown along with the failure traceback.

To disable output capturing and to get the `stdout` and `stderr` normally, use `-s` or `--capture=no`:

```bash
pytest -s tests/utils/test_logging.py
```

To send test results to JUnit format output:

```bash
py.test tests --junitxml=result.xml
```

### Color control

To have no color (e.g., yellow on white background is not readable):

```bash
pytest --color=no tests/utils/test_logging.py
```

### Sending test report to online pastebin service

Creating a URL for each test failure:

```bash
pytest --pastebin=failed tests/utils/test_logging.py
```

This will submit test run information to a remote Paste service and provide a URL for each failure. You may select
tests as usual or add for example -x if you only want to send one particular failure.","How many GPUs are needed to see the distributed training tests in action?
",2,huggingface/transformers/blob/main/docs/source/en/testing.md,4.0,"The context explains that distributed training tests in action require at least 2 GPUs, but it does not explicitly state whether more GPUs are needed or possible. It also mentions specific tests and how to run them with multiple GPUs, but it does not provide a general rule for the minimum number of GPUs required.

",4.0,"This question appears to be useful as it suggests that the user wants to test the distributed training functionality of the Hugging Face ecosystem, which requires multiple GPUs. The answer will likely involve explaining how to configure and utilize multiple GPUs for distributed training, providing essential information for users who want to scale up their models.
",5.0,"The question seems to be self-contained and requires knowledge of the concept of distributed training, which can be found through general documentation. There's no specific context or setting mentioned that would require external information.
"
"#### Predicting the language of a dataset

Once we have some examples of text from a dataset, we need to predict the language. There are various options here, but for this work, we used the [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model created by [Meta](https://huggingface.co/facebook) as part of the [No Language Left Behind](https://ai.facebook.com/research/no-language-left-behind/) work. This model can detect 217 languages which will likely represent the majority of languages for datasets hosted on the Hub.

We pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.

Once we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:

- Grouping the predictions for each dataset by language: some datasets return predictions for multiple languages. We group these predictions by the language predicted i.e. if a dataset returns predictions for English and Dutch, we group the English and Dutch predictions together.
- For datasets with multiple languages predicted, we count how many predictions we have for each language. If a language is predicted less than 20% of the time, we discard this prediction. i.e. if we have 18 predictions for English and only 2 for Dutch we discard the Dutch predictions.
- We calculate the mean score for all predictions for a language. If the mean score associated with a languages prediction is below 80% we discard this prediction.","How many languages can the facebook/fasttext-language-identification model detect?
",217,huggingface/blog/blob/main/huggylingo.md,5.0,"The context explicitly states that the model can detect 217 languages, which directly answers the question. There is no ambiguity or unclear information in the context regarding the number of languages the model can detect.
",4.0,"This question appears to be relevant to NLP developers working with the Hugging Face ecosystem, as it involves a specific pre-trained model (facebook/fasttext-language-identification) that is designed for language identification tasks. The answer would provide useful information about the capabilities of this particular model.

",5.0,"This question clearly specifies the model name (facebook/fasttext-language-identification) and asks a precise question about its capabilities, making it understandable without any additional context.
"
"|      |      |[TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ](https://huggingface.co/TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ)                                        |257         |5       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ/blob/main/LICENSE.txt)                    |                                                                                                    |             |
|      |      |[TheBloke/airoboros-l2-7B-gpt4-m2.0-GPTQ](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-m2.0-GPTQ)                                          |234         |6       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/airoboros-l2-7B-gpt4-m2.0-GPTQ/blob/main/LICENSE.txt)                     |                                                                                                    |             |
|      |      |[ixa-ehu/ixambert-base-cased](https://huggingface.co/ixa-ehu/ixambert-base-cased)                                                                  |230         |3       |                         |                                                                                   |[LICENSE](https://huggingface.co/ixa-ehu/ixambert-base-cased/blob/main/LICENSE)                                         |                                                                                                    |             |","How many parameters does the model ""TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ"" have?
",257 million,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,5.0,"This context appears to be a list of models from the Hugging Face model hub, showing information about each model such as its name, number of parameters, and licensing information. However, there is no specific mention or indication of the number of parameters for the model ""TheBloke/Airoboros-L2-70B-GPT4-m2.0-GPTQ"" beyond a numeric value that seems to correspond to this model's parameter count (257). This suggests that the question can be answered unambiguously with the given context.

",4.0,"This question seems to be useful because it requires knowledge of a specific Hugging Face model and its architecture, which suggests that the person asking this question is already familiar with the Hugging Face ecosystem and is looking for a specific piece of information. The answer can help users understand the size and complexity of the model, which can inform their decision-making when working with NLP applications.

",5.0,"The question clearly identifies a specific model, and the task of determining its number of parameters is straightforward without any additional context.
"
"Here, we will put together the above sections and use a pretrained model for inference.

First we'll need an image to do inference on. Here we load a picture of a leaf from the web:

```py
>>> import requests
>>> from PIL import Image
>>> from io import BytesIO
>>> url = 'https://datasets-server.huggingface.co/assets/imagenet-1k/--/default/test/12/image/image.jpg'
>>> image = Image.open(requests.get(url, stream=True).raw)
>>> image
```

Here's the image we loaded:

<img src=""https://datasets-server.huggingface.co/assets/imagenet-1k/--/default/test/12/image/image.jpg"" alt=""An Image from a link"" width=""300""/>

Now, we'll create our model and transforms again. This time, we make sure to set our model in evaluation mode.

```py
>>> model = timm.create_model('mobilenetv3_large_100', pretrained=True).eval()
>>> transform = timm.data.create_transform(
    **timm.data.resolve_data_config(model.pretrained_cfg)
)
```

We can prepare this image for the model by passing it to the transform.

```py
>>> image_tensor = transform(image)
>>> image_tensor.shape
torch.Size([3, 224, 224])
```

Now we can pass that image to the model to get the predictions. We use `unsqueeze(0)` in this case, as the model is expecting a batch dimension.

```py
>>> output = model(image_tensor.unsqueeze(0))
>>> output.shape
torch.Size([1, 1000])
```

To get the predicted probabilities, we apply softmax to the output. This leaves us with a tensor of shape `(num_classes,)`.

```py
>>> probabilities = torch.nn.functional.softmax(output[0], dim=0)
>>> probabilities.shape
torch.Size([1000])
```

Now we'll find the top 5 predicted class indexes and values using `torch.topk`.

```py
>>> values, indices = torch.topk(probabilities, 5)
>>> indices
tensor([162, 166, 161, 164, 167])
```

If we check the imagenet labels for the top index, we can see what the model predicted...","What is the shape of the output from the `model(image_tensor.unsqueeze(0))` call?
","torch.Size([1, 1000])",huggingface/pytorch-image-models/blob/main/hfdocs/source/quickstart.mdx,5.0,"The context provides a clear example of how to use a pre-trained model (mobilenetv3_large_100) to make predictions on an image. It shows that after passing the transformed image tensor through the model, the output is a tensor with shape [1, 1000], which represents the predicted probabilities for each class in the ImageNet dataset. This suggests that the output from `model(image_tensor.unsqueeze(0))` is indeed a tensor representing the predicted probabilities for each class.
",4.0,"The question appears to be asking about a specific detail related to using the Hugging Face model with an image tensor. It seems like the person asking this question has a good understanding of how to use the model and is looking for clarification on a particular aspect, which suggests they are already familiar with the basics of NLP development with the Hugging Face ecosystem.

",5.0,"This question makes sense by itself because it clearly specifies a model, an input (image tensor), and an operation (`unsqueeze(0)`), implying that the questioner has some knowledge about the model's behavior. However, there is no mention of any specific context or setting outside of this operation.

"
"<!--
Type: model-index
Collections:
- Name: AdvProp
  Paper:
    Title: Adversarial Examples Improve Image Recognition
    URL: https://paperswithcode.com/paper/adversarial-examples-improve-image
Models:
- Name: tf_efficientnet_b0_ap
  In Collection: AdvProp
  Metadata:
    FLOPs: 488688572
    Parameters: 5290000
    File Size: 21385973
    Architecture:
    - 1x1 Convolution
    - Average Pooling
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Dropout
    - Inverted Residual Block
    - Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AdvProp
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tf_efficientnet_b0_ap
    LR: 0.256
    Epochs: 350
    Crop Pct: '0.875'
    Momentum: 0.9
    Batch Size: 2048
    Image Size: '224'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1334
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 77.1%
      Top 5 Accuracy: 93.26%
- Name: tf_efficientnet_b1_ap
  In Collection: AdvProp
  Metadata:
    FLOPs: 883633200
    Parameters: 7790000
    File Size: 31515350
    Architecture:
    - 1x1 Convolution
    - Average Pooling
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Dropout
    - Inverted Residual Block
    - Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AdvProp
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:","What is the number of parameters in the tf_efficientnet_b0_ap model?
",5290000,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/advprop.mdx,5.0,"The context provides the exact number of parameters for the tf_efficientnet_b0_ap model, which is listed as 5290000. This information is clearly and explicitly stated in the Metadata section of the context.

",4.0,"The question appears to be asking for a specific piece of information about a pre-trained model, which can be verified by checking the model's configuration or documentation. This type of question is likely to be answered with a simple number and may not require extensive explanation.

",5.0,"This question can be understood without any additional context, as it directly asks for a specific numerical value related to a well-known model architecture. The mention of ""tf_efficientnet_b0_ap"" refers to a specific pre-trained model available in various libraries and frameworks, implying that the reader is expected to have basic knowledge of deep learning models.

"
"--
title: ""Supercharged Searching on the ğŸ¤— Hub""
thumbnail: /blog/assets/48_hubsearch/thumbnail.png
authors:
- user: muellerzr
---

# Supercharged Searching on the Hugging Face Hub


<a target=""_blank"" href=""https://colab.research.google.com/github/muellerzr/hf-blog-notebooks/blob/main/Searching-the-Hub.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

The `huggingface_hub` library is a lightweight interface that provides a programmatic approach to exploring the hosting endpoints Hugging Face provides: models, datasets, and Spaces.

Up until now, searching on the Hub through this interface was tricky to pull off, and there were many aspects of it a user had to ""just know"" and get accustomed to.

In this article, we will be looking at a few exciting new features added to `huggingface_hub` to help lower that bar and provide users with a friendly API to search for the models and datasets they want to use without leaving their Jupyter or Python interfaces.

> Before we begin, if you do not have the latest version of the `huggingface_hub` library on your system, please run the following cell:


```python
!pip install huggingface_hub -U
```

## Situating the Problem:

First, let's imagine the scenario you are in. You'd like to find all models hosted on the Hugging Face Hub for Text Classification, were trained on the GLUE dataset, and are compatible with PyTorch.

You may simply just open https://huggingface.co/models and use the widgets on there. But this requires leaving your IDE and scanning those results, all of which requires a few button clicks to get you the information you need.

What if there were a solution to this without having to leave your IDE? With a programmatic interface, it also could be easy to see this being integrated into workflows for exploring the Hub.

This is where the `huggingface_hub` comes in.","What is the name of the library that provides a lightweight interface to explore Hugging Face hosting endpoints?
",huggingface_hub,huggingface/blog/blob/main/searching-the-hub.md,5.0,"The question asks about the name of the library that provides a lightweight interface to explore Hugging Face hosting endpoints. However, based on the provided context, it's clear that the library being referred to is called ""huggingface_hub"". There's no ambiguity or uncertainty in this regard.

",4.0,"The question appears to be asking about a specific library related to the Hugging Face ecosystem, which suggests it's focused on practical information relevant to NLP developers using this framework. This level of specificity could indicate that the answer might be useful for someone trying to integrate or explore hosted endpoints in their project.

",5.0,"The question clearly refers to an existing library (Gradio) and its purpose, and it does not require any additional information from the context to be understood.
"
"The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the lowest peak memory for text generation. However, AWQ has the largest forward latency per batch size. For a more detailed discussion about the pros and cons of each quantization method, read the [Overview of natively supported quantization schemes in ğŸ¤— Transformers](https://huggingface.co/blog/overview-quantization-transformers) blog post.

### Fused AWQ modules

The [TheBloke/Mistral-7B-OpenOrca-AWQ](https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ) model was benchmarked with `batch_size=1` with and without fused modules.

<figcaption class=""text-center text-gray-500 text-lg"">Unfused module</figcaption>

|   Batch Size |   Prefill Length |   Decode Length |   Prefill tokens/s |   Decode tokens/s | Memory (VRAM)   |
|-------------:|-----------------:|----------------:|-------------------:|------------------:|:----------------|
|            1 |               32 |              32 |            60.0984 |           38.4537 | 4.50 GB (5.68%) |
|            1 |               64 |              64 |          1333.67   |           31.6604 | 4.50 GB (5.68%) |
|            1 |              128 |             128 |          2434.06   |           31.6272 | 4.50 GB (5.68%) |
|            1 |              256 |             256 |          3072.26   |           38.1731 | 4.50 GB (5.68%) |
|            1 |              512 |             512 |          3184.74   |           31.6819 | 4.59 GB (5.80%) |
|            1 |             1024 |            1024 |          3148.18   |           36.8031 | 4.81 GB (6.07%) |
|            1 |             2048 |            2048 |          2927.33   |           35.2676 | 5.73 GB (7.23%) |

<figcaption class=""text-center text-gray-500 text-lg"">Fused module</figcaption>","What is the peak memory for text generation when using AWQ quantization?
",4.50 GB (5.68%),huggingface/transformers/blob/main/docs/source/en/quantization.md,4.0,"The question asks about the peak memory for text generation when using AWQ quantization, but the provided context only mentions that AWQ quantization has the lowest peak memory for text generation without providing specific numbers. However, it does provide a table with benchmarking results for a model using fused AWQ modules, which includes information on VRAM usage (which can be interpreted as peak memory). Therefore, one can infer that the peak memory is around 5.73 GB (7.23% of VRAM) or possibly less depending on the prefill length and decode length.

",4.0,"The question is specific to a particular technique (AWQ quantization) and a specific use case (text generation), which could help narrow down the scope of possible answers. However, without more context, it's unclear what specific aspect of peak memory usage the questioner is concerned with, such as maximum memory usage during inference or training. Additionally, there may be variations in how different models and environments handle AWQ quantization, so a precise answer might require additional information about the specific model, hardware, and environment being used.
",5.0,"This question can be understood on its own because it clearly states what it's asking about (peak memory for text generation) and specifies a particular technique (AWQ quantization). The technical terms are also easily identifiable, making it clear that the question is asking about a specific aspect of model performance under a given condition.

"
"@gradio/state

## 0.1.0

### Features

- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Adds the ability to build the frontend and backend of custom components in preparation for publishing to pypi using `gradio_component build`.  Thanks [@pngwn](https://github.com/pngwn)!
- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`287fe6782`](https://github.com/gradio-app/gradio/commit/287fe6782825479513e79a5cf0ba0fbfe51443d7) - Publish all components to npm.  Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.0-beta.2

### Features

- [#6016](https://github.com/gradio-app/gradio/pull/6016) [`83e947676`](https://github.com/gradio-app/gradio/commit/83e947676d327ca2ab6ae2a2d710c78961c771a0) - Format js in v4 branch.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!

## 0.1.0-beta.1

### Features

- [#5960](https://github.com/gradio-app/gradio/pull/5960) [`319c30f3f`](https://github.com/gradio-app/gradio/commit/319c30f3fccf23bfe1da6c9b132a6a99d59652f7) - rererefactor frontend files.  Thanks [@pngwn](https://github.com/pngwn)!
- [#5498](https://github.com/gradio-app/gradio/pull/5498) [`85ba6de13`](https://github.com/gradio-app/gradio/commit/85ba6de136a45b3e92c74e410bb27e3cbe7138d7) - Adds the ability to build the frontend and backend of custom components in preparation for publishing to pypi using `gradio_component build`.  Thanks [@pngwn](https://github.com/pngwn)!

## 0.0.2-beta.0

### Features

- [#5648](https://github.com/gradio-app/gradio/pull/5648) [`c573e2339`](https://github.com/gradio-app/gradio/commit/c573e2339b86c85b378dc349de5e9223a3c3b04a) - Publish all components to npm.  Thanks [@freddyaboulton](https://github.com/freddyaboulton)!","What is the version of gradio that adds the ability to build the frontend and backend of custom components in preparation for publishing to pypi using `gradio_component build`?
",0.1.0,gradio-app/gradio/blob/main/js/state/CHANGELOG.md,5.0,"The context provides a detailed changelog of the gradio library, specifically highlighting the version (0.1.0) that adds the ability to build the frontend and backend of custom components in preparation for publishing to pypi using `gradio_component build`. This information is clearly stated in multiple places throughout the changelog.

",4.0,"The question seems to be asking about a specific feature or functionality in Gradio, but it's not entirely clear what the version number is being requested. However, based on my understanding, I believe the answer might be related to Gradio 3.0, which introduced significant changes and improvements, including better support for building custom components and publishing them as packages.
",5.0,"This question requires knowledge about Gradio, a library used for building web applications. It specifically asks about a particular feature or version of Gradio that enables building custom components and preparing them for publication on PyPI (Python Package Index) using the `gradio_component build` command. The question is clear and unambiguous, but it assumes familiarity with Gradio's capabilities and its packaging processes.
"
"| Notebook     |      Description      |   |   |
|:----------|:-------------|:-------------|------:|
| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|
| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)|
| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)|","How to fine tune a model on text classification?
",Show how to preprocess the data and fine-tune a pretrained model on any GLUE task.,huggingface/transformers/blob/main/notebooks/README.md,5.0,"The context directly points to a specific notebook titled ""How to fine-tune a model on text classification"", which is likely to contain detailed information and code for the task. Additionally, the presence of other notebooks related to training tokenizers and language models suggests that the authors have experience in text-related tasks, lending credibility to the notebook on fine-tuning a model.

",4.0,"This question appears to be very general and does not provide enough context or specificity about what aspect of fine-tuning a model on text classification they are struggling with. However, it's still a valid and relevant topic for machine learning developers working with the Hugging Face ecosystem.
",5.0,"The question asks about the process of fine-tuning a model, which is a general concept that can be understood without referencing specific context. It's clear that the questioner wants to know how to adapt a pre-trained model for text classification tasks.

"
"[`snapshot_download`] downloads the latest revision by default. If you want a specific repository revision, use the
`revision` parameter:

```python
>>> from huggingface_hub import snapshot_download
>>> snapshot_download(repo_id=""lysandre/arxiv-nlp"", revision=""refs/pr/1"")
```

### Filter files to download

[`snapshot_download`] provides an easy way to download a repository. However, you don't always want to download the
entire content of a repository. For example, you might want to prevent downloading all `.bin` files if you know you'll
only use the `.safetensors` weights. You can do that using `allow_patterns` and `ignore_patterns` parameters.

These parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing
patterns) as documented [here](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). The pattern matching is
based on [`fnmatch`](https://docs.python.org/3/library/fnmatch.html).

For example, you can use `allow_patterns` to only download JSON configuration files:

```python
>>> from huggingface_hub import snapshot_download
>>> snapshot_download(repo_id=""lysandre/arxiv-nlp"", allow_patterns=""*.json"")
```

On the other hand, `ignore_patterns` can exclude certain files from being downloaded. The
following example ignores the `.msgpack` and `.h5` file extensions:

```python
>>> from huggingface_hub import snapshot_download
>>> snapshot_download(repo_id=""lysandre/arxiv-nlp"", ignore_patterns=[""*.msgpack"", ""*.h5""])
```

Finally, you can combine both to precisely filter your download. Here is an example to download all json and markdown
files except `vocab.json`.

```python
>>> from huggingface_hub import snapshot_download
>>> snapshot_download(repo_id=""gpt2"", allow_patterns=[""*.md"", ""*.json""], ignore_patterns=""vocab.json"")
```

## Download file(s) to local folder","What is the default revision downloaded by `snapshot_download`?
",The latest revision.,huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md,4.0,"The context clearly states that `snapshot_download` downloads the latest revision by default, but does not explicitly mention what the ""default revision"" is. However, based on general understanding of version control systems, it can be inferred that the default revision is likely the most recent one, which is also the case when no specific revision is provided.
",4.0,"This question appears to be very specific and relevant to the Hugging Face ecosystem, particularly to users who are working with the Transformers library. The `snapshot_download` function is used to download pre-trained models from a given repository or snapshot, but it does not specify what revision is downloaded by default unless otherwise stated. Therefore, this question is likely looking for clarification on the default behavior of the function.
",5.0,"This question does not refer to any specific context, model, or setting. It asks about a specific function (`snapshot_download`) and its behavior, which can be understood without additional information.
"
"# Check the model output on a sample from evaluation dataset


```python
import random

i = random.randint(0, len(dataset[""test""]))
context = dataset[""test""][i][""context""]

batch = tokenizer(context, return_tensors=""pt"")
batch = {k: v.to(""cuda"") for k, v in batch.items()}
model.eval()
output_tokens = model.generate(
    **batch,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.2,
    top_p=0.95,
    top_k=50,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)
target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(""<|endcontext|>"")[1]
target = dataset[""test""][i][""target""]
print(f""{context=} \n\n {target_predicted=} \n\n {target=}"")
```

# Save the Adapter model

When the lora layers are applied to embedding layers, the corresponding base model embedding layers are also saved.


```python
trainer.push_to_hub()
trainer.model.push_to_hub(training_args.output_dir)
```

# Check the model loading is working as expected and generating plausible outputs.


```python
from peft import PeftModel

inference_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    # use_flash_attention_2=True,
)
inference_model.resize_token_embeddings(len(tokenizer))

inference_model = PeftModel.from_pretrained(inference_model, ""smangrul/mistral_lora_clm_with_added_tokens"")
inference_model.to(""cuda"")
inference_model.eval()

output_tokens = inference_model.generate(
    **batch,
    max_new_tokens=256,
    do_sample=True,
    temperature=0.2,
    top_p=0.95,
    top_k=50,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

target_predicted = tokenizer.decode(output_tokens[0], skip_special_tokens=False).split(""<|endcontext|>"")[1]
print(f""{context=} \n\n {target_predicted=} \n\n {target=}"")
```","What is the maximum number of new tokens that can be generated by the model?
",256,huggingface/peft/blob/main/examples/causal_language_modeling/peft_lora_clm_with_additional_tokens.ipynb,5.0,"The context snippet appears to be code excerpts from a larger program, and the relevant information about generating new tokens by the model is scattered across different sections. However, there's a specific line of code where it says `max_new_tokens=256`, which directly implies that the maximum number of new tokens that can be generated by the model in this particular instance (or configuration) is 256.

",4.0,"This question seems to be relevant to understanding the limitations or capabilities of a specific model in the Hugging Face ecosystem, likely related to autoregressive language models like T5 or BART. The answer could provide valuable information for developers working with these models, particularly when it comes to planning and implementing tasks that involve generating new text.
",5.0,"The question explicitly mentions ""the model"", which is a clear reference to an entity with specific properties. However, it does not require any additional context to understand what is being asked - it is asking for a general property of the model.
"
"## Send us feedback!

At Hugging Face, weâ€™re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. Weâ€™ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.

One of the most useful things you can contribute as part of the community is to send us feedback! Weâ€™d love to hear from you on top priorities for model evaluation. Let us know your feedback and feature requests by posting on the Evaluation on the Hub [Community](https://huggingface.co/spaces/autoevaluate/model-evaluator/discussions) tab, or the [forums](https://discuss.huggingface.co/)!","What is one way to contribute to the Hugging Face community for model evaluation?
",Posting on the Evaluation on the Hub Community tab.,huggingface/blog/blob/main/zero-shot-eval-on-the-hub.md,5.0,"The context explicitly mentions that one way to contribute to the Hugging Face community for model evaluation is to send feedback by posting on the Evaluation on the Hub Community tab or the forums. This answer is clear and unambiguous, making it easy to determine what action is being suggested.

",5.0,"This question appears to be quite relevant to machine learning developers using the Hugging Face ecosystem, as it involves contributing back to the community. The specific request for ""model evaluation"" suggests that the user is interested in helping to improve the accuracy and reliability of NLP models, which could have a direct impact on their own work or other users' experiences.

",5.0,"The question refers to a specific task (model evaluation) within a particular community (Hugging Face), but it does not require additional context or information about a specific setting. An operator with access to documentation can understand that contributing to the Hugging Face community involves various activities, and evaluating models is one of them.

"
"- Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AdvProp
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tf_efficientnet_b4_ap
    LR: 0.256
    Epochs: 350
    Crop Pct: '0.922'
    Momentum: 0.9
    Batch Size: 2048
    Image Size: '380'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1374
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ap-dedb23e6.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 83.26%
      Top 5 Accuracy: 96.39%
- Name: tf_efficientnet_b5_ap
  In Collection: AdvProp
  Metadata:
    FLOPs: 13176501888
    Parameters: 30390000
    File Size: 122403150
    Architecture:
    - 1x1 Convolution
    - Average Pooling
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Dropout
    - Inverted Residual Block
    - Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AdvProp
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tf_efficientnet_b5_ap
    LR: 0.256
    Epochs: 350
    Crop Pct: '0.934'
    Momentum: 0.9
    Batch Size: 2048
    Image Size: '456'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1384","What is the LR used in training tf_efficientnet_b5_ap?
",0.256,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/advprop.mdx,5.0,"The context provides specific details about the training process of a particular neural network model, tf_efficientnet_b5_ap. It includes hyperparameters such as learning rate (LR), number of epochs, batch size, image size, and others. Given that one of the entries in the metadata section directly mentions ""LR"" along with its value (0.256) for the model tf_efficientnet_b5_ap, it is clear that the LR used during training can be easily determined from the provided context.

",4.0,"This question appears to be relevant to machine learning developers working with the Hugging Face ecosystem, as it involves a specific architecture (tf_efficientnet_b5_ap) and asks about a hyperparameter (learning rate, or LR). The answer would likely involve understanding the default settings or common practices for training this particular model.
",5.0,"The question asks about the Learning Rate (LR) used in training a specific model, ""tf_efficientnet_b5_ap"". The model name suggests it's a PyTorch model, and the context implies that this is a technical discussion about machine learning. However, the question doesn't require additional information beyond recognizing the model name to understand what is being asked.

"
">>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
>>> model = AutoModel.from_pretrained(""bert-base-uncased"")

>>> inputs = tokenizer(""Hello world!"", return_tensors=""pt"")
>>> outputs = model(**inputs)
```

And here is the equivalent code for TensorFlow:
```python
>>> from transformers import AutoTokenizer, TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
>>> model = TFAutoModel.from_pretrained(""bert-base-uncased"")

>>> inputs = tokenizer(""Hello world!"", return_tensors=""tf"")
>>> outputs = model(**inputs)
```

The tokenizer is responsible for all the preprocessing the pretrained model expects and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.

The model itself is a regular [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) or a [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (depending on your backend) which you can use as usual. [This tutorial](https://huggingface.co/docs/transformers/training) explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our `Trainer` API to quickly fine-tune on a new dataset.

## Why should I use transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding & generation, computer vision, and audio tasks.
    - Low barrier to entry for educators and practitioners.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.","What is the difference in how tensors are returned by the tokenizer when used with PyTorch versus TensorFlow?
",tf.,huggingface/transformers/blob/main/README.md,4.0,"The context provides information about the usage of `AutoTokenizer` and `TFAutoModel` in both PyTorch and TensorFlow environments, but it does not explicitly mention how tensors are returned by the tokenizer. However, based on general knowledge of the Hugging Face Transformers library, it can be inferred that when using `return_tensors=""pt""` with PyTorch, the tokenizer will return PyTorch tensors, while using `return_tensors=""tf""` with TensorFlow will return TensorFlow tensors.

",4.0,"This question seems to be focused on a specific detail of working with Hugging Face's transformers and tokenizers, specifically comparing their behavior across different deep learning frameworks (PyTorch vs. TensorFlow). The ability to understand these differences is crucial for developers who plan to integrate the Hugging Face ecosystem into their projects that require compatibility with either or both of these frameworks.

",5.0,"This question can be understood without any additional context about a specific document, setting, or previous conversation. It clearly refers to a common scenario where a tokenizer from a model library (like Hugging Face Transformers) is used in conjunction with different deep learning frameworks (PyTorch and TensorFlow). The question implies that there might be differences in how tensors are handled or returned when using these two frameworks, which is something an operator familiar with the subject matter could understand without needing further context.

"
"<p align=""center"">
    <a href=""https://circleci.com/gh/huggingface/transformers"">
        <img alt=""Build"" src=""https://img.shields.io/circleci/build/github/huggingface/transformers/main"">
    </a>
    <a href=""https://github.com/huggingface/transformers/blob/main/LICENSE"">
        <img alt=""GitHub"" src=""https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"">
    </a>
    <a href=""https://huggingface.co/docs/transformers/index"">
        <img alt=""Documentation"" src=""https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"">
    </a>
    <a href=""https://github.com/huggingface/transformers/releases"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/huggingface/transformers.svg"">
    </a>
    <a href=""https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"">
        <img alt=""Contributor Covenant"" src=""https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"">
    </a>
    <a href=""https://zenodo.org/badge/latestdoi/155220641""><img src=""https://zenodo.org/badge/155220641.svg"" alt=""DOI""></a>
</p>","What is the license of the transformers library?
",MIT,huggingface/transformers/blob/main/README_pt-br.md,5.0,"The context provided is a summary of the transformers library's GitHub page, featuring various badges and links to its documentation, releases, license, and more. The license badge specifically points to the GitHub repository's LICENSE file, which contains the licensing information for the library. Given this context, it is possible to unambiguously determine the license of the transformers library by clicking on the ""GitHub"" badge or accessing the LICENSE file linked in the context.

",5.0,"This question appears to be straightforward and aimed at understanding the licensing terms associated with the transformers library, which is a crucial piece of information for developers who plan to use or distribute the library in their applications. It's practical and directly applicable to developers working with the Hugging Face ecosystem.

",5.0,"This question can be understood on its own without needing additional information or context about a specific setting or project, as it directly refers to a known library (transformers) and asks for its licensing details.
"
"Downloading datasets

## Integrated libraries

If a dataset on the Hub is tied to a [supported library](./datasets-libraries), loading the dataset can be done in just a few lines. For information on accessing the dataset, you can click on the ""Use in dataset library"" button on the dataset page to see how to do so. For example, [`samsum`](https://huggingface.co/datasets/samsum?library=true) shows how to do so with ğŸ¤— Datasets below.

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-usage.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-usage-dark.png""/>
</div>

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-usage-modal.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-usage-modal-dark.png""/>
</div>

## Using the Hugging Face Client Library

You can use the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub) library to create, delete, update and retrieve information from repos. You can also download files from repos or integrate them into your library! For example, you can quickly load a CSV dataset with a few lines using Pandas.

```py
from huggingface_hub import hf_hub_download
import pandas as pd

REPO_ID = ""YOUR_REPO_ID""
FILENAME = ""data.csv""

dataset = pd.read_csv(
    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=""dataset"")
)
```

## Using Git

Since all datasets on the Hub are Git repositories, you can clone the datasets locally by running:

```bash
git lfs install
git clone git@hf.co:datasets/<dataset ID> # example: git clone git@hf.co:datasets/allenai/c4
```","What is the command to download a file from a repository using huggingface_hub library?
","hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=""dataset"")",huggingface/hub-docs/blob/main/docs/hub/datasets-downloading.md,4.0,"The context provides a specific code snippet that demonstrates how to download a file from a repository using the `huggingface_hub` library. This makes it clear and unambiguous how to accomplish this task. However, the question asks for the ""command"" which is more of a shell/bash command level query, whereas the provided code snippet appears to be written in Python. Furthermore, although the context does mention downloading files using the `huggingface_hub` library, it's focused on datasets and not directly answering the question about file download from a repository.

",4.0,"This question shows a good understanding of the Hugging Face ecosystem and specifically asks about a common task involving the `huggingface_hub` library. The answer to this question would likely provide valuable information for machine learning developers working with NLP applications, such as how to retrieve pre-trained models or other resources from the Hub.
",5.0,"The question clearly states what it's asking, which is the specific command to perform a particular action (downloading a file) using a well-defined library (huggingface_hub). This makes the question self-contained and easy to understand without needing additional context.

"
"1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (æ¥è‡ª SenseTime Research) ä¼´éšè®ºæ–‡ [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) ç”± Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai å‘å¸ƒã€‚
1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) ç”± Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, HervÃ© JÃ©gou å‘å¸ƒã€‚
1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (æ¥è‡ª Google AI) ä¼´éšè®ºæ–‡ [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) ç”± Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun å‘å¸ƒã€‚
1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (æ¥è‡ª The University of Texas at Austin) ä¼´éšè®ºæ–‡ [NMS Strikes Back](https://arxiv.org/abs/2212.06137) ç”± Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp KrÃ¤henbÃ¼hl å‘å¸ƒã€‚
1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (æ¥è‡ª Facebook) ä¼´éšè®ºæ–‡ [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) ç”± Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko å‘å¸ƒã€‚
1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (æ¥è‡ª Microsoft Research) ä¼´éšè®ºæ–‡ [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) ç”± Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan å‘å¸ƒã€‚","Who is the author of the paper ""End-to-End Object Detection with Transformers""?
",Nicolas Carion.,huggingface/transformers/blob/main/README_zh-hans.md,5.0,"The context provides a list of papers and their corresponding authors, but it does not specifically mention the paper ""End-to-End Object Detection with Transformers"". However, one of the papers in the list is mentioned as having the same title, which is ""End-to-End Object Detection with Transformers"" by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. The context provides a clear link between the paper's title and its authors.
",4.0,"This question appears to be a trivia or knowledge-based question, but it can still be useful for machine learning developers building NLP applications with the Hugging Face ecosystem because the authors of influential papers like ""End-to-End Object Detection with Transformers"" are often experts in their field and have contributed significantly to the development of transformer models. Knowing who the author is might provide insight into the context and potential future directions of research related to object detection using transformers.

",4.0,"The question refers to a specific paper by its title, which implies that it requires background knowledge about the paper and its authors. However, it does not require any additional context beyond the paper's title itself.
"
"![output](https://github.com/pngwn/MDsveX/assets/12937446/ef108f0b-0e84-4292-9984-9dc66b3e144d)

For more information check the [`FileExplorer` documentation](https://gradio.app/docs/fileexplorer).

Thanks [@aliabid94](https://github.com/aliabid94)!","What is the name of the Gradio component used for file uploads?
",FileExplorer,gradio-app/gradio/blob/main/js/fileexplorer/CHANGELOG.md,5.0,"The context explicitly mentions a link to the FileExplorer documentation, which directly answers the question. Additionally, the code snippet shows an output that is likely related to the FileExplorer component.
",5.0,"The question is quite specific to the Hugging Face ecosystem, specifically related to Gradio components. It shows a clear understanding of what is being asked and demonstrates an attempt to seek information about a particular aspect of Gradio.
",5.0,"This question makes sense by itself because it refers to a specific entity (Gradio component) without any additional context, and it's clear what is being asked.
"
"The abstract from the paper is the following:

*Large-scale language models show promising text generation capabilities, but users cannot easily control particular
aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model,
trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were
derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while
providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the
training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data
via model-based source attribution.*

This model was contributed by [keskarnitishr](https://huggingface.co/keskarnitishr). The original code can be found
[here](https://github.com/salesforce/ctrl).

## Usage tips","What is the size of the parameter count of the CTRL language model?
",1.63 billion,huggingface/transformers/blob/main/docs/source/en/model_doc/ctrl.md,5.0,"The context explicitly mentions the parameter count of the CTRL language model, stating it has ""1.63 billion parameters"", which directly answers the question.
",5.0,"The question seems to be about retrieving specific information regarding the Hugging Face ecosystem, specifically the parameter count of a particular model. This type of detail can be crucial for understanding resource requirements and performance expectations in NLP applications.

",5.0,"The question asks about the specific attribute (parameter count) of a well-known object (CTRL language model), which can be understood without any additional context.
"
"|                                  Model                                   | PyTorch support | TensorFlow support | Flax Support |
|:------------------------------------------------------------------------:|:---------------:|:------------------:|:------------:|
|                        [ALBERT](model_doc/albert)                        |       âœ…        |         âœ…         |      âœ…      |
|                         [ALIGN](model_doc/align)                         |       âœ…        |         âŒ         |      âŒ      |
|                       [AltCLIP](model_doc/altclip)                       |       âœ…        |         âŒ         |      âŒ      |
| [Audio Spectrogram Transformer](model_doc/audio-spectrogram-transformer) |       âœ…        |         âŒ         |      âŒ      |
|                    [Autoformer](model_doc/autoformer)                    |       âœ…        |         âŒ         |      âŒ      |
|                          [Bark](model_doc/bark)                          |       âœ…        |         âŒ         |      âŒ      |
|                          [BART](model_doc/bart)                          |       âœ…        |         âœ…         |      âœ…      |
|                       [BARThez](model_doc/barthez)                       |       âœ…        |         âœ…         |      âœ…      |
|                       [BARTpho](model_doc/bartpho)                       |       âœ…        |         âœ…         |      âœ…      |
|                          [BEiT](model_doc/beit)                          |       âœ…        |         âŒ         |      âœ…      |
|                          [BERT](model_doc/bert)                          |       âœ…        |         âœ…         |      âœ…      |
|               [Bert Generation](model_doc/bert-generation)               |       âœ…        |         âŒ         |      âŒ      |
|                 [BertJapanese](model_doc/bert-japanese)                  |       âœ…        |         âœ…         |      âœ…      |","Is BERT supported by Flax?
",Yes.,huggingface/transformers/blob/main/docs/source/en/index.md,5.0,"The context provides a table with a list of models, including BERT, and indicates whether each model is supported by PyTorch, TensorFlow, or Flax. The table clearly shows that BERT is supported by both PyTorch and TensorFlow, but the relevant information for answering the question is that BERT is also supported by Flax.

",4.0,"This question is useful because it directly asks about the compatibility of BERT with a specific library (Flax), which is crucial information for developers who want to use BERT within the Hugging Face ecosystem. The answer will help them determine if they can integrate BERT into their project using Flax.

",4.0,"This question is clear and concise, but it assumes that the reader is familiar with BERT and Flax. The question doesn't provide any additional context or information about what BERT and Flax are, which might make it unclear for someone without prior knowledge.

"
"1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.
1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.
1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.
1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (from Salesforce) released with the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.
1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).
1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.","Who is one of the authors of the paper ""Recipes for building an open-domain chatbot""?
",Stephen Roller,huggingface/transformers/blob/main/README.md,5.0,"The context explicitly mentions the authors of the paper ""Recipes for building an open-domain chatbot"" as Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, and Jason Weston.
",4.0,"The question appears to be asking about a specific author associated with a particular research paper. This type of question could potentially be useful in understanding the credentials and expertise behind a certain NLP application or technique, which is relevant to machine learning developers working within the Hugging Face ecosystem.
",4.0,"This question can be understood by itself, as it refers to a specific paper with a well-known title. However, without additional context or knowledge about the paper's authors, it might not be possible to determine which author is being asked about. Nevertheless, this limitation does not seem to significantly impact the overall understanding of the question.
"
"[^6]: See Appendix A.

[^7]: See GSA / US Census Bureau Collaboration on Model Card Generator.

[^8]: By â€œHugging Face models,â€ we mean models shared by Hugging Face, not another organisation, on the Hub. Formally, these are models without a â€˜/â€™ in their model ID.","What does ""Hugging Face models"" specifically refer to?
","Models shared by Hugging Face, not another organisation, on the Hub.",huggingface/hub-docs/blob/main/docs/hub/model-card-landscape-analysis.md,5.0,"The context clearly specifies that ""Hugging Face models"" refers to models shared by Hugging Face on its Hub platform, specifically those models whose IDs do not contain a forward slash (/). This definition is explicit and unambiguous.
",5.0,"This question seems straightforward and essential for machine learning developers who are new to the Hugging Face ecosystem. Understanding what ""Hugging Face models"" refers to is crucial to grasp other concepts and functionalities in the Hugging Face library, such as Transformers, Tokenizers, and Pipelines.

",5.0,"This question refers to a specific term from the domain of natural language processing, but it doesn't rely on any external context. An operator should be able to understand what Hugging Face models are without needing additional information.

"
"## XLMForMultipleChoice

[[autodoc]] XLMForMultipleChoice
    - forward

## XLMForTokenClassification

[[autodoc]] XLMForTokenClassification
    - forward

## XLMForQuestionAnsweringSimple

[[autodoc]] XLMForQuestionAnsweringSimple
    - forward

## XLMForQuestionAnswering

[[autodoc]] XLMForQuestionAnswering
    - forward

</pt>
<tf>

## TFXLMModel

[[autodoc]] TFXLMModel
    - call

## TFXLMWithLMHeadModel

[[autodoc]] TFXLMWithLMHeadModel
    - call

## TFXLMForSequenceClassification

[[autodoc]] TFXLMForSequenceClassification
    - call

## TFXLMForMultipleChoice

[[autodoc]] TFXLMForMultipleChoice
    - call

## TFXLMForTokenClassification

[[autodoc]] TFXLMForTokenClassification
    - call

## TFXLMForQuestionAnsweringSimple

[[autodoc]] TFXLMForQuestionAnsweringSimple
    - call

</tf>
</frameworkcontent>","What is the name of the library for which XLMForMultipleChoice and TFXLMForMultipleChoice classes are defined?
",TensorFlow.,huggingface/transformers/blob/main/docs/source/en/model_doc/xlm.md,5.0,"The context clearly lists the classes XLMForMultipleChoice and TFXLMForMultipleChoice, along with their respective forward methods. Since these classes are listed together in a section titled ""XLM Models"", it can be inferred that they belong to the same library.

",4.0,"The question appears to be asking about specific class names from a well-known NLP library, suggesting that it may be related to Hugging Face Transformers. This is a relatively narrow and technical query, likely relevant to developers working with the XLM (cross-lingual language model) architecture in the context of multiple-choice tasks.

",5.0,"The question refers to specific classes (XLMForMultipleChoice and TFXLMForMultipleChoice) but does not specify any particular context, such as a document or section. However, the mention of XLM and TFXLM implies that the question is about models related to cross-lingual language understanding from the Transformers library by Hugging Face.

"
"Let's take a look at all available Datasets on the [Hugging Face Hub](https://huggingface.co/datasets). On the left side, you can filter the datasets according to *Task Categories* as well as *Tasks* which are more specific. Our use case corresponds to *Text Classification* -> *Sentiment Analysis* so let's select [these filters](https://huggingface.co/datasets?task_categories=task_categories:text-classification&task_ids=task_ids:sentiment-classification&sort=downloads). We are left with *ca.* 80 datasets at the time of writing this notebook. Two aspects should be evaluated when picking a dataset:

-   **Quality**: Is the dataset of high quality? More specifically: Does the data correspond to the data you expect to deal with in your use case? Is the data diverse, unbiased, ...?
-   **Size**: How big is the dataset? Usually, one can safely say the bigger the dataset, the better.

It's quite tricky to evaluate whether a dataset is of high quality efficiently, and it's even more challenging to know whether and how the dataset is biased.
An efficient and reasonable heuristic for high quality is to look at the download statistics. The more downloads, the more usage, the higher chance that the dataset is of high quality. The size is easy to evaluate as it can usually be quickly read upon. Let's take a look at the most downloaded datasets:

-   [Glue](https://huggingface.co/datasets/glue)
-   [Amazon polarity](https://huggingface.co/datasets/amazon_polarity)
-   [Tweet eval](https://huggingface.co/datasets/tweet_eval)
-   [Yelp review full](https://huggingface.co/datasets/yelp_review_full)
-   [Amazon reviews multi](https://huggingface.co/datasets/amazon_reviews_multi)","How many datasets are available for Text Classification -> Sentiment Analysis on the Hugging Face Hub?
",80,huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md,5.0,"The context clearly states that after filtering the datasets according to ""Text Classification"" and then ""Sentiment Analysis"", there are approximately 80 datasets available on the Hugging Face Hub. This information directly answers the question, making it unambiguously answerable.
",4.0,"This question is useful as it provides a clear and specific request for information about the number of available datasets on the Hugging Face Hub. Knowing the availability and variety of datasets can help machine learning developers make informed decisions when choosing tasks, models, and training data for their NLP applications.

",5.0,"The question clearly specifies a particular task (Text Classification -> Sentiment Analysis) and a platform (Hugging Face Hub), making it self-contained. There is no implicit mention of additional context, such as a specific document or setting.

"
"1. **[Phi](https://huggingface.co/docs/main/transformers/model_doc/phi)** (from Microsoft Research) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CÃ©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, SÃ©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, SÃ©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.
1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.
1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.
1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.
1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.","Who is the author of the paper ""Unified Pre-training for Program Understanding and Generation""?
",Wasi Uddin Ahmad.,huggingface/transformers/blob/main/README_ru.md,5.0,"The context provides a list of pre-trained models released with various papers. One of the papers mentioned is ""Unified Pre-training for Program Understanding and Generation"" by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang. This information directly answers the question about the authorship of the paper in question.
",4.0,"This question seems to be relevant to the topic of NLP with Hugging Face, but it's not directly related to a specific task or concept within the ecosystem. However, understanding the authorship of a research paper like ""Unified Pre-training for Program Understanding and Generation"" could be important for staying up-to-date on recent advancements in program synthesis and generation, which might have applications within the Hugging Face community.

",5.0,"The question explicitly mentions a specific research paper, but it does not require any additional context beyond that to be understood. It is clear that the question is asking about the authorship of a particular publication.
"
"This page provides an overview of the different speech and audio, computer vision, and NLP tasks that can be solved with the ğŸ¤— Transformers library in just three lines of code!

## Audio

Audio and speech processing tasks are a little different from the other modalities mainly because audio as an input is a continuous signal. Unlike text, a raw audio waveform can't be neatly split into discrete chunks the way a sentence can be divided into words. To get around this, the raw audio signal is typically sampled at regular intervals. If you take more samples within an interval, the sampling rate is higher, and the audio more closely resembles the original audio source.

Previous approaches preprocessed the audio to extract useful features from it. It is now more common to start audio and speech processing tasks by directly feeding the raw audio waveform to a feature encoder to extract an audio representation. This simplifies the preprocessing step and allows the model to learn the most essential features.

### Audio classification

Audio classification is a task that labels audio data from a predefined set of classes. It is a broad category with many specific applications, some of which include:

* acoustic scene classification: label audio with a scene label (""office"", ""beach"", ""stadium"")
* acoustic event detection: label audio with a sound event label (""car horn"", ""whale calling"", ""glass breaking"")
* tagging: label audio containing multiple sounds (birdsongs, speaker identification in a meeting)
* music classification: label music with a genre label (""metal"", ""hip-hop"", ""country"")

```py
>>> from transformers import pipeline","What is typically done to the raw audio signal before feeding it into a feature encoder?
",It is typically sampled at regular intervals.,huggingface/transformers/blob/main/docs/source/en/task_summary.md,4.0,"The context explains that the raw audio signal is typically sampled at regular intervals to address its continuous nature. It also mentions that previous approaches preprocessed the audio to extract useful features, implying that sampling is a common preprocessing step before feeding the raw audio into a feature encoder.

",4.0,"The question appears to be asking about a fundamental step in processing audio data for NLP applications using Hugging Face models. A useful question would help clarify any specific preprocessing techniques commonly used with their ecosystem, which could aid developers in optimizing model performance and accuracy.
",5.0,"This question requires some basic knowledge of audio processing and deep learning, but the context is not explicitly stated. The terms ""raw audio signal"" and ""feature encoder"" are technical, but they can be understood without additional information, such as specific settings or documents.

"
"Normalization and pre-tokenization[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames=""absolute z-10 right-0 top-0""
  notebooks={[
    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb""},
    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb""},
]} />

Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we'll first take a look at the preprocessing that each tokenizer applies to text. Here's a high-level overview of the steps in the tokenization pipeline:

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg"" alt=""The tokenization pipeline."">
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg"" alt=""The tokenization pipeline."">
</div>

Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: _normalization_ and _pre-tokenization_.

## Normalization[[normalization]]

<Youtube id=""4IIC2jI9CaU""/>

The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you're familiar with [Unicode normalization](http://www.unicode.org/reports/tr15/) (such as NFC or NFKC), this is also something the tokenizer may apply.

The ğŸ¤— Transformers `tokenizer` has an attribute called `backend_tokenizer` that provides access to the underlying tokenizer from the ğŸ¤— Tokenizers library:

```py
from transformers import AutoTokenizer","What are the two preprocessing steps that a tokenizer applies to text before splitting it into subtokens?
",Normalization and pre-tokenization.,huggingface/course/blob/main/chapters/en/chapter6/4.mdx,5.0,"The context clearly states that a tokenizer applies two preprocessing steps before splitting text into subtokens, which are normalization and pre-tokenization. Normalization is defined as cleaning up text by removing unnecessary whitespace, lowercasing, and/or removing accents, whereas pre-tokenization is not explicitly described in the provided snippet but can be inferred from the tokenization pipeline diagram to be a step before splitting text into subtokens.

",4.0,"This question is quite specific, but still seems like a good foundation for understanding how tokenizers work. The Hugging Face ecosystem uses BPE (Byte-Pair Encoding) and WordPiece encoding as two common preprocessing steps in their tokenizers, so this question could be useful if someone wants to learn about the basics of text preprocessing before using Hugging Face models.

",5.0,"The question is clear and self-contained, asking about a specific process (preprocessing) applied by a well-defined entity (tokenizer). It doesn't rely on any external context or information.

"
"**ZeRO Stage-3 CPU Offload DeepSpeed Config File Example**
```bash
compute_environment: LOCAL_MACHINE
deepspeed_config:
 deepspeed_config_file: /path/to/zero3_offload_config_accelerate.json
 zero3_init_flag: true
distributed_type: DEEPSPEED
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 2
use_cpu: false
```

Now, run below command for training:
```bash
accelerate launch run_clm_no_trainer.py \
--config_name ""gpt2-xl"" \
--tokenizer_name ""gpt2-xl"" \
--dataset_name ""wikitext"" \
--dataset_config_name ""wikitext-2-raw-v1"" \
--block_size 128 \
--output_dir ""/tmp/clm_deepspeed_stage3_offload__accelerate"" \
--learning_rate 5e-4 \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 1 \
--num_train_epochs 1 \
--with_tracking \
--report_to ""wandb""\
```

---
| Method | Batch Size Max | Train time per epoch (seconds) | Notes |
| --- | --- | --- | --- |
| DDP (Distributed Data Parallel) | - | - | OOM Error
| DeepSpeed ZeRO Stage 3 | **16** | 6608.35 | |


Table 3: Benchmarking DeepSpeed ZeRO Stage-3 CPU Offload on GPT-XL (1.5B) model

---
DDP will result in OOM error even with batch size 1. On the other hand, with DeepSpeed ZeRO Stage-3 CPU offload, we can train with a batch size of 16.

Finally, please, remember that, ğŸ¤—  `Accelerate` only integrates DeepSpeed, therefore if you
have any problems or questions with regards to DeepSpeed usage, please, file an issue with [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues).

# References

[1] [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](http://nlp.cs.berkeley.edu/pubs/Li-Wallace-Shen-Lin-Keutzer-Klein-Gonzalez_2020_Transformers_paper.pdf)

[2] [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054v3.pdf)","What is the maximum batch size that can be trained with DeepSpeed ZeRO Stage 3 CPU offload on a GPT-XL (1.5B) model?
",16,huggingface/blog/blob/main/accelerate-deepspeed.md,5.0,"The context provides a clear example of DeepSpeed ZeRO Stage 3 CPU offload configuration and a benchmark table showing the maximum batch size that can be trained with this method on a GPT-XL (1.5B) model, which is 16. Additionally, the context explains the difference in performance between DDP and DeepSpeed ZeRO Stage 3, indicating that DeepSpeed ZeRO Stage 3 can handle larger batch sizes without running out of memory.

",4.0,"This question appears to be specific and technical, targeting a particular use case within the Hugging Face ecosystem. To provide an accurate answer, one would need in-depth knowledge of DeepSpeed ZeRO Stage 3, its CPU offload capabilities, and the computational requirements of the GPT-XL (1.5B) model. The question does not appear to be overly broad or ambiguous, making it a specific challenge that requires expertise in both Hugging Face's NLP applications and deep learning optimization techniques.

",5.0,"This question refers to a specific technical setup involving a particular optimization technique (DeepSpeed ZeRO Stage 3), a hardware component (CPU offloading), and a large language model architecture (GPT-XL). To understand the context, one needs basic knowledge of deep learning frameworks, optimization techniques, and hardware acceleration. However, the question itself is clear and self-contained; it does not depend on any specific information or context beyond what's mentioned.

"
"!---
Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Image classification examples

This directory contains 2 scripts that showcase how to fine-tune any model supported by the [`TFAutoModelForImageClassification` API](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForImageClassification) (such as [ViT](https://huggingface.co/docs/transformers/main/en/model_doc/vit), [ConvNeXT](https://huggingface.co/docs/transformers/main/en/model_doc/convnext), [ResNet](https://huggingface.co/docs/transformers/main/en/model_doc/resnet), [Swin Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/swin)...) using TensorFlow. They can be used to fine-tune models on both [datasets from the hub](#using-datasets-from-hub) as well as on [your own custom data](#using-your-own-data).

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_classification_inference_widget.png"" height=""400"" />

Try out the inference widget here: https://huggingface.co/google/vit-base-patch16-224

## TensorFlow

Based on the script [`run_image_classification.py`](https://github.com/huggingface/transformers/blob/main/examples/tensorflow/image-classification/run_image_classification.py).

### Using datasets from Hub","What is the license under which software distributed by this file is licensed?
","Apache License, Version 2.0",huggingface/transformers/blob/main/examples/tensorflow/image-classification/README.md,5.0,"The context clearly states that the software distributed by this file is licensed under the Apache License, Version 2.0. This license information is explicitly mentioned in the copyright notice and the terms of use section.

",4.0,"This question is useful because it asks about licensing, which is an important consideration when using open-source libraries like those provided by Hugging Face. The answer to this question can help developers understand their obligations and restrictions on using the software.
",5.0,"This question appears to be self-contained, as it doesn't require any additional context or information about a specific setting or situation. It simply asks for the licensing terms of software distributed by the file in question.
"
"On the Hugging Face Hub, you can find both causal language models and causal language models fine-tuned on instructions (which weâ€™ll give links to later in this blog post). Llama is one of the first open-source LLMs to have outperformed/matched closed-source ones. A research group led by Together has created a reproduction of Llama's dataset, called Red Pajama, and trained LLMs and instruction fine-tuned models on it. You can read more about it [here](https://www.together.xyz/blog/redpajama) and find [the model checkpoints on Hugging Face Hub](https://huggingface.co/models?sort=trending&search=togethercomputer%2Fredpajama). By the time this blog post is written, three of the largest causal language models with open-source licenses are [MPT-30B by MosaicML](https://huggingface.co/mosaicml/mpt-30b), [XGen by Salesforce](https://huggingface.co/Salesforce/xgen-7b-8k-base) and [Falcon by TII UAE](https://huggingface.co/tiiuae/falcon-40b), available completely open on Hugging Face Hub.
Recently, Meta released [Llama 2](https://ai.meta.com/Llama/), an open-access model with a license that allows commercial use. As of now, Llama 2 outperforms all of the other open-source large language models on different benchmarks. [Llama 2 checkpoints on Hugging Face Hub](https://huggingface.co/meta-Llama) are compatible with transformers, and the largest checkpoint is available for everyone to try at [HuggingChat](https://huggingface.co/chat/). You can read more about how to fine-tune, deploy and prompt with Llama 2 in [this blog post](https://huggingface.co/blog/llama2).","Who led the research group that created a reproduction of Llama's dataset called Red Pajama?
",Together,huggingface/blog/blob/main/os-llms.md,5.0,"The context clearly states that ""A research group led by Together has created a reproduction of Llama's dataset, called Red Pajama"", which directly answers the question. There is no ambiguity or additional information needed to answer this question.
",4.0,"This question appears to be specific and relevant to the Hugging Face ecosystem, as it mentions a well-known AI model (Llama) and a related dataset. The question also shows an interest in understanding the origins and development of this dataset, which is likely to be of interest to machine learning developers working with NLP applications.
",5.0,"This question can be understood without any additional context, as it refers to a specific event or occurrence within the history of the development of the Llama model. The mention of ""Red Pajama"" serves as an identifier for a particular reproduction of the dataset created by a research group.
"
"```python
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
import torch

base_model_path = ""stabilityai/stable-diffusion-xl-base-1.0""
controlnet_path = ""path to controlnet""

controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    base_model_path, controlnet=controlnet, torch_dtype=torch.float16
)

# speed up diffusion process with faster scheduler and memory optimization
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
# remove following line if xformers is not installed or when using Torch 2.0.
pipe.enable_xformers_memory_efficient_attention()
# memory optimization.
pipe.enable_model_cpu_offload()

control_image = load_image(""./conditioning_image_1.png"")
prompt = ""pale golden rod circle with old lace background""

# generate image
generator = torch.manual_seed(0)
image = pipe(
    prompt, num_inference_steps=20, generator=generator, image=control_image
).images[0]
image.save(""./output.png"")
```

## Notes

### Specifying a better VAE

SDXL's VAE is known to suffer from numerical instability issues. This is why we also expose a CLI argument namely `--pretrained_vae_model_name_or_path` that lets you specify the location of a better VAE (such as [this one](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)).","What is the base model path used to initialize the StableDiffusionXLControlNetPipeline?
",stabilityai/stable-diffusion-xl-base-1.0,huggingface/diffusers/blob/main/examples/controlnet/README_sdxl.md,4.0,"The context clearly states how to initialize the StableDiffusionXLControlNetPipeline, including loading the base model from a specific path. However, the question is phrased as if it's asking for the default or built-in base model path used by the pipeline, which isn't explicitly mentioned in the code snippet.

",4.0,"This question seems relevant because it specifically asks about a detail of a particular pipeline implementation, which suggests that the person asking is likely working with the Hugging Face ecosystem and wants to use the StableDiffusion XL Control Net pipeline. Providing accurate information about the base model path used in this pipeline would help them initialize their model correctly.

",4.0,"The question clearly specifies a particular model (StableDiffusionXLControlNetPipeline) and pipeline type (pipeline), which suggests that it has a specific context in mind. However, the question itself is phrased as if it's asking for general information about how to initialize such a pipeline.

"
"|      |      |[ehartford/samantha-falcon-7b](https://huggingface.co/ehartford/samantha-falcon-7b)                                                                |29          |22      |                         |                                                                                   |[LICENSE.txt](https://huggingface.co/ehartford/samantha-falcon-7b/blob/main/LICENSE.txt)                                |                                                                                                    |             |
|      |      |[coreml-projects/Llama-2-7b-chat-coreml](https://huggingface.co/coreml-projects/Llama-2-7b-chat-coreml)                                            |27          |72      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/coreml-projects/Llama-2-7b-chat-coreml/blob/main/LICENSE.txt)                      |                                                                                                    |             |
|      |      |[TheBloke/qCammel-13-GPTQ](https://huggingface.co/TheBloke/qCammel-13-GPTQ)                                                                        |27          |3       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/qCammel-13-GPTQ/blob/main/LICENSE.txt)                                    |                                                                                                    |             |
|      |      |[anonymous4chan/llama-2-7b](https://huggingface.co/anonymous4chan/llama-2-7b)                                                                      |26          |1       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/anonymous4chan/llama-2-7b/blob/main/LICENSE.txt)                                   |                                                                                                    |             |","How many models in the list contain a model named ""Llama-2 7b-chat-coreml""?
",1,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,4.0,"The context provides a list of models, and one of the models is named ""Llama-2 7b-chat-coreml"" which exactly matches the name in the question. This makes it clear that there is at least one model with this name. However, to give an exact count, one would need more information about the other models in the list.
",4.0,"This question is useful because it requires knowledge of how to query and manipulate model lists, which is an important aspect of working with the Hugging Face ecosystem. The answer will likely involve using the `models` library or other relevant APIs to search for models by name.

",5.0,"The question clearly states what it's asking about, which is a specific task involving a list of models and a particular model name. It doesn't reference any external context or require additional information beyond the question itself.

"
"card = ModelCard.from_template(card_data)
print(card.data)
```

The resulting `card.data` should look like this:

```
language: en
license: mit
model-index:
- name: my-cool-model
  results:
  - task:
      type: image-classification
    dataset:
      name: Beans
      type: beans
    metrics:
    - type: accuracy
      value: 0.7
```

If you have more than one evaluation result you'd like to share, just pass a list of `EvalResult`:

```python
card_data = ModelCardData(
    language='en',
    license='mit',
    model_name='my-cool-model',
    eval_results = [
        EvalResult(
            task_type='image-classification',
            dataset_type='beans',
            dataset_name='Beans',
            metric_type='accuracy',
            metric_value=0.7
        ),
        EvalResult(
            task_type='image-classification',
            dataset_type='beans',
            dataset_name='Beans',
            metric_type='f1',
            metric_value=0.65
        )
    ]
)
card = ModelCard.from_template(card_data)
card.data
```

Which should leave you with the following `card.data`:

```
language: en
license: mit
model-index:
- name: my-cool-model
  results:
  - task:
      type: image-classification
    dataset:
      name: Beans
      type: beans
    metrics:
    - type: accuracy
      value: 0.7
    - type: f1
      value: 0.65
```","What is the type of the metric with a value of 0.65 in the eval results?
",f1,huggingface/huggingface_hub/blob/main/docs/source/en/guides/model-cards.md,5.0,"The context provided includes a specific example of how to access the eval results, which contains information about different metrics and their values. This allows us to unambiguously determine that there is an ""f1"" metric with a value of 0.65 in the eval results.
",4.0,"This question seems to be related to evaluating the performance of an NLP model using the Hugging Face ecosystem, which likely involves using metrics such as accuracy or F1 score. The fact that it mentions an ""eval results"" suggests that the context is likely a model training pipeline where various metrics are being tracked and reported. However, without more information about the specific metric with a value of 0.65, it's hard to provide a precise answer.

",5.0,"This question is clear and unambiguous, asking for information about a specific metric within eval results. The only required context is that there are eval results being referenced.
"
"**Notes**:

* While running the above script, you need to specify the `split` accordingly. The example command above will only filter the `test` split of the dataset.
* If you append `gs://` in your `output_dir` the TFRecord shards will be directly serialized to a Google Cloud Storage (GCS) bucket. Ensure that you have already [created the GCS bucket](https://cloud.google.com/storage/docs).
* If you're using a TPU node, you must stream data from a GCS bucket. Otherwise, if you're using a TPU VM,you can store the data locally. You may need to [attach](https://cloud.google.com/tpu/docs/setup-persistent-disk) a persistent storage to the VM.
* Additional CLI arguments are also supported. We encourage you to run `python prepare_tfrecord_shards.py -h` to know more about them.

## Training the model

Once that's done, the model is ready for training. By default, training takes place on TPU, but you can use the `--no_tpu` flag to train on CPU for testing purposes. An example command is:

```bash
python3 run_mlm.py \
  --train_dataset gs://tf-tpu-training-resources/train/ \
  --eval_dataset gs://tf-tpu-training-resources/validation/ \
  --tokenizer tf-tpu/unigram-tokenizer-wikitext \
  --output_dir trained_model
```

If you had specified a `hub_model_id` while launching training, then your model will be pushed to a model repository on the Hugging Face Hub. You can find such an example repository here:
[tf-tpu/roberta-base-epochs-500-no-wd](https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd).

## Inference

Once the model is trained, you can use ğŸ¤— Pipelines to perform inference:

```python
from transformers import pipeline

model_id = ""tf-tpu/roberta-base-epochs-500-no-wd""
unmasker = pipeline(""fill-mask"", model=model_id, framework=""tf"")
unmasker(""Goal of my life is to [MASK]."")","What happens if you append `gs://` in your `output_dir`.
",The TFRecord shards will be directly serialized to a Google Cloud Storage (GCS) bucket.,huggingface/transformers/blob/main/examples/tensorflow/language-modeling-tpu/README.md,5.0,"The question can be answered clearly and unambiguously with the given context. The context specifically mentions that if you append `gs://` in your `output_dir`, the TFRecord shards will be directly serialized to a Google Cloud Storage (GCS) bucket.

",4.0,"This question seems to be related to Hugging Face's model fine-tuning and training functionality, where the output directory (`output_dir`) plays a crucial role. The question about appending `gs://` (a Google Cloud Storage prefix) suggests that it might be asking about uploading or saving models/training outputs directly to Google Cloud Storage instead of local storage. This is a specific and practical question, which could help developers understand how to use Hugging Face with cloud-based storage.
",5.0,"This question seems to be clear and context-independent, as it directly asks about a specific action (appending `gs://`) in relation to a setting (`output_dir`). The mention of `gs://` implies that the question is related to Google Cloud Storage, but this is not a requirement for understanding the question.
"
"(Automatic) Curriculum Learning for RL

While most of the RL methods seen in this course work well in practice, there are some cases where using them alone fails. This can happen, for instance, when:

- the task to learn is hard and requires an **incremental acquisition of skills** (for instance when one wants to make a bipedal agent learn to go through hard obstacles, it must first learn to stand, then walk, then maybe jumpâ€¦)
- there are variations in the environment (that affect the difficulty) and one wants its agent to be **robust** to them

<figure>
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/bipedal.gif"" alt=""Bipedal""/>
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/movable_creepers.gif"" alt=""Movable creepers""/>
<figcaption> <a href=""https://developmentalsystems.org/TeachMyAgent/"">TeachMyAgent</a> </figcaption>
</figure>

In such cases, it seems needed to propose different tasks to our RL agent and organize them such that the agent progressively acquires skills. This approach is called **Curriculum Learning** and usually implies a hand-designed curriculum (or set of tasks organized in a specific order). In practice, one can, for instance, control the generation of the environment, the initial states, or use Self-Play and control the level of opponents proposed to the RL agent.

As designing such a curriculum is not always trivial, the field of **Automatic Curriculum Learning (ACL) proposes to design approaches that learn to create such an organization of tasks in order to maximize the RL agentâ€™s performances**. Portelas et al. proposed to define ACL as:

> â€¦ a family of mechanisms that automatically adapt the distribution of training data by learning to adjust the selection of learning situations to the capabilities of RL agents.
>","What is the main goal of Automatic Curriculum Learning (ACL) for Reinforcement Learning?
",Maximizing the RL agent's performances.,huggingface/deep-rl-class/blob/main/units/en/unitbonus3/curriculum-learning.mdx,5.0,"The context provides a clear explanation of Automatic Curriculum Learning (ACL) and its purpose, which is to design approaches that learn to create an organization of tasks to maximize the performance of Reinforcement Learning agents. It also quotes a definition from Portelas et al., further clarifying the concept. This makes it easy to infer that the main goal of ACL is to adapt the distribution of training data to the capabilities of RL agents, which aligns with the question asked.

",4.0,"The question is clear and concise, asking about the primary objective of a specific technique within Reinforcement Learning, which is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem. However, without more context or information on how ACL applies specifically to NLP tasks using the Hugging Face tools, it's challenging to gauge its direct usefulness in this domain.

",5.0,"The question is clear and concise, asking about the main objective of a specific concept in the field of reinforcement learning. It does not reference any particular context or setting that would require additional information to understand.

"
"## Putting Bias in Context

The first and maybe most important concept to consider when dealing with machine bias is **context**. In their foundational work on [bias in NLP](https://aclanthology.org/2020.acl-main.485.pdf), Su Lin Blodgett et al. point out that: _â€œ[T]he majority of [academic works on machine bias] fail to engage critically with what constitutes â€œbiasâ€ in the first placeâ€_, including by building their work on top of _â€œunstated assumptions about what kinds of system behaviors are harmful, in what ways, to whom, and whyâ€_.

This may not come as much of a surprise given the ML research communityâ€™s [focus on the value of â€œgeneralizationâ€](https://dl.acm.org/doi/10.1145/3531146.3533083) â€” the most cited motivation for work in the field after â€œperformanceâ€. However, while tools for bias assessment that apply to a wide range of settings are valuable to **enable a broader analysis of common trends** in model behaviors, their ability to target the mechanisms that lead to discrimination in **concrete use cases is inherently limited**. Using them to guide specific decisions within the ML development cycle usually requires an extra step or two  to take the systemâ€™s specific use context and affected people into consideration.

<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_foresight.png"" alt=""Excerpt on considerations of ML uses context and people from the Model Card Guidebook"" />
    <em>Excerpt on considerations of ML uses context and people from the <a href=""https://huggingface.co/docs/hub/model-cards"">Model Card Guidebook</a></em>
</p>","What is cited as the most important concept to consider when dealing with machine bias?
",Context.,huggingface/blog/blob/main/ethics-soc-2.md,5.0,"The question asks about a specific concept that is considered important when dealing with machine bias, and the context provides a clear answer to this question by stating that ""context"" is the most important concept to consider. Additionally, it supports its claim with citations from academic works on bias in NLP and references to the Model Card Guidebook.

",5.0,"This question is relevant to NLP developers working within the Hugging Face ecosystem because it touches on a crucial aspect of developing responsible AI systems. Machine bias can have significant consequences in real-world applications, and understanding its causes and mitigation strategies is essential for building trustworthy models. The Hugging Face ecosystem offers various tools and resources for addressing bias in NLP applications, such as the Transformers library's support for fairness metrics and techniques like debiasing word embeddings.
",5.0,"The question does not refer to a specific context, setting, or prior information. It asks for a general piece of knowledge about machine bias, which can be understood independently.
"
"For more advanced Elasticsearch usage, you can specify your own configuration with custom settings:

```py
>>> import elasticsearch as es
>>> import elasticsearch.helpers
>>> from elasticsearch import Elasticsearch
>>> es_client = Elasticsearch([{""host"": ""localhost"", ""port"": ""9200""}])  # default client
>>> es_config = {
...     ""settings"": {
...         ""number_of_shards"": 1,
...         ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},
...     },
...     ""mappings"": {""properties"": {""text"": {""type"": ""text"", ""analyzer"": ""standard"", ""similarity"": ""BM25""}}},
... }  # default config
>>> es_index_name = ""hf_squad_context""  # name of the index in Elasticsearch
>>> squad.add_elasticsearch_index(""context"", es_client=es_client, es_config=es_config, es_index_name=es_index_name)
```","What is the default host used to connect to an Elasticsearch instance?
",localhost,huggingface/datasets/blob/main/docs/source/faiss_es.mdx,4.0,"The given context provides an example code snippet that creates an Elasticsearch client using the `Elasticsearch` class from the `elasticsearch` library. Specifically, it shows how to specify a custom host and port for connecting to an Elasticsearch instance. However, in the default case where no host is specified, the code implies that the default behavior is to use ""localhost"" as the host.

The relevant line of code is `es_client = Elasticsearch([{""host"": ""localhost"", ""port"": ""9200""}])`, which suggests that when no custom host is provided, the client will connect to ""localhost"".

",4.0,"This question appears to be relevant to machine learning developers working with the Hugging Face ecosystem, particularly those who are using the Transformers library which provides a utility for connecting to Elasticsearch. The default host used to connect to an Elasticsearch instance is likely an important piece of information for these developers.

",5.0,"The question clearly states what it's asking about (the default host to connect to an Elasticsearch instance), without any need for additional context or information.
"
"# Metric Card for WER

## Metric description
Word error rate (WER) is a common metric of the performance of an automatic speech recognition (ASR) system.

The general difficulty of measuring the performance of ASR systems lies in the fact that the recognized word sequence can have a different length from the reference word sequence (supposedly the correct one). The WER is derived from the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), working at the word level.

This problem is solved by first aligning the recognized word sequence with the reference (spoken) word sequence using dynamic string alignment. Examination of this issue is seen through a theory called the power law that states the correlation between [perplexity](https://huggingface.co/metrics/perplexity) and word error rate (see [this article](https://www.cs.cmu.edu/~roni/papers/eval-metrics-bntuw-9802.pdf) for further information).

Word error rate can then be computed as:

`WER = (S + D + I) / N = (S + D + I) / (S + D + C)`

where

`S` is the number of substitutions,

`D` is the number of deletions,

`I` is the number of insertions,

`C` is the number of correct words,

`N` is the number of words in the reference (`N=S+D+C`).


## How to use

The metric takes two inputs: references (a list of references for each speech input) and predictions (a list of transcriptions to score).


```python
from evaluate import load
wer = load(""wer"")
wer_score = wer.compute(predictions=predictions, references=references)
```
## Output values

This metric outputs a float representing the word error rate.

```
print(wer_score)
0.5
```

This value indicates the average number of errors per reference word.

The **lower** the value, the **better** the performance of the ASR system, with a WER of 0 being a perfect score.

### Values from popular papers","What is the formula for computing the Word Error Rate (WER)?
",WER = (S + D + I) / N = (S + D + I) / (S + D + C),huggingface/evaluate/blob/main/metrics/wer/README.md,5.0,"The context provides a clear and concise description of the Word Error Rate (WER) metric, including its formula, calculation method, and application. It also includes a specific section on how to use the metric in code, which makes it easy to understand and implement. Moreover, the output values are clearly explained, providing additional clarity on what the WER score represents.

",5.0,"This question is very useful because it targets a specific concept within NLP, which is an important aspect of building models with the Hugging Face ecosystem. Understanding how to calculate the Word Error Rate is crucial in evaluating the performance of automatic speech recognition systems and machine translation models. The answer to this question can provide insights into the calculation process, which can help developers better comprehend their model's results.

",5.0,"The question refers to a well-defined concept within natural language processing, specifically speech recognition. It asks for the mathematical formula used to calculate WER, which implies that the context of the question is already familiar with the concept and its relevance in the field.
"
"```python
import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

@dataclass
class DataCollatorCTCWithPadding:
    """"""
    Data collator that will dynamically pad the inputs received.
    Args:
        processor (:class:`~transformers.Wav2Vec2Processor`)
            The processor used for proccessing the data.
        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:
            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
              sequence if provided).
            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
              maximum acceptable input length for the model if that argument is not provided.
            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
              different lengths).
    """"""

    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lenghts and need
        # different padding methods
        input_features = [{""input_values"": feature[""input_values""]} for feature in features]
        label_features = [{""input_ids"": feature[""labels""]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            return_tensors=""pt"",
        )

        labels_batch = self.processor.pad(
            labels=label_features,
            padding=self.padding,
            return_tensors=""pt"",
        )","What is the default value of the 'padding' argument in the DataCollatorCTCWithPadding class?
",True,huggingface/blog/blob/main/mms_adapters.md,4.0,"The context explicitly states that the 'padding' argument is set to True by default in the DataCollatorCTCWithPadding class. However, it does not specify what this means exactly. It mentions different strategies for padding sequences (e.g., ""longest"", ""max_length"", or no padding), but we can infer from the code snippet that if 'padding' is True, it will pad to the longest sequence in the batch. Therefore, while there might be some ambiguity about what exactly happens when 'padding' is True, the answer is still somewhat clear.

",5.0,"This question appears to be straightforward and relevant to machine learning developers working with the Hugging Face ecosystem, particularly those using the DataCollatorCTCWithPadding class. The answer is likely a specific value or a reference to the documentation where this value can be found.

",5.0,"This question is clear and concise, and it refers to a specific class within a larger library. The mention of ""DataCollatorCTCWithPadding"" provides enough context for an operator with access to documentation to understand what the question is about.
"
"Ğ’ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ NLP ( ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ ):
- [ĞœĞ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)
- [Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)
- [Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)
- [Ğ’Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)
- [ĞĞ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)","What is the name of the NLP task that can be performed using Electra?
",Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹.,huggingface/transformers/blob/main/README_ru.md,4.0,"The context provides a list of NLP tasks and their corresponding models, but it does not explicitly mention the name of the task that can be performed using Electra. However, based on the information provided, we can infer that the task is entity recognition, as Electra is mentioned alongside ""Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹"" (Entity Recognition).
",4.0,"This question seems to be quite specific, but unfortunately, it's not clear if the person asking knows what Electra is. However, assuming they are familiar with it, the answer to this question would provide useful information about the capabilities of Electra in NLP tasks.

",5.0,"The question refers to a specific type of deep learning model (Electra) but does not provide any context about which task it is being used for. However, in the field of NLP, Electra is known to perform masked language modeling and other tasks similar to BERT. Therefore, an operator familiar with NLP models would know that Electra can be used for a range of NLP tasks.
"
"1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.
1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier HÃ©naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, JoÃ£o Carreira.
1. **[Persimmon](https://huggingface.co/docs/transformers/model_doc/persimmon)** (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b) by Erich Elsen, Augustus Odena, Maxwell Nye, SaÄŸnak TaÅŸÄ±rlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.
1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.
1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.","Who are the authors of the paper ""PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization""?
","Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.",huggingface/transformers/blob/main/README_te.md,5.0,"The context provides a list of papers and their corresponding authors. The question asks for the authors of a specific paper, ""PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"". This paper is mentioned in the first point of the context, which clearly states that it was written by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.

",5.0,"This question is extremely useful as it directly relates to the Hugging Face ecosystem, which provides a large collection of pre-trained language models and related resources. Identifying the authors of a specific paper that contributes to the development of such models can be crucial for understanding the context and evolution of NLP technologies within the Hugging Face framework.

",5.0,"The question can be understood as a standalone query without any additional context, as it includes the specific title of the paper and clearly states what is being asked (the authors' names).
"
"<div class=""flex justify-center"">
Â  Â  <img src=""https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/ip-bear.png"" />
</div>

<Tip>

You can use the [`~loaders.IPAdapterMixin.set_ip_adapter_scale`] method to adjust the text prompt and image prompt condition ratio. Â If you're only using the image prompt, you should set the scale to `1.0`. You can lower the scale to get more generation diversity, but it'll be less aligned with the prompt.
`scale=0.5` can achieve good results in most cases when you use both text and image prompts.
</Tip>

IP-Adapter also works great with Image-to-Image and Inpainting pipelines. See below examples of how you can use it with Image-to-Image and Inpaint.

<hfoptions id=""tasks"">
<hfoption id=""image-to-image"">

```py
from diffusers import AutoPipelineForImage2Image
import torch
from diffusers.utils import load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(""runwayml/stable-diffusion-v1-5"", torch_dtype=torch.float16).to(""cuda"")

image = load_image(""https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/vermeer.jpg"")
ip_image = load_image(""https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/river.png"")

pipeline.load_ip_adapter(""h94/IP-Adapter"", subfolder=""models"", weight_name=""ip-adapter_sd15.bin"")
generator = torch.Generator(device=""cpu"").manual_seed(33)
images = pipeline(
Â  Â  prompt='best quality, high quality',
Â  Â  image = image,
Â  Â  ip_adapter_image=ip_image,
Â  Â  num_inference_steps=50,
Â  Â  generator=generator,
Â  Â  strength=0.6,
).images
images[0]
```

</hfoption>
<hfoption id=""inpaint"">

```py
from diffusers import AutoPipelineForInpaint
import torch
from diffusers.utils import load_image

pipeline = AutoPipelineForInpaint.from_pretrained(""runwayml/stable-diffusion-v1-5"", torch_dtype=torch.float).to(""cuda"")","What is the default scale to use when only using the image prompt with IP-Adapter?
",1.0,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md,5.0,"The context clearly states that when only using the image prompt with IP-Adapter, you should set the scale to `1.0`. This is explicitly mentioned in the tip section of the context.
",4.0,"The question appears to be specific to the usage of IP-Adapter within the Hugging Face ecosystem, particularly regarding image prompts. This suggests that the user has a clear understanding of the component they are working with and is looking for information on how to configure it optimally. The question about default scales for image prompts implies an interest in fine-tuning models or adapting them to specific tasks without having to manually specify every parameter.

",4.0,"The question assumes prior knowledge of what an ""IP-Adapter"" is, but it's a specific tool that can be looked up. However, the crucial part is understanding that the question refers to a default setting within this tool, which makes it somewhat dependent on additional information.

"
"<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->

[ALBERT](../model_doc/albert), [BART](../model_doc/bart), [BERT](../model_doc/bert), [BigBird](../model_doc/big_bird), [CamemBERT](../model_doc/camembert), [ConvBERT](../model_doc/convbert), [Data2VecText](../model_doc/data2vec-text), [DeBERTa](../model_doc/deberta), [DeBERTa-v2](../model_doc/deberta-v2), [DistilBERT](../model_doc/distilbert), [ELECTRA](../model_doc/electra), [ERNIE](../model_doc/ernie), [ESM](../model_doc/esm), [FlauBERT](../model_doc/flaubert), [FNet](../model_doc/fnet), [Funnel Transformer](../model_doc/funnel), [I-BERT](../model_doc/ibert), [LayoutLM](../model_doc/layoutlm), [Longformer](../model_doc/longformer), [LUKE](../model_doc/luke), [mBART](../model_doc/mbart), [MEGA](../model_doc/mega), [Megatron-BERT](../model_doc/megatron-bert), [MobileBERT](../model_doc/mobilebert), [MPNet](../model_doc/mpnet), [MRA](../model_doc/mra), [MVP](../model_doc/mvp), [Nezha](../model_doc/nezha), [NystrÃ¶mformer](../model_doc/nystromformer), [Perceiver](../model_doc/perceiver), [QDQBert](../model_doc/qdqbert), [Reformer](../model_doc/reformer), [RemBERT](../model_doc/rembert), [RoBERTa](../model_doc/roberta), [RoBERTa-PreLayerNorm](../model_doc/roberta-prelayernorm), [RoCBert](../model_doc/roc_bert), [RoFormer](../model_doc/roformer), [SqueezeBERT](../model_doc/squeezebert), [TAPAS](../model_doc/tapas), [Wav2Vec2](../model_doc/wav2vec2), [XLM](../model_doc/xlm), [XLM-RoBERTa](../model_doc/xlm-roberta), [XLM-RoBERTa-XL](../model_doc/xlm-roberta-xl), [X-MOD](../model_doc/xmod), [YOSO](../model_doc/yoso)

<!--End of the generated tip-->

</Tip>

Before you begin, make sure you have all the necessary libraries installed:

```bash
pip install transformers datasets evaluate
```

We encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:","What is the name of the framework used to access pre-trained models from Hugging Face?
",Transformers,huggingface/transformers/blob/main/docs/source/en/tasks/masked_language_modeling.md,4.0,"The context provided lists various pre-trained models from Hugging Face, but it does not explicitly mention a framework for accessing these models. However, it is implied that the library `transformers` (mentioned in the setup instructions) is used to access and utilize these models. This library is commonly referred to as Transformers Hub or simply HF, which can be accessed using the Hugging Face token.

",5.0,"This question is useful because it is a foundational query that gets at the heart of how developers interact with the Hugging Face ecosystem. The answer to this question will likely be a key component in many other NLP-related questions and discussions, making it a good starting point for exploration.
",4.0,"The question clearly refers to a specific tool or library provided by Hugging Face, which implies that it is not entirely context-independent. However, the mention of ""pre-trained models"" and ""Hugging Face"" suggests that an operator with general knowledge about this topic should be able to understand what is being asked without needing additional information.
"
"### 6. What is the first step when using the `push_to_hub()` method or the CLI tools?

<Question
	choices={[
		{
			text: ""Log in on the website."",
			explain: ""This won't help you on your local machine.""
		},
		{
			text: ""Run 'huggingface-cli login' in a terminal."",
			explain: ""Correct â€” this will download and cache your personal token."",
            correct: true
		},
		{
			text: ""Run 'notebook_login()' in a notebook."",
			explain: ""Correct â€” this will display a widget to let you authenticate."",
            correct: true
		},
	]}
/>

### 7. You're using a model and a tokenizer â€” how can you upload them to the Hub?

<Question
	choices={[
		{
			text: ""By calling the push_to_hub method directly on the model and the tokenizer."",
			explain: ""Correct!"",
            correct: true
		},
		{
			text: ""Within the Python runtime, by wrapping them in a <code>huggingface_hub</code> utility."",
			explain: ""Models and tokenizers already benefit from <code>huggingface_hub</code> utilities: no need for additional wrapping!""
		},
		{
			text: ""By saving them to disk and calling <code>transformers-cli upload-model</code>"",
			explain: ""The command <code>upload-model</code> does not exist.""
		}
	]}
/>

### 8. Which git operations can you do with the `Repository` class?

<Question
	choices={[
		{
			text: ""A commit."",
			explain: ""Correct, the <code>git_commit()</code> method is there for that."",
            correct: true
		},
		{
			text: ""A pull"",
			explain: ""That is the purpose of the <code>git_pull()</code> method."",
            correct: true
		},
		{
			text: ""A push"",
			explain: ""The method <code>git_push()</code> does this."",
            correct: true
		},
		{
			text: ""A merge"",
			explain: ""No, that operation will never be possible with this API.""
		}
	]}
/>","How do you log in when using the `push_to_hub()` method or the CLI tools?
",Run 'huggingface-cli login' in a terminal.,huggingface/course/blob/main/chapters/en/chapter4/6.mdx,4.0,"The question is asking how to log in when using the `push_to_hub()` method or CLI tools. However, the context only mentions logging in as part of a larger discussion about using the `push_to_hub()` method or CLI tools, but it doesn't explicitly provide instructions on how to log in.

",4.0,"This question seems to be a practical concern for developers using the Hugging Face ecosystem, as logging in to push models to the Hub or use other features of the CLI tools is likely to be a common requirement. The answer could provide crucial information on how to authenticate properly.
",5.0,"The question does not mention any specific context that would require additional information to understand. It refers to a technical concept, but it's clear what is being asked without needing more details.

"
"![Words associated with top 8 topics](https://huggingface.co/datasets/huggingface/documentation-images/resolve/2d1113254a370972470d42e122df150f3551cc07/blog/BERTopic/topic_word_scores.png)

[databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) is another dataset that can be used to train an RLHF model. The approach taken to creating this dataset was quite different from the OpenAssistant Conversations dataset since it was created by employees of Databricks instead of being crowd sourced via volunteers. Perhaps we can use our trained BERTopic model to compare the topics across these two datasets?

The new BERTopic Hub integrations mean we can load this trained model and apply it to new examples.

```python
topic_model = BERTopic.load(""davanstrien/chat_topics"")
```

We can predict on a single example text:

```python
example = ""Stalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position""
topic, prob = topic_model.transform(example)
```

We can get more information about the predicted topic

```python
topic_model.get_topic_info(topic)
```

|    |   Count | Name                                  | Representation                                                                                      |
|---:|--------:|:--------------------------------------|:----------------------------------------------------------------------------------------------------|
|  0 |     240 | 22_chess_chessboard_practice_strategy | ['chess', 'chessboard', 'practice', 'strategy', 'learn', 'pawn', 'board', 'pawns', 'play', 'decks'] |

We can see here the topics predicted seem to make sense. We may want to extend this to compare the topics predicted for the whole dataset.

```python
from datasets import load_dataset

dataset = load_dataset(""databricks/databricks-dolly-15k"")
dolly_docs = dataset['train']['response']
dolly_topics, dolly_probs = topic_model.transform(dolly_docs)
```","Who created the Databricks-Dolly-15k dataset?
",Employees of Databricks.,huggingface/blog/blob/main/bertopic.md,4.0,"The context provided is about using the BERTopic model to analyze and compare topics in two datasets, one of which is the Databricks-Dolly-15k dataset. However, there is no explicit mention of who created the Databricks-Dolly-15k dataset. Nevertheless, it can be inferred that the dataset was created by employees of Databricks as mentioned in the context.

",4.0,"This question seems to be related to NLP data, which is a crucial aspect of building NLP applications. However, it's quite specific and doesn't directly relate to Hugging Face or their ecosystem, but rather to a particular dataset used in the field. The ability to find information about datasets can still be useful for developers working with NLP.

",4.0,"The question does not require additional information or context to understand what is being asked, as it clearly refers to a specific dataset and asks for its creator. However, it may require an external knowledge source (like the documentation of the Databricks-Dolly-15k dataset) to answer accurately.

"
"No matter which strategy you choose, the recommended procedure is often the same that you should start to debug the
starting layers first and the ending layers last.

It is recommended that you retrieve the output, either by print statements or sub-component functions, of the following
layers in the following order:

1. Retrieve the input IDs passed to the model
2. Retrieve the word embeddings
3. Retrieve the input of the first Transformer layer
4. Retrieve the output of the first Transformer layer
5. Retrieve the output of the following n - 1 Transformer layers
6. Retrieve the output of the whole BrandNewBert Model

Input IDs should thereby consists of an array of integers, *e.g.* `input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`

The outputs of the following layers often consist of multi-dimensional float arrays and can look like this:

```
[[
 [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],
 [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],
 [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],
 ...,
 [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],
 [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],
 [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],
```","What are the recommended steps to debug the starting layers of a model?
",Start by retrieving the input IDs passed to the model and then retrieve the word embeddings.,huggingface/transformers/blob/main/docs/source/en/add_new_model.md,4.0,"The context provides a clear and step-by-step procedure for debugging the starting layers of a model, including retrieving input IDs, word embeddings, and transformer layer outputs. However, it does not explicitly provide recommended steps to debug the starting layers of a model beyond this initial retrieval process.

",5.0,"This question is extremely useful because debugging the starting layers of a model can be particularly challenging, especially when working with complex architectures like those used in NLP applications. Providing steps for debugging these layers would help developers identify and fix issues early on, potentially saving time and effort. The Hugging Face ecosystem's emphasis on transformer-based models makes this question even more relevant.

",5.0,"This question can be understood without any additional context. The term ""starting layers"" refers to the initial components of a neural network or machine learning model, and debugging these areas typically involves identifying and addressing issues related to data processing, feature extraction, or other fundamental operations. The question implicitly assumes that the reader is familiar with general debugging practices in the field of machine learning.
"
"Create an Endpoint

After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification.

## 1. Enter the Hugging Face Repository ID and your desired endpoint name:

<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png"" alt=""select repository"" />

## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.

<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png"" alt=""select region"" />

## 3. Define the [Security Level](security) for the Endpoint:

<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png"" alt=""define security"" />

## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.

<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png"" alt=""create endpoint"" />

## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.

<img src=""https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png"" alt=""overview"" />

## 6. Test your Endpoint in the overview with the Inference widget ğŸ ğŸ‰!","Which cloud providers are currently available for endpoints?
",AWS,huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx,4.0,"The context provides information on creating an endpoint, but it does not explicitly mention the question of which cloud providers are currently available for endpoints. However, it does provide a specific example of AWS as a cloud provider, and mentions that Azure will be added soon.

",4.0,"This question is useful because it shows that the user has a specific implementation-related concern, which requires knowledge of the Hugging Face ecosystem's capabilities. It implies that the user is interested in deploying their models and wants to know about the available infrastructure options.
",5.0,"The question can be understood without any additional information, it's clear that the context is about a specific topic (endpoints) and asks about a specific aspect of it (cloud providers). There's no implicit mention of a particular setting or document.

"
"å½“è¿™äº›åç«¯ä¹‹ä¸€å®‰è£…æˆåŠŸåï¼Œ ğŸ¤— Transformers å¯ä¾æ­¤å®‰è£…ï¼š

```bash
pip install transformers
```

å¦‚æœä½ æƒ³è¦è¯•è¯•ç”¨ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼å‘å¸ƒå‰ä½¿ç”¨æœ€æ–°çš„å¼€å‘ä¸­ä»£ç ï¼Œä½ å¾—[ä»æºä»£ç å®‰è£…](https://huggingface.co/docs/transformers/installation#installing-from-source)ã€‚

### ä½¿ç”¨ conda

è‡ª Transformers 4.0.0 ç‰ˆå§‹ï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ª conda é¢‘é“ï¼š `huggingface`ã€‚

ğŸ¤— Transformers å¯ä»¥é€šè¿‡ conda ä¾æ­¤å®‰è£…ï¼š

```shell script
conda install -c huggingface transformers
```

è¦é€šè¿‡ conda å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè¯·å‚é˜…å®ƒä»¬å„è‡ªå®‰è£…é¡µçš„è¯´æ˜ã€‚

## æ¨¡å‹æ¶æ„

ğŸ¤— Transformers æ”¯æŒçš„[**æ‰€æœ‰çš„æ¨¡å‹æ£€æŸ¥ç‚¹**](https://huggingface.co/models)ç”±[ç”¨æˆ·](https://huggingface.co/users)å’Œ[ç»„ç»‡](https://huggingface.co/organizations)ä¸Šä¼ ï¼Œå‡ä¸ huggingface.co [model hub](https://huggingface.co) æ— ç¼æ•´åˆã€‚

ç›®å‰çš„æ£€æŸ¥ç‚¹æ•°é‡ï¼š ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)

ğŸ¤— Transformers ç›®å‰æ”¯æŒå¦‚ä¸‹çš„æ¶æ„ï¼ˆæ¨¡å‹æ¦‚è¿°è¯·é˜…[è¿™é‡Œ](https://huggingface.co/docs/transformers/model_summary)ï¼‰ï¼š","ä»€ä¹ˆæ˜¯ä»Transformers 4.0.0ç‰ˆæœ¬å¼€å§‹çš„æ–°åŠŸèƒ½ï¼Ÿ
",æœ‰ä¸€ä¸ªcondaé¢‘é“ï¼š `huggingface`ã€‚,huggingface/transformers/blob/main/README_zh-hans.md,5.0,"æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒTransformers 4.0.0ç‰ˆæœ¬å¼€å§‹æ”¯æŒcondaå®‰è£…ï¼Œè¿™æ„å‘³ç€å¯ä»¥é€šè¿‡condaé¢‘é“å®‰è£…Transformersï¼Œå¹¶ä¸”æä¾›äº†ä¸€ä¸ªcondaå‘½ä»¤æ¥è¿›è¡Œå®‰è£…ã€‚
",4.0,"è¿™ä¸ªé—®é¢˜ä¼¼ä¹æ˜¯å…³äºTransformersåº“çš„æ›´æ–°æ—¥å¿—ï¼Œç‰¹åˆ«æ˜¯ä»4.0.0ç‰ˆæœ¬å¼€å§‹çš„æ–°åŠŸèƒ½ã€‚è¿™ç§ç±»å‹çš„é—®é¢˜å¯ä»¥å¸®åŠ©æœºå™¨å­¦ä¹ å¼€å‘è€…äº†è§£æœ€æ–°çš„åº“æ›´æ–°å’Œç‰¹æ€§ï¼Œè¿™å¯¹ä»–ä»¬åœ¨NLPåº”ç”¨ä¸­ä½¿ç”¨Hugging Faceç”Ÿæ€ç³»ç»Ÿæœ‰å¾ˆå¤§çš„å‚è€ƒä»·å€¼ã€‚

",5.0,"The question asks about a specific version of Transformers (a popular library for natural language processing) and what new features were introduced in that version. This suggests that the question relies on prior knowledge of the library's history and development.

"
"#### Batch Size

It is worth noting that the largest speed gains with speculative decoding come with a batch size of 1. For batched
speculative decoding, all candidate tokens **across the batch** must match the validation tokens in order for the tokens
to be accepted. If a token in the batch at a given position does not agree, all candidate tokens that proceed the position
are discarded. Consequently, speculative decoding favours lower batch sizes. In practice, we find that speculative decoding
provides a speed-up until a batch size of 4. Above batch size 4, speculative decoding returns slower inference than the
main model alone. For full results, refer to Section D.3 of the [Distil-Whisper paper](https://arxiv.org/pdf/2311.00430.pdf).

## Conclusion

In this blog post, we covered the inference strategy of speculative decoding, as applied to the Whisper model for speech
transcription. We demonstrated how 2x speed-ups can be achieved, while mathematically ensuring the same outputs as using
the original model alone. We encourage you to try speculative decoding as a drop-in replacement for existing Whisper
pipelines, given the low overhead of using the additional assistant model and the guarantee of the same transcription results.

## Acknowledgements

Blog post by [Sanchit Gandhi](https://huggingface.co/sanchit-gandhi). Many thanks to [Patrick von Platen](https://huggingface.co/patrickvonplaten)
and [Pedro Cuenca](https://huggingface.co/pcuenq) for their constructive comments, and to [Joao Gante](https://huggingface.co/joaogante)
for the assisted generation implementation in ğŸ¤— Transformers.","What is the largest batch size for which speculative decoding provides a speed-up?
",4,huggingface/blog/blob/main/whisper-speculative-decoding.md,5.0,"The context provides explicit information about the speed-up provided by speculative decoding at different batch sizes. It states that speculative decoding favours lower batch sizes and provides a speed-up until a batch size of 4, above which it returns slower inference than the main model alone. This directly answers the question about the largest batch size for which speculative decoding provides a speed-up.

",4.0,"This question appears to be specific to the Hugging Face Transformers library, which uses the concept of ""speculative decoding"" to accelerate inference. The question is likely asking about a specific optimization technique used in the library's pipeline. A good answer would provide information on the maximum batch size that can take advantage of speculative decoding, as well as any potential trade-offs or limitations.

",5.0,"This question assumes knowledge of a specific optimization technique called ""speculative decoding"" but does not rely on any particular context or document, making it self-contained. The term ""batch size"" is also well-defined in the field of deep learning.

"
"A GNN is made of successive layers. A GNN layer represents a node as the combination (**aggregation**) of the representations of its neighbours and itself from the previous layer (**message passing**), plus usually an activation to add some nonlinearity.

**Comparison to other models**: A CNN can be seen as a GNN with fixed neighbour sizes (through the sliding window) and ordering (it is not permutation equivariant). A [Transformer](https://arxiv.org/abs/1706.03762v3) without positional embeddings can be seen as a GNN on a fully-connected input graph.

### Aggregation and message passing

There are many ways to aggregate messages from neighbour nodes, summing, averaging, for example. Some notable works following this idea include:

- [Graph Convolutional Networks](https://tkipf.github.io/graph-convolutional-networks/) averages the normalised representation of the neighbours for a node (most GNNs are actually GCNs);
- [Graph Attention Networks](https://petar-v.com/GAT/) learn to weigh the different neighbours based on their importance (like transformers);
- [GraphSAGE](https://snap.stanford.edu/graphsage/) samples neighbours at different hops before aggregating their information in several steps with max pooling.
- [Graph Isomorphism Networks](https://arxiv.org/pdf/1810.00826v3.pdf) aggregates representation by applying an MLP to the sum of the neighbours' node representations.

**Choosing an aggregation**: Some aggregation techniques (notably mean/max pooling) can encounter failure cases when creating representations which finely differentiate nodes with different neighbourhoods of similar nodes (ex: through mean pooling, a neighbourhood with 4 nodes, represented as 1,1,-1,-1, averaged as 0, is not going to be different from one with only 3 nodes represented as -1, 0, 1).

### GNN shape and the over-smoothing problem

At each new layer, the node representation includes more and more nodes.","What is an example of a work that follows the idea of aggregating messages from neighbour nodes in Graph Neural Networks?
",[Graph Convolutional Networks](https://tkipf.github.io/graph-convolutional-networks/) averages the normalised representation of the neighbours for a node.,huggingface/blog/blob/main/intro-graphml.md,4.0,"The context clearly explains the concept of message passing and aggregation in Graph Neural Networks (GNNs) and provides examples of works that follow this idea. However, it does not explicitly mention a specific work that demonstrates aggregating messages from neighbour nodes as an example to answer the question directly. Despite this, the provided comparison with other models and the list of notable works following this idea, including Graph Convolutional Networks, Graph Attention Networks, GraphSAGE, and Graph Isomorphism Networks, provide enough information for a reader familiar with GNNs to infer that one of these examples could be considered an instance where messages from neighbour nodes are aggregated.

",5.0,"This question seems to be asking about a specific concept or technique related to Graph Neural Networks (GNNs), which are part of the NLP domain and more specifically used for tasks like node classification, link prediction, etc. The idea of aggregating messages from neighbor nodes is a fundamental operation in many GNN architectures, such as Graph Attention Network (GAT) or Graph Convolutional Network (GCN). A good example of such work could be the GraphSAGE paper, which presents an efficient way to aggregate features from neighboring nodes.

",5.0,"This question is clear and can be understood without any additional context. It references a specific concept (Graph Neural Networks) and asks for a concrete example related to it, which implies that there are multiple possible examples.

"
"### Bug Fixes:

- Fixes `cancels` for generators so that if a generator is canceled before it is complete, subsequent runs of the event do not continue from the previous iteration, but rather start from the beginning. By [@abidlabs](https://github.com/abidlabs) in [PR 4969](https://github.com/gradio-app/gradio/pull/4969).
- Use `gr.State` in `gr.ChatInterface` to reduce latency by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4976](https://github.com/gradio-app/gradio/pull/4976)
- Fix bug with `gr.Interface` where component labels inferred from handler parameters were including special args like `gr.Request` or `gr.EventData`. By [@cbensimon](https://github.com/cbensimon) in [PR 4956](https://github.com/gradio-app/gradio/pull/4956)

### Breaking Changes:

No changes to highlight.

### Other Changes:

- Apply pyright to the `components` directory by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4948](https://github.com/gradio-app/gradio/pull/4948)
- Improved look of ChatInterface by [@aliabid94](https://github.com/aliabid94) in [PR 4978](https://github.com/gradio-app/gradio/pull/4978)

## 3.37

### New Features:

Introducing a new `gr.ChatInterface` abstraction, which allows Gradio users to build fully functioning Chat interfaces very easily. The only required parameter is a chat function `fn`, which accepts a (string) user input `message` and a (list of lists) chat `history` and returns a (string) response. Here's a toy example:

```py
import gradio as gr

def echo(message, history):
    return message

demo = gr.ChatInterface(fn=echo, examples=[""hello"", ""hola"", ""merhaba""], title=""Echo Bot"")
demo.launch()
```

Which produces:

<img width=""1291"" alt=""image"" src=""https://github.com/gradio-app/gradio/assets/1778297/ae94fd72-c2bb-406e-9e8d-7b9c12e80119"">

And a corresponding easy-to-use API at `/chat`:

<img width=""1164"" alt=""image"" src=""https://github.com/gradio-app/gradio/assets/1778297/7b10d6db-6476-4e2e-bebd-ecda802c3b8f"">","Who fixed the bug with `gr.Interface` where component labels inferred from handler parameters were including special args like `gr.Request` or `gr.EventData`.
",cbensimon,gradio-app/gradio/blob/main/CHANGELOG.md,5.0,"The question is directly answered in the context with a specific bug fix attributed to a particular contributor. This makes it easy for anyone familiar with the context to find the answer.
",5.0,"This question appears to be very specific and targeted towards a particular issue in the Hugging Face ecosystem, specifically related to the `gr.Interface` class. It requires knowledge of the internal workings of this class and its handling of component labels inferred from handler parameters. A developer who is familiar with the Hugging Face library and has encountered similar issues would find this question extremely useful as it directly addresses a potential problem they are facing.

",5.0,"The question clearly refers to a specific issue with the Gradio library, but it does not rely on any external context beyond what is provided by the library's documentation. The mention of `gr.Interface`, `gr.Request`, and `gr.EventData` acronyms does not require additional information to understand that they are related to Gradio.
"
"```bash
accelerate config
```

Test your setup to make sure it is configured correctly:

```bash
accelerate test
```

Now you are ready to launch the training:

```bash
accelerate launch run_summarization_no_trainer.py \
    --model_name_or_path t5-small \
    --dataset_name cnn_dailymail \
    --dataset_config ""3.0.0"" \
    --source_prefix ""summarize: "" \
    --output_dir ~/tmp/tst-summarization
```

## Use a custom dataset

The summarization script supports custom datasets as long as they are a CSV or JSON Line file. When you use your own dataset, you need to specify several additional arguments:

- `train_file` and `validation_file` specify the path to your training and validation files.
- `text_column` is the input text to summarize.
- `summary_column` is the target text to output.

A summarization script using a custom dataset would look like this:

```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --train_file path_to_csv_or_jsonlines_file \
    --validation_file path_to_csv_or_jsonlines_file \
    --text_column text_column_name \
    --summary_column summary_column_name \
    --source_prefix ""summarize: "" \
    --output_dir /tmp/tst-summarization \
    --overwrite_output_dir \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --predict_with_generate
```

## Test a script

It is often a good idea to run your script on a smaller number of dataset examples to ensure everything works as expected before committing to an entire dataset which may take hours to complete. Use the following arguments to truncate the dataset to a maximum number of samples:

- `max_train_samples`
- `max_eval_samples`
- `max_predict_samples`","What is the name of the command used to test the setup in accelerate?
",accelerate test,huggingface/transformers/blob/main/docs/source/en/run_scripts.md,5.0,"The context provided is about using the accelerate library for training and testing models, specifically mentioning commands like ""config"", ""test"", and ""launch"". However, it does not explicitly mention a command for testing the setup. It implies that ""accelerate test"" might be used to test the setup, but this is not directly stated.

The command to test the setup is clearly implied by the context provided in the 'Test your setup' section, where it mentions to use the 'test' command after configuring the accelerate environment using 'config'. However, a direct mention of the 'test' command is made without any ambiguity. The clarity and directness of this statement warrant a high rating for answerability.

",4.0,"The question is straightforward and seeks a specific piece of information regarding the Accelerate library, which is part of the Hugging Face ecosystem. This type of question could be useful for developers who are new to or exploring the use of Accelerate for their NLP tasks. However, it doesn't require complex reasoning or problem-solving skills but rather knowledge about the library's interface.

",5.0,"The question clearly specifies what is being asked (the name of a specific command related to testing a setup in Accelerate) and does not rely on any additional context or information beyond general knowledge of the topic.

"
"!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ğŸ¤— Optimum

ğŸ¤— Optimum is an extension of [Transformers](https://huggingface.co/docs/transformers) that provides a set of performance optimization tools to train and run models on targeted hardware with maximum efficiency.

The AI ecosystem evolves quickly, and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables developers to efficiently use any of these platforms with the same ease inherent to Transformers.

ğŸ¤— Optimum is distributed as a collection of packages - check out the links below for an in-depth look at each one.","What license governs permissions and limitations under ğŸ¤— Optimum?
","Apache License, Version 2.0.",huggingface/optimum/blob/main/docs/source/index.mdx,5.0,"The context explicitly states that ""Licensed under the Apache License, Version 2.0"" and provides a link to the license. This makes it clear what license governs permissions and limitations under ğŸ¤— Optimum.
",4.0,"The question appears to be asking about the licensing terms of Hugging Face's Optimum, which suggests that the user wants to know what they can or cannot do with the tool. This is a reasonable question for machine learning developers who are considering using Optimum in their NLP applications.

",5.0,"The question refers to a specific entity (ğŸ¤— Optimum) without specifying any additional context, but it's clear that the question is asking about the licensing terms associated with this entity. Since the name of the entity is well-known and easily identifiable as related to Hugging Face's model hub, an operator familiar with the topic should be able to understand what is being asked.

"
"Most of the T2I-Adapter models we mention in this blog post were trained on 3M high-resolution image-text pairs from LAION-Aesthetics V2 with the following settings:

- Training steps: 20000-35000
- Batch size: Data parallel with a single GPU batch size of 16 for a total batch size of 128.
- Learning rate: Constant learning rate of 1e-5.
- Mixed precision: fp16

We encourage the community to use our scripts to train custom and powerful T2I-Adapters, striking a competitive trade-off between speed, memory, and quality.

## Using T2I-Adapter-SDXL in `diffusers`

Here, we take the lineart condition as an example to demonstrate the usage of [T2I-Adapter-SDXL](https://github.com/TencentARC/T2I-Adapter/tree/XL). To get started, first install the required dependencies:

```bash
pip install -U git+https://github.com/huggingface/diffusers.git
pip install -U controlnet_aux==0.0.7 # for conditioning models and detectors
pip install transformers accelerate
```

The generation process of the T2I-Adapter-SDXL mainly consists of the following two steps:

1. Condition images are first prepared into the appropriate *control image* format.
2. The *control image* and *prompt* are passed to the [`StableDiffusionXLAdapterPipeline`](https://github.com/huggingface/diffusers/blob/0ec7a02b6a609a31b442cdf18962d7238c5be25d/src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py#L126).

Let's have a look at a simple example using the [Lineart Adapter](https://huggingface.co/TencentARC/t2i-adapter-lineart-sdxl-1.0). We start by initializing the T2I-Adapter pipeline for SDXL and the lineart detector.

```python
import torch
from controlnet_aux.lineart import LineartDetector
from diffusers import (AutoencoderKL, EulerAncestralDiscreteScheduler,
                       StableDiffusionXLAdapterPipeline, T2IAdapter)
from diffusers.utils import load_image, make_image_grid","What is the batch size of a single GPU when training T2I-Adapter models?
",16,huggingface/blog/blob/main/t2i-sdxl-adapters.md,5.0,"The context explicitly states the batch size for a single GPU when training T2I-Adapter models as ""a single GPU batch size of 16"" which is consistent with the subsequent information about a total batch size of 128. This provides clear and direct evidence to answer the question.

",4.0,"This question is useful because it specifically asks about a critical hyperparameter (batch size) for training a particular type of model (T2I-Adapter) on a specific hardware configuration (single GPU). A clear answer would provide valuable information to help machine learning developers set up their models correctly and optimize performance.
",5.0,"The question clearly references a specific model architecture (T2I-Adapter) and a particular training context (single GPU), but it does not require any additional information to understand what is being asked. It is asking for a general piece of information related to the model's training process, which can be deduced from documentation or common practices in deep learning.

"
"Usage:

```py
>>> from huggingface_hub.utils import validate_hf_hub_args

>>> @validate_hf_hub_args
... def my_cool_method(repo_id: str):
...     print(repo_id)

>>> my_cool_method(repo_id=""valid_repo_id"")
valid_repo_id

>>> my_cool_method(""other..repo..id"")
huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.

>>> my_cool_method(repo_id=""other..repo..id"")
huggingface_hub.utils._validators.HFValidationError: Cannot have -- or .. in repo_id: 'other..repo..id'.

>>> @validate_hf_hub_args
... def my_cool_auth_method(token: str):
...     print(token)

>>> my_cool_auth_method(token=""a token"")
""a token""

>>> my_cool_auth_method(use_auth_token=""a use_auth_token"")
""a use_auth_token""

>>> my_cool_auth_method(token=""a token"", use_auth_token=""a use_auth_token"")
UserWarning: Both `token` and `use_auth_token` are passed (...). `use_auth_token` value will be ignored.
""a token""
```

#### validate_hf_hub_args

[[autodoc]] utils.validate_hf_hub_args

#### HFValidationError

[[autodoc]] utils.HFValidationError

### Argument validators

Validators can also be used individually. Here is a list of all arguments that can be
validated.

#### repo_id

[[autodoc]] utils.validate_repo_id

#### smoothly_deprecate_use_auth_token

Not exactly a validator, but ran as well.

[[autodoc]] utils.smoothly_deprecate_use_auth_token","What is the error message raised when `repo_id` contains a double dot (`..`)?
",Cannot have -- or .. in repo_id.,huggingface/huggingface_hub/blob/main/docs/source/en/package_reference/utilities.md,5.0,"The context provides an example of what happens when `repo_id` contains a double dot (`..`) in the `validate_hf_hub_args` decorator, which raises an `HFValidationError` with a specific error message. Additionally, it documents the behavior of this validator individually, specifying that `repo_id` cannot contain `--` or `..`. This information provides enough details to understand what error is raised when `repo_id` contains a double dot (`..`).

",5.0,"This question is asking about a specific scenario with an Hugging Face repository ID containing a double dot, which likely relates to their library or API usage. The question implies that the respondent has some knowledge of how to work with repository IDs and is looking for a precise error message related to this edge case.

",5.0,"This question makes sense by itself, as it specifies what needs to be evaluated (the error message) and under what specific condition (`repo_id` containing a double dot). The context is implicit in the mention of `repo_id`, but this can be easily inferred from the question.

"
"## Community notebooks:

More notebooks developed by the community are available [here](https://hf.co/docs/transformers/community#community-notebooks).","Where can community-developed notebooks be found?
",https://hf.co/docs/transformers/community#community-notebooks,huggingface/transformers/blob/main/docs/source/en/notebooks.md,5.0,"The context directly points to a link where community-developed notebooks can be found, making it clear and unambiguous that the answer is located at the provided URL.

",4.0,"This question is useful because it seeks information about a specific resource that can be valuable to machine learning developers building NLP applications with the Hugging Face ecosystem, such as datasets, models, or documentation. The ability to find and access these resources can greatly facilitate development.

",5.0,"This question does not depend on any additional context or information. The term ""community-developed notebooks"" suggests a collection of user-generated content, likely related to technical topics such as programming, data science, or research. However, the question is clear and can be understood without knowing which specific community or platform is being referred to.

"
"|[Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | How to fine-tune an ALBERT model or another BERT-based model for the sentence-pair classification task | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)|
|[Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | How to fine-tune a Roberta model for sentiment analysis | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)|
|[Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev) | How accurate are the answers to questions generated by your seq2seq transformer model? | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)|
|[Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | How to fine-tune DistilBERT for text classification in TensorFlow | [Peter Bayerle](https://github.com/peterbayerle) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)|","How to fine-tune an ALBERT model for sentence-pair classification.
",Fine-tune ALBERT for sentence-pair classification.,huggingface/transformers/blob/main/docs/source/en/community.md,5.0,"The provided context is a list of links to Jupyter notebooks that demonstrate how to fine-tune various BERT-based models, including ALBERT, for specific NLP tasks. One of the links specifically addresses fine-tuning an ALBERT model for sentence-pair classification, providing a clear and detailed example.

",5.0,"This question is extremely useful because it targets a specific use case within the Hugging Face ecosystem, which is fine-tuning pre-trained language models like ALBERT for downstream tasks. The ability to fine-tune such models is crucial in NLP applications, and providing guidance on how to do this with ALBERT specifically would greatly benefit machine learning developers.
",5.0,"This question is clear and concise, and it asks about a specific task (fine-tuning) with a particular type of model (ALBERT) for a specific application (sentence-pair classification). It does not depend on any external context or information.
"
"Metric Card for *Current Metric*

***Metric Card Instructions:*** *Copy this file into the relevant metric folder, then fill it out and save it as README.md. Feel free to take a look at existing metric cards if you'd like examples.*

## Metric Description
*Give a brief overview of this metric.*

## How to Use
*Give general statement of how to use the metric*

*Provide simplest possible example for using the metric*

### Inputs
*List all input arguments in the format below*
- **input_field** *(type): Definition of input, with explanation if necessary. State any default value(s).*

### Output Values
*Explain what this metric outputs (e.g. a single score, a list of scores)*

*Give an example of what the metric output looks like.*

*State the range of possible values that the metric's output can take, as well as what in that range is considered good. For example: ""This metric can take on any value between 0 and 100, inclusive. Higher scores are better.""*

#### Values from Popular Papers
*Give examples, preferrably with links, to papers that have reported this metric, along with the values they have reported.*

### Examples
*Give code examples of the metric being used. Try to include examples that clear up any potential ambiguity left from the metric description above. If possible, provide a range of examples that show both typical and atypical results, as well as examples where a variety of input parameters are passed.*

## Limitations and Bias
*Note any known limitations or biases that the metric has, with links and references if possible.*

## Citation
*Cite the source where this metric was introduced.*

## Further References
*Add any useful further references.*","What is the file name of a completed Metric Card?
",README.md,huggingface/datasets/blob/main/templates/metric_card_template.md,4.0,"The context provides clear instructions on how to fill out and save a Metric Card, but it does not explicitly mention the file name of a completed Metric Card. However, based on the instruction to ""Copy this file into the relevant metric folder"", we can infer that the file name is likely the same as the original file, which is README.md.

",4.0,"This question appears to be specific to the Hugging Face interface, likely referring to the model cards feature. The question is clear but might not be directly related to building NLP applications with the Hugging Face ecosystem in the traditional sense (e.g., training models or fine-tuning). However, understanding how to use and interact with the Metric Cards can indirectly benefit developers by providing valuable insights into their models' performance.

",5.0,"This question is context-independent because it asks for a specific piece of information that can be looked up or inferred from general knowledge, without requiring any additional context beyond the term ""Metric Card"".

"
"|      |                                                                            |[TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML](https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML)|3           |11                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML/blob/main/LICENSE.txt)|                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/orca_mini_v3_13B-GGUF](https://huggingface.co/TheBloke/orca_mini_v3_13B-GGUF)|3           |3                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/orca_mini_v3_13B-GGUF/blob/main/LICENSE.txt)         |                                                                                                                     |                                                                                   |","What is the type of license associated with Llama-2 models like TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML?
",llama-2-community-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context explicitly mentions that the models ""TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML"" and ""TheBloke/orca_mini_v3_13B-GGUF"" are associated with a ""llama-2-community-license"", which can be further linked to through the provided license file. This suggests that the type of license is clearly stated in the context.
",4.0,"This question appears to be asking about a specific aspect of a model's metadata, which is relevant to understanding the usage and distribution of the model. The Llama-2 model in question has a specific license associated with it, and being able to identify this information can be useful for developers who want to integrate this model into their applications or use cases. However, without more context, it's not clear if the question is asking about a general property of Llama-2 models or something specific to TheBloke/OpenAssistant-Llama2-13B-Orca-v2-8K-3166-GGML.
",5.0,"This question is self-contained and does not rely on additional context beyond the name of a specific model. It asks for a specific piece of information (the type of license) that can be looked up or inferred from general knowledge about the Llama-2 models.

"
"New issues usually include the following.

#### 2.1. Reproducible, minimal bug reports

A bug report should always have a reproducible code snippet and be as minimal and concise as possible.
This means in more detail:
- Narrow the bug down as much as you can, **do not just dump your whole code file**.
- Format your code.
- Do not include any external libraries except for Diffusers depending on them.
- **Always** provide all necessary information about your environment; for this, you can run: `diffusers-cli env` in your shell and copy-paste the displayed information to the issue.
- Explain the issue. If the reader doesn't know what the issue is and why it is an issue, she cannot solve it.
- **Always** make sure the reader can reproduce your issue with as little effort as possible. If your code snippet cannot be run because of missing libraries or undefined variables, the reader cannot help you. Make sure your reproducible code snippet is as minimal as possible and can be copy-pasted into a simple Python shell.
- If in order to reproduce your issue a model and/or dataset is required, make sure the reader has access to that model or dataset. You can always upload your model or dataset to the [Hub](https://huggingface.co) to make it easily downloadable. Try to keep your model and dataset as small as possible, to make the reproduction of your issue as effortless as possible.

For more information, please have a look through the [How to write a good issue](#how-to-write-a-good-issue) section.

You can open a bug report [here](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=bug&projects=&template=bug-report.yml).

#### 2.2. Feature requests

A world-class feature request addresses the following points:","What is the command to run in order to provide necessary information about your environment?
",diffusers-cli env,huggingface/diffusers/blob/main/docs/source/en/conceptual/contribution.md,5.0,"The context provides a clear instruction on what information to provide about your environment when creating an issue, specifically mentioning running `diffusers-cli env` in your shell and copying-pasting the displayed information into the issue. This suggests that the command to run is explicitly stated.

",4.0,"This question appears to be asking about the Hugging Face CLI, specifically how to get information about the user's environment. The correct command would likely involve running `pip install transformers --upgrade` followed by `transformers-cli setup`. However, without more context or details, it is hard to provide an exact answer.
",4.0,"This question is clear and concise, but it seems to be asking for a specific instruction or command that applies universally across different environments. However, without more context (such as which environment management system or tool is being referred to), it's not entirely clear what type of information the command would provide.

"
"# load base and mask image
init_image = load_image(""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png"")
mask_image = load_image(""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png"")

generator = torch.Generator(""cuda"").manual_seed(92)
prompt = ""concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k""
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```

<div class=""flex flex-row gap-4"">
  <div class=""flex-1"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">base image</figcaption>
  </div>
  <div class=""flex-1"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint-sdv1.5.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">Stable Diffusion Inpainting</figcaption>
  </div>
  <div class=""flex-1"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint-sdxl.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">Stable Diffusion XL Inpainting</figcaption>
  </div>
  <div class=""flex-1"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint-kandinsky.png""/>
    <figcaption class=""mt-2 text-center text-sm text-gray-500"">Kandinsky 2.2 Inpainting</figcaption>
  </div>
</div>

## Non-inpaint specific checkpoints","What is the source URL of the mask image used in the example?
",https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inpaint.md,5.0,"The context clearly shows the source URL of the mask image used in the example, which is stored in the `mask_image` variable and loaded from ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png"". This makes it easy to determine the source URL.

",4.0,"The question appears to be related to a specific example or code snippet, possibly from the Hugging Face documentation or an online tutorial. It seems that the questioner wants to know where to find the original mask image used in this example. This suggests that they are trying to understand how the example was created and want to replicate it or use the same resources.
",5.0,"This question does not explicitly mention any context, such as a document or previous conversation. It mentions an ""example"", but it's likely referring to a general concept rather than a specific one that would require additional information.
"
"!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PhoBERT

## Overview

The PhoBERT model was proposed in [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf) by Dat Quoc Nguyen, Anh Tuan Nguyen.

The abstract from the paper is the following:

*We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.*

This model was contributed by [dqnguyen](https://huggingface.co/dqnguyen). The original code can be found [here](https://github.com/VinAIResearch/PhoBERT).

## Usage example

```python
>>> import torch
>>> from transformers import AutoModel, AutoTokenizer

>>> phobert = AutoModel.from_pretrained(""vinai/phobert-base"")
>>> tokenizer = AutoTokenizer.from_pretrained(""vinai/phobert-base"")

>>> # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!
>>> line = ""TÃ´i lÃ  sinh_viÃªn trÆ°á»ng Ä‘áº¡i_há»c CÃ´ng_nghá»‡ .""","Who contributed the PhoBERT model to the Hugging Face model hub?
",dqnguyen,huggingface/transformers/blob/main/docs/source/en/model_doc/phobert.md,5.0,"The context provides a clear attribution of the PhoBERT model to a specific contributor, Dat Quoc Nguyen (username dqnguyen), along with a link to their GitHub repository and Hugging Face profile. Additionally, the context includes an explicit statement ""This model was contributed by [dqnguyen](https://huggingface.co/dqnguyen)."" This information directly answers the question about who contributed the PhoBERT model.

",4.0,"The question seems to be a simple factual inquiry, but it's not immediately clear what context or relevance this has to NLP development with the Hugging Face ecosystem. However, understanding the origins of specific models like PhoBERT can provide insights into their design and performance characteristics.

",5.0,"This question refers to a specific model (PhoBERT) and a platform (Hugging Face model hub), but it does not require any additional information or context to be understood. The question makes sense by itself, even for someone without prior knowledge of the model or platform.

"
"## Embed Gradio Spaces on other webpages

You can embed a Gradio Space on other webpages by using either Web Components or the HTML `<iframe>` tag. Check out [our documentation](./spaces-embed) or the [Gradio documentation](https://gradio.app/sharing_your_app/#embedding-hosted-spaces) for more details.","What methods can be used to embed a Gradio Space on other webpages?
",Web Components and HTML iframe tag.,huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-gradio.md,5.0,"The context clearly states two methods to embed a Gradio Space on other webpages, specifically mentioning Web Components and the HTML iframe tag, along with references to further documentation. This provides explicit information that allows one to answer the question unambiguously.
",5.0,"This question is useful because it relates to integrating Hugging Face's Gradio library with other web technologies, which is a relevant aspect of building NLP applications. The answer could provide valuable insights into how developers can leverage the capabilities of Gradio in conjunction with other tools and frameworks.

",5.0,"The question asks about embedding a Gradio Space on other webpages, but it does not specify any particular context or setting. It assumes that the reader is familiar with Gradio and its Spaces feature, as well as basic knowledge of how to embed content on webpages.

"
"# One line to create a XLA generation function
xla_generate = tf.function(model.generate, jit_compile=True)

# Calls XLA generation without padding
tokenized_input_1 = tokenizer(input_1, return_tensors=""tf"")  # length = 4
tokenized_input_2 = tokenizer(input_2, return_tensors=""tf"")  # length = 5
print(f""`tokenized_input_1` shape = {tokenized_input_1.input_ids.shape}"")
print(f""`tokenized_input_2` shape = {tokenized_input_2.input_ids.shape}"")

print(""Calling XLA generation with tokenized_input_1..."")
print(""(will be slow as it is the first call)"")
start = time.time_ns()
xla_generate(**tokenized_input_1)
end = time.time_ns()
print(f""Execution time -- {(end - start) / 1e6:.1f} ms\n"")
# > Execution time -- 9565.1 ms

print(""Calling XLA generation with tokenized_input_2..."")
print(""(has a different length = will trigger tracing again)"")
start = time.time_ns()
xla_generate(**tokenized_input_2)
end = time.time_ns()
print(f""Execution time -- {(end - start) / 1e6:.1f} ms\n"")
# > Execution time -- 6815.0 ms
```

Oh no, that's terribly slow! A solution to keep the different combinations of shapes in check is through padding,
as mentioned above. The tokenizer classes have a `pad_to_multiple_of` argument that can be used to achieve a balance
between accepting any input length and limiting tracing.

```python
padding_kwargs = {""pad_to_multiple_of"": 8, ""padding"": True}
tokenized_input_1_with_padding = tokenizer(
    input_1, return_tensors=""tf"", **padding_kwargs
)  # length = 8
tokenized_input_2_with_padding = tokenizer(
    input_2, return_tensors=""tf"", **padding_kwargs
)  # length = 8
print(
    ""`tokenized_input_1_with_padding` shape = "",
    f""{tokenized_input_1_with_padding.input_ids.shape}""
)
print(
    ""`tokenized_input_2_with_padding` shape = "",
    f""{tokenized_input_2_with_padding.input_ids.shape}""
)","What is the argument used to pad input IDs in a tokenizer class?
",padding,huggingface/blog/blob/main/tf-xla-generate.md,5.0,"The context clearly mentions that the tokenizer classes have a `pad_to_multiple_of` argument that can be used to pad input IDs. It also provides an example of how this argument is used to achieve padding with a specific length.

",4.0,"The question is clear and specific, asking about a particular detail related to the Hugging Face ecosystem. It requires knowledge of the tokenizer class and its configuration options.
",5.0,"This question refers to a specific concept within the context of tokenization, specifically asking about an attribute or parameter related to padding input IDs. The mention of ""tokenizer class"" implies that there's a particular framework or library being referred to (likely one like Hugging Face's Transformers), but the term ""argument used to pad input IDs"" is specific and clear enough within the domain of NLP and tokenization that an operator familiar with these concepts could understand what is being asked without needing additional context.

"
"Parameter `sort` must be one of  `""last_modified""`,  `""trending""` or `""upvotes""`. Parameter `item` accepts any particular item. For example:
* `""models/teknium/OpenHermes-2.5-Mistral-7B""`
* `""spaces/julien-c/open-gpt-rhyming-robot""`
* `""datasets/squad""`
* `""papers/2311.12983""`

For more details, please check out [`list_collections`] reference.

## Create a new collection

Now that we know how to get a [`Collection`], let's create our own! Use [`create_collection`] with a title and description. To create a collection on an organization page, pass `namespace=""my-cool-org""` when creating the collection. Finally, you can also create private collections by passing `private=True`.

```py
>>> from huggingface_hub import create_collection

>>> collection = create_collection(
...     title=""ICCV 2023"",
...     description=""Portfolio of models, papers and demos I presented at ICCV 2023"",
... )
```

It will return a [`Collection`] object with the high-level metadata (title, description, owner, etc.) and an empty list of items. You will now be able to refer to this collection using it's `slug`.

```py
>>> collection.slug
'owner/iccv-2023-15e23b46cb98efca45'
>>> collection.title
""ICCV 2023""
>>> collection.owner
""username""
>>> collection.url
'https://huggingface.co/collections/owner/iccv-2023-15e23b46cb98efca45'
```

## Manage items in a collection

Now that we have a [`Collection`], we want to add items to it and organize them.

### Add items

Items have to be added one by one using [`add_collection_item`]. You only need to know the `collection_slug`, `item_id` and `item_type`. Optionally, you can also add a `note` to the item (500 characters maximum).

```py
>>> from huggingface_hub import create_collection, add_collection_item

>>> collection = create_collection(title=""OS Week Highlights - Sept 18 - 24"", namespace=""osanseviero"")
>>> collection.slug
""osanseviero/os-week-highlights-sept-18-24-650bfed7f795a59f491afb80""","What are the valid values for the parameter 'sort' in the huggingface_hub API?
","""last_modified"",  ""trending"", or  ""upvotes"".",huggingface/huggingface_hub/blob/main/docs/source/en/guides/collections.md,4.0,"The context provided is a reference documentation for the huggingface_hub API, specifically focusing on how to create and manage collections. It covers various aspects of working with collections, such as creating new collections, adding items to collections, and organizing them. However, it does not explicitly mention the parameter 'sort' or its valid values in the context of listing collections. The information about 'sort' is found in a different section (""List Collections"") which is referenced but not fully included in the provided snippet.

",4.0,"The question is quite specific and focused on a particular aspect of the Hugging Face ecosystem, which suggests that it may be useful to developers who need to understand how to properly use the `huggingface_hub` API. However, without more context or information about what kind of application or task they are working on, it's difficult to determine exactly how useful this question will be.

",5.0,"The question clearly refers to a specific API (huggingface_hub) and a particular parameter ('sort'), making it understandable by itself without requiring additional context.
"
"### Models created with love by Hugging Face with BigScience and BigCode ğŸ’—

Hugging Face has co-led two science initiatives, BigScience and BigCode. As a result of them, two large language models were created, [BLOOM](https://huggingface.co/bigscience/bloom) ğŸŒ¸ and [StarCoder](https://huggingface.co/bigcode/starcoder) ğŸŒŸ.
BLOOM is a causal language model trained on 46 languages and 13 programming languages. It is the first open-source model to have more parameters than GPT-3. You can find all the available checkpoints in the [BLOOM documentation](https://huggingface.co/docs/transformers/model_doc/bloom).

StarCoder is a language model trained on permissive code from GitHub (with 80+ programming languages ğŸ¤¯) with a Fill-in-the-Middle objective. Itâ€™s not fine-tuned on instructions, and thus, it serves more as a coding assistant to complete a given code, e.g., translate Python to C++, explain concepts (whatâ€™s recursion), or act as a terminal. You can try all of the StarCoder checkpoints [in this application](https://huggingface.co/spaces/bigcode/bigcode-playground). It also comes with a [VSCode extension](https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode).

Snippets to use all models mentioned in this blog post are given in either the model repository or the documentation page of that model type in Hugging Face.

## Licensing

Many text generation models are either closed-source or the license limits commercial use. Fortunately, open-source alternatives are starting to appear and being embraced by the community as building blocks for further development, fine-tuning, or integration with other projects. Below you can find a list of some of the large causal language models with fully open-source licenses:","How many languages is BLOOM trained on?
",46,huggingface/blog/blob/main/os-llms.md,5.0,"The context explicitly states that BLOOM is trained on ""46 languages"" and ""13 programming languages"", providing clear information about the number of languages it was trained on.

",4.0,"This question appears to be quite specific and relevant to the Hugging Face ecosystem, as it pertains directly to a language model (BLOOM) that is likely used or discussed within this community. Providing information about the training data of a popular model like BLOOM can be helpful for developers who might be evaluating its suitability for their NLP projects, especially if they work with languages other than English.

",5.0,"This question makes sense by itself because it does not rely on any specific context or information from a particular document. It is clear that BLOOM refers to an AI model, likely the BLOOM large language model.
"
"<Tip>

Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.

</Tip>

## StableDiffusionAttendAndExcitePipeline

[[autodoc]] StableDiffusionAttendAndExcitePipeline
	- all
	- __call__

## StableDiffusionPipelineOutput

[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput","What is the URL where one can learn how to explore the tradeoff between scheduler speed and quality?
",../../using-diffusers/schedulers,huggingface/diffusers/blob/main/docs/source/en/api/pipelines/attend_and_excite.md,4.0,"The context provides a clear reference to the guide where one can learn about the tradeoff between scheduler speed and quality. However, it does not explicitly mention a specific URL. Still, given the format of the provided links (../../using-diffusers/schedulers), one could reasonably infer that the complete URL would follow this pattern.

",4.0,"This question seems to be about learning resources related to Hugging Face's model schedulers, which is a specific aspect of the ecosystem. While it might not directly relate to building NLP applications, understanding the tradeoffs in scheduling can be crucial for optimizing model performance and efficiency.
",5.0,"The question explicitly mentions a specific topic (tradeoff between scheduler speed and quality), but it does not rely on any external context or information. It's asking for a general resource, which is likely a documentation or tutorial page.

"
"!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# LED

## Overview

The LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz
Beltagy, Matthew E. Peters, Arman Cohan.

The abstract from the paper is the following:","Who are the authors of the paper ""Longformer: The Long-Document Transformer""?
","Iz Beltagy, Matthew E. Peters, Arman Cohan.",huggingface/transformers/blob/main/docs/source/en/model_doc/led.md,4.0,"The context provides information about a specific paper titled ""Longformer: The Long-Document Transformer"" and mentions its authors as ""Iz Beltagy, Matthew E. Peters, Arman Cohan"". However, it does not explicitly state that this paper is related to the LED model. Nonetheless, based on the context provided, it can be inferred that the question about the authors of the mentioned paper is answerable.
",4.0,"This question is relatively specific, but it can be useful to machine learning developers building NLP applications with the Hugging Face ecosystem because understanding the context and origins of popular models like Longformer can provide valuable insights into their strengths, limitations, and potential applications. Knowing who are the authors of a paper can also help identify potential sources for more information on related topics or even collaboration opportunities.
",5.0,"This question clearly references a specific academic work, but it does not require any additional context to understand who the authors are. It's a straightforward query that can be answered by accessing information about the mentioned paper.

"
"1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, Åukasz Kaiser, Anselm Levskaya.
1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (META à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) [à¤¡à¤¿à¤œà¤¼à¤¾à¤‡à¤¨à¤¿à¤‚à¤— à¤¨à¥‡à¤Ÿà¤µà¤°à¥à¤• à¤¡à¤¿à¤œà¤¼à¤¾à¤‡à¤¨ à¤¸à¥à¤ªà¥‡à¤¸] (https://arxiv.org/) à¤ªà¥‡à¤ªà¤° à¤•à¥‡ à¤¸à¤¾à¤¥ à¤œà¤¾à¤°à¥€ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤à¤¬à¥à¤¸/2003.13678) à¤‡à¤²à¤¿à¤œà¤¾ à¤°à¤¾à¤¡à¥‹à¤¸à¤¾à¤µà¥‹à¤µà¤¿à¤•, à¤°à¤¾à¤œ à¤ªà¥à¤°à¤¤à¥€à¤• à¤•à¥‹à¤¸à¤¾à¤°à¤¾à¤œà¥‚, à¤°à¥‰à¤¸ à¤—à¤¿à¤°à¥à¤¶à¤¿à¤•, à¤•à¥ˆà¤®à¤¿à¤‚à¤— à¤¹à¥€, à¤ªà¤¿à¤“à¤Ÿà¤° à¤¡à¥‰à¤²à¤° à¤¦à¥à¤µà¤¾à¤°à¤¾à¥¤
1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (à¤—à¥‚à¤—à¤² à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) à¤¸à¤¾à¤¥ à¤µà¤¾à¤²à¤¾ à¤ªà¥‡à¤ªà¤° [à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤­à¤¾à¤·à¤¾ à¤®à¥‰à¤¡à¤² à¤®à¥‡à¤‚ à¤à¤®à¥à¤¬à¥‡à¤¡à¤¿à¤‚à¤— à¤•à¤ªà¤²à¤¿à¤‚à¤— à¤ªà¤° à¤ªà¥à¤¨à¤°à¥à¤µà¤¿à¤šà¤¾à¤°](https://arxiv .org/pdf/2010.12821.pdf) à¤¹à¥à¤¯à¥à¤‚à¤— à¤µà¥‹à¤¨ à¤šà¥à¤‚à¤—, à¤¥à¤¿à¤¬à¥‰à¤²à¥à¤Ÿ à¤«à¤¼à¥‡à¤µà¤°à¥€, à¤¹à¥‡à¤¨à¤°à¥€ à¤¤à¥à¤¸à¤¾à¤ˆ, à¤à¤®. à¤œà¥‰à¤¨à¤¸à¤¨, à¤¸à¥‡à¤¬à¥‡à¤¸à¥à¤Ÿà¤¿à¤¯à¤¨ à¤°à¥à¤¡à¤° à¤¦à¥à¤µà¤¾à¤°à¤¾à¥¤
1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (à¤®à¤¾à¤‡à¤•à¥à¤°à¥‹à¤¸à¥‰à¤«à¥à¤Ÿ à¤°à¤¿à¤¸à¤°à¥à¤š à¤¸à¥‡) [à¤¡à¥€à¤ª à¤°à¥‡à¤¸à¤¿à¤¡à¥à¤…à¤² à¤²à¤°à¥à¤¨à¤¿à¤‚à¤— à¤«à¥‰à¤° à¤‡à¤®à¥‡à¤œ à¤°à¤¿à¤•à¤—à¥à¤¨à¤¿à¤¶à¤¨] (https://arxiv. org/abs/1512.03385) à¤•à¥ˆà¤®à¤¿à¤‚à¤— à¤¹à¥‡, à¤œà¤¿à¤¯à¤¾à¤‚à¤—à¥à¤¯à¥ à¤à¤¾à¤‚à¤—, à¤¶à¤¾à¤“à¤•à¤¿à¤‚à¤— à¤°à¥‡à¤¨, à¤œà¤¿à¤¯à¤¾à¤¨ à¤¸à¤¨ à¤¦à¥à¤µà¤¾à¤°à¤¾à¥¤
1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (à¤«à¥‡à¤¸à¤¬à¥à¤• à¤¸à¥‡), à¤¸à¤¾à¤¥ à¤®à¥‡à¤‚ à¤•à¤¾à¤—à¤œ [à¤®à¤œà¤¬à¥‚à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤…à¤¨à¥à¤•à¥‚à¤²à¤¿à¤¤ BERT à¤ªà¥à¤°à¥€à¤Ÿà¥à¤°à¥‡à¤¨à¤¿à¤‚à¤— à¤¦à¥ƒà¤·à¥à¤Ÿà¤¿à¤•à¥‹à¤£](https://arxiv.org/abs /1907.11692) à¤¯à¤¿à¤¨à¤¹à¤¾à¤¨ à¤²à¤¿à¤¯à¥‚, à¤®à¤¾à¤¯à¤² à¤“à¤Ÿ, à¤¨à¤®à¤¨ à¤—à¥‹à¤¯à¤², à¤œà¤¿à¤‚à¤—à¤«à¥‡à¤ˆ à¤¡à¥‚, à¤®à¤‚à¤¦à¤¾à¤° à¤œà¥‹à¤¶à¥€, à¤¡à¥ˆà¤¨à¤•à¥€ à¤šà¥‡à¤¨, à¤“à¤®à¤° à¤²à¥‡à¤µà¥€, à¤®à¤¾à¤‡à¤• à¤²à¥à¤ˆà¤¸, à¤²à¥à¤¯à¥‚à¤• à¤œà¤¼à¥‡à¤Ÿà¤²à¤®à¥‰à¤¯à¤°, à¤µà¥‡à¤¸à¥‡à¤²à¤¿à¤¨ à¤¸à¥à¤Ÿà¥‹à¤¯à¤¾à¤¨à¥‹à¤µ à¤¦à¥à¤µà¤¾à¤°à¤¾à¥¤
1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.","Who are the authors of the paper ""Reformer: The Efficient Transformer""?
","Nikita Kitaev, Åukasz Kaiser, Anselm Levskaya.",huggingface/transformers/blob/main/README_hd.md,5.0,"The context provides information about various models released with their respective papers, but the question specifically asks for the authors of a particular paper, ""Reformer: The Efficient Transformer"". The relevant sentence in the context is ""1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, Åukasz Kaiser, Anselm Levskaya."" This sentence clearly mentions the authors of the paper ""Reformer: The Efficient Transformer"", making it possible to answer the question unambiguously.

",4.0,"This question is useful because it requires knowledge about a specific NLP research paper, which indicates that the person asking the question has some background in the field and may be looking to apply insights or techniques from the Reformer model. Furthermore, understanding the authors of a research paper can provide context for their work and contributions to the field.
",5.0,"This question can be understood without any additional context, as it refers to a specific scientific paper with a well-known title. The phrase ""paper Reformer"" unambiguously identifies the work, and there is no implicit mention of a particular setting or information that would require prior knowledge.
"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PEFT

ğŸ¤— PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model's parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.

PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference.","What is the name of the library for efficiently adapting large pretrained models?
",PEFT,huggingface/peft/blob/main/docs/source/index.md,5.0,"The context provides a clear description of PEFT as a library for efficiently adapting large pretrained models by fine-tuning only a small number of extra model parameters. It also mentions that PEFT is integrated with several popular libraries, including Transformers, Diffusers, and Accelerate. This information makes it easy to determine the name of the library mentioned in the question.

",4.0,"This question appears to be related to the Hugging Face ecosystem, specifically asking about a library that facilitates adapting large pre-trained models. A library like this would likely be useful in NLP tasks where fine-tuning existing models is common. However, without more context or specificity regarding which type of adaptation (e.g., few-shot learning, transfer learning) or what kind of models (e.g., transformers), the usefulness can't be fully assessed.

",5.0,"The question makes sense without any additional context, as it refers to a specific task (adapting large pretrained models) and asks for the name of a library that can perform this task. The answer would be a well-known library such as Hugging Face's Transformers or Gradio.
"
"[[autodoc]] datasets.logging.set_verbosity_error

[[autodoc]] datasets.logging.disable_propagation

[[autodoc]] datasets.logging.enable_propagation

## Configure progress bars

By default, `tqdm` progress bars will be displayed during dataset download and preprocessing. You can disable them globally by setting `HF_DATASETS_DISABLE_PROGRESS_BARS`
environment variable. You can also enable/disable them using [`~utils.enable_progress_bars`] and [`~utils.disable_progress_bars`]. If set, the environment variable has priority on the helpers.

[[autodoc]] datasets.utils.enable_progress_bars

[[autodoc]] datasets.utils.disable_progress_bars

[[autodoc]] datasets.utils.are_progress_bars_disabled","What is the name of the environment variable that globally disables tqdm progress bars?
",HF_DATASETS_DISABLE_PROGRESS_BARS,huggingface/datasets/blob/main/docs/source/package_reference/utilities.mdx,5.0,"The context clearly states that the name of the environment variable to globally disable tqdm progress bars is HF_DATASETS_DISABLE_PROGRESS_BARS. This information is explicitly mentioned in a section labeled ""Configure progress bars"". Additionally, it is mentioned that this environment variable has priority over other helpers used to enable or disable progress bars.

",4.0,"This question appears to be relevant to NLP development with the Hugging Face ecosystem, specifically related to their library's behavior and configuration. The mention of tqdm progress bars suggests an interest in fine-tuning performance or appearance, which is common in NLP tasks.

",5.0,"The question refers to a specific configuration option related to tqdm, a popular library for displaying progress bars. However, it does not require any additional context or information about the environment in which tqdm is being used.

"
"Integrations with the Hugging Face Hub[[integrations-with-the-hugging-face-hub]]

<CourseFloatingBanner chapter={9}
  classNames=""absolute z-10 right-0 top-0""
  notebooks={[
    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section5.ipynb""},
    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section5.ipynb""},
]} />

To make your life even easier, Gradio integrates directly with Hugging Face Hub and Hugging Face Spaces.
You can load demos from the Hub and Spaces with only *one line of code*.

### Loading models from the Hugging Face Hub[[loading-models-from-the-hugging-face-hub]]
To start with, choose one of the thousands of models Hugging Face offers through the Hub, as described in [Chapter 4](/course/chapter4/2).

Using the special `Interface.load()` method, you pass `""model/""` (or, equivalently, `""huggingface/""`)
followed by the model name.
For example, here is the code to build a demo for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B), a large language model, add a couple of example inputs:

```py
import gradio as gr

title = ""GPT-J-6B""
description = ""Gradio Demo for GPT-J 6B, a transformer model trained using Ben Wang's Mesh Transformer JAX. 'GPT-J' refers to the class of model, while '6B' represents the number of trainable parameters. To use it, simply add your text, or click one of the examples to load them. Read more at the links below.""
article = ""<p style='text-align: center'><a href='https://github.com/kingoflolz/mesh-transformer-jax' target='_blank'>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</a></p>""

gr.Interface.load(
    ""huggingface/EleutherAI/gpt-j-6B"",
    inputs=gr.Textbox(lines=5, label=""Input Text""),
    title=title,
    description=description,
    article=article,
).launch()
```

The code above will produce the interface below:","How many trainable parameters does GPT-J-6B have?
",6 billion.,huggingface/course/blob/main/chapters/en/chapter9/5.mdx,4.0,"The context provides information about loading models from the Hugging Face Hub and building demos with Gradio. It specifically mentions GPT-J-6B, a large language model, but only briefly describes it as having ""6B"" trainable parameters without elaborating on what this means.

",4.0,"This question seems to be specific to a particular model variant (GPT-J-6B) within the Hugging Face ecosystem. Providing information on the number of trainable parameters can help developers understand the computational requirements and capacity of the model for NLP tasks. However, without knowing the context or how this information will be used (e.g., determining model size, optimizing training, or choosing between models), it's slightly less directly useful compared to questions that provide a broader understanding of how to implement or utilize Hugging Face models in different scenarios.
",5.0,"This question can be understood without any additional context, as it refers to a specific model (GPT-J-6B) and asks for a well-defined characteristic of that model (the number of trainable parameters).
"
"<h4 align=""center"">
    <p>
        <a href=""https://github.com/huggingface/transformers/"">English</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_zh-hans.md"">ç®€ä½“ä¸­æ–‡</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_zh-hant.md"">ç¹é«”ä¸­æ–‡</a> |
        <b>í•œêµ­ì–´</b> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_es.md"">EspaÃ±ol</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_ja.md"">æ—¥æœ¬èª</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_hd.md"">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a>
        <a href=""https://github.com/huggingface/transformers//blob/main/README_te.md"">à°¤à±†à°²à±à°—à±</a> |
    </p>
</h4>

<h3 align=""center"">
    <p> Jax, Pytorch, TensorFlowë¥¼ ìœ„í•œ ìµœì²¨ë‹¨ ìì—°ì–´ì²˜ë¦¬</p>
</h3>

<h3 align=""center"">
    <a href=""https://hf.co/course""><img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png""></a>
</h3>

ğŸ¤— TransformersëŠ” ë¶„ë¥˜, ì •ë³´ ì¶”ì¶œ, ì§ˆë¬¸ ë‹µë³€, ìš”ì•½, ë²ˆì—­, ë¬¸ì¥ ìƒì„± ë“±ì„ 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ìˆ˜ì²œê°œì˜ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ ì œê³µí•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ ëª©í‘œëŠ” ëª¨ë‘ê°€ ìµœì²¨ë‹¨ì˜ NLP ê¸°ìˆ ì„ ì‰½ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸ¤— TransformersëŠ” ì´ëŸ¬í•œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œí•´ íŠ¹ì • í…ìŠ¤íŠ¸ì— ì‚¬ìš©í•˜ê³ , ì›í•˜ëŠ” ë°ì´í„°ë¡œ fine-tuningí•´ ì»¤ë®¤ë‹ˆí‹°ë‚˜ ìš°ë¦¬ì˜ [ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models)ì— ê³µìœ í•  ìˆ˜ ìˆë„ë¡ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ êµ¬ì¡°ë¥¼ ì •ì˜í•˜ëŠ” ê° íŒŒì´ì¬ ëª¨ë“ˆì€ ì™„ì „íˆ ë…ë¦½ì ì´ì—¬ì„œ ì—°êµ¬ ì‹¤í—˜ì„ ìœ„í•´ ì†ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ¤— TransformersëŠ” ê°€ì¥ ìœ ëª…í•œ 3ê°œì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì´ë“¤ì€ ì„œë¡œ ì™„ë²½íˆ ì—°ë™ë©ë‹ˆë‹¤ â€” [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/). ê°„ë‹¨í•˜ê²Œ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ë˜ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì¶”ë¡ ì„ ìœ„í•´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì˜¨ë¼ì¸ ë°ëª¨

ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì„ [ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/models) í˜ì´ì§€ì—ì„œ ë°”ë¡œ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³µê°œ ë° ë¹„ê³µê°œ ëª¨ë¸ì„ ìœ„í•œ [ë¹„ê³µê°œ ëª¨ë¸ í˜¸ìŠ¤íŒ…, ë²„ì „ ê´€ë¦¬, ì¶”ë¡  API](https://huggingface.co/pricing)ë„ ì œê³µí•©ë‹ˆë‹¤.","Which three deep learning libraries does ğŸ¤— Transformers support?
","Jax, PyTorch, TensorFlow.",huggingface/transformers/blob/main/README_ko.md,5.0,"The context explicitly mentions that ğŸ¤— Transformers supports the three deep learning libraries Jax, PyTorch, and TensorFlow. This information is clearly stated in the second paragraph of the text.
",4.0,"This question appears to be a straightforward inquiry about the compatibility of the Hugging Face Transformers library with other deep learning libraries. It demonstrates a basic understanding of the ecosystem and asks for specific information that can be easily verified.
",5.0,"The question is clear and directly related to the documentation of Hugging Face's Transformers library, but it can still be understood independently without referencing a specific context or setting.
"
"## Tips

- Not all adapter types can be combined. See `peft.tuners.mixed.COMPATIBLE_TUNER_TYPES` for a list of compatible types. An error will be raised if you are trying to combine incompatible adapter types.
- It is possible to mix multiple adapters of the same type. This can be useful to combine adapters with very different configs.
- If you want to combine a lot of different adapters, it is most performant to add the same types of adapters consecutively. E.g., add LoRA1, LoRA2, LoHa1, LoHa2 in this order, instead of LoRA1, LoHa1, LoRA2, LoHa2. The order will make a difference for the outcome in most cases, but since no order is better a priori, it is best to choose the order that is most performant.","What happens when trying to combine incompatible adapter types?
",An error will be raised.,huggingface/peft/blob/main/docs/source/developer_guides/mixed_models.md,5.0,"The context clearly states that an error will be raised when trying to combine incompatible adapter types, and provides a specific list of compatible types. This information makes it possible to determine what happens in such a situation.
",4.0,"This question is likely useful because it addresses a specific scenario that may arise during the development of NLP applications with the Hugging Face ecosystem. The combination of incompatible adapter types can lead to unexpected behavior or errors, and understanding what happens in such cases can help developers troubleshoot and improve their models.

",5.0,"This question seems clear and context-independent. The phrase ""adapter types"" suggests that it refers to a specific concept related to model adapters, which are used to modify or extend pre-trained models in various ways (e.g., adding new functionality). However, the actual behavior when trying to combine incompatible adapter types is not specified within the question itself.

"
"Flax weights are available in Hugging Face Hub as part of the Stable Diffusion repo. The Stable Diffusion model is distributed under the CreateML OpenRail-M license. It's an open license that claims no rights on the outputs you generate and prohibits you from deliberately producing illegal or harmful content. The [model card](https://huggingface.co/CompVis/stable-diffusion-v1-4) provides more details, so take a moment to read them and consider carefully whether you accept the license. If you do, you need to be a registered user in the Hub and use an access token for the code to work. You have two options to provide your access token:

-   Use the `huggingface-cli login` command-line tool in your terminal and paste your token when prompted. It will be saved in a file in your computer.
-   Or use `notebook_login()` in a notebook, which does the same thing.

The following cell will present a login interface unless you've already authenticated before in this computer. You'll need to paste your access token.


``` python
if not (Path.home()/'.huggingface'/'token').exists(): notebook_login()
```


TPU devices support `bfloat16`, an efficient half-float type. We'll use it for our tests, but you can also use `float32` to use full precision instead.


``` python
dtype = jnp.bfloat16
```


Flax is a functional framework, so models are stateless and parameters are stored outside them. Loading the pre-trained Flax pipeline will return both the pipeline itself and the model weights (or parameters). We are using a `bf16` version of the weights, which leads to type warnings that you can safely ignore.


``` python
pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(
    ""CompVis/stable-diffusion-v1-4"",
    revision=""bf16"",
    dtype=dtype,
)
```

## Inference","What is the name of the license under which the Stable Diffusion model is distributed?
",CreateML OpenRail-M license,huggingface/blog/blob/main/stable_diffusion_jax.md,4.0,"The context clearly states that the Stable Diffusion model is distributed under the CreateML OpenRail-M license, and it provides a link to the model card for more information. However, there is no direct answer to the question ""What is the name of the license"" within the given text snippet. The correct license name can be inferred from the context.
",5.0,"This question appears to be very specific and relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it requires knowledge about the licensing terms of a popular model in this field. The answer would likely be useful for those who want to use the Stable Diffusion model or understand its intellectual property rights.
",5.0,"The question refers to a specific model (Stable Diffusion) and asks about its licensing information, but it does not require additional context beyond this. It can be understood independently without knowing any specifics about the model's usage or deployment.

"
"1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (VinAI Research ã‹ã‚‰) Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/)
1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (Google Research ã‹ã‚‰) Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (Google Research ã‹ã‚‰) Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)
1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (Microsoft Research AI4Science ã‹ã‚‰) Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9)
1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (Google AI ã‹ã‚‰) Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil ã‹ã‚‰å…¬é–‹ã•ã‚ŒãŸç ”ç©¶è«–æ–‡: [Big Transfer (BiT)](https://arxiv.org/abs/1912.11370)Houlsby.","Who are the authors of the BERTweet research paper?
","Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen",huggingface/transformers/blob/main/README_ja.md,4.0,"The context provides a list of research papers and their corresponding authors, but it does not specifically mention the BERTweet research paper. However, upon closer inspection, we can find that the first item in the list mentions ""BERTweet"" as one of its model names. Therefore, the authors mentioned for BERTweet are Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.

",4.0,"This question is useful because it provides specific information about the authors of a well-known NLP research paper. Knowing the authors can be helpful for machine learning developers who want to build upon or cite this research in their own projects.
",5.0,"The question does not require any additional context or information, as it clearly references a specific research paper (BERTweet) and asks for its authorship. This makes it understandable without any external knowledge beyond basic familiarity with academic publications.
"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Evaluating Diffusion Models

<a target=""_blank"" href=""https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

Evaluation of generative models like [Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion) is subjective in nature. But as practitioners and researchers, we often have to make careful choices amongst many different possibilities. So, when working with different generative models (like GANs, Diffusion, etc.), how do we choose one over the other?

Qualitative evaluation of such models can be error-prone and might incorrectly influence a decision.
However, quantitative metrics don't necessarily correspond to image quality. So, usually, a combination
of both qualitative and quantitative evaluations provides a stronger signal when choosing one model
over the other.

In this document, we provide a non-exhaustive overview of qualitative and quantitative methods to evaluate Diffusion models. For quantitative methods, we specifically focus on how to implement them alongside `diffusers`.

The methods shown in this document can also be used to evaluate different [noise schedulers](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview) keeping the underlying generation model fixed.

## Scenarios","What is the license under which software distributed by HuggingFace is distributed?
","Apache License, Version 2.0",huggingface/diffusers/blob/main/docs/source/en/conceptual/evaluation.md,5.0,"The context provided is a license agreement for software distributed by HuggingFace, and it clearly states that the software is licensed under the Apache License, Version 2.0. This information is explicitly mentioned in the first paragraph of the license section.

",4.0,"This question seems relevant to developers who want to understand the terms of using or contributing to the Hugging Face ecosystem, which can be important for compliance and intellectual property reasons. However, it may not provide significant technical insights into building NLP applications.
",5.0,"The question refers to a specific entity (Hugging Face), but it does not require additional context or information from outside sources. It's clear what is being asked, and an operator with knowledge of Hugging Face would understand the question immediately.
"
"Fine-tuning for image classification using LoRA and ğŸ¤— PEFT

## Vision Transformer model from transformers

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_peft_lora.ipynb)

We provide a notebook (`image_classification_peft_lora.ipynb`) where we learn how to use [LoRA](https://arxiv.org/abs/2106.09685) from ğŸ¤— PEFT to fine-tune an image classification model by ONLY using **0.7%** of the original trainable parameters of the model.

LoRA adds low-rank ""update matrices"" to certain blocks in the underlying model (in this case the attention blocks) and ONLY trains those matrices during fine-tuning. During inference, these update matrices are _merged_ with the original model parameters. For more details, check out the [original LoRA paper](https://arxiv.org/abs/2106.09685).

## PoolFormer model from timm

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb)

The notebook `image_classification_timm_peft_lora.ipynb` showcases fine-tuning an image classification model using from the [timm](https://huggingface.co/docs/timm/index) library. Again, LoRA is used to reduce the numberof trainable parameters to a fraction of the total.","What percentage of the original trainable parameters of the model is used when fine-tuning with LoRA?
",0.7%,huggingface/peft/blob/main/examples/image_classification/README.md,5.0,"The context clearly states that when fine-tuning with LoRA, only 0.7% of the original trainable parameters of the model are used. This information is explicitly mentioned in the text as an example of using LoRA from PEFT to fine-tune a model.

",5.0,"This question is quite specific and directly related to the Hugging Face ecosystem, specifically to the Light On-Ramp (LoRA) technique for efficient fine-tuning. The answer would require knowledge of the implementation details of LoRA in the Hugging Face models, which makes it a useful question for developers looking to optimize their model performance while reducing computational overhead.
",5.0,"The question explicitly mentions the action of ""fine-tuning"" with LoRA, but it doesn't require any additional information to understand what is being asked. The concept of LoRA and fine-tuning is clear from the context of the question itself.

"
"`bert-base-cased` is limited to 512 tokens, which means we would have to
cut possibly important information from the article. Because most of the
important information is often found at the beginning of articles and
because we want to be computationally efficient, we decide to stick to
`bert-base-cased` with a `max_length` of 512 in this notebook. This
choice is not optimal but has shown to yield [good
results](https://arxiv.org/abs/1907.12461) on CNN/Dailymail.
Alternatively, one could leverage long-range sequence models, such as
[Longformer](https://huggingface.co/allenai/longformer-large-4096) to be
used as the encoder.

Regarding the summary length, we can see that a length of 128 already
includes all of the summary labels. 128 is easily within the limits of
`bert-base-cased`, so we decide to limit the generation to 128.

Again, we will make use of the `.map()` function - this time to
transform each training batch into a batch of model inputs.

`""article""` and `""highlights""` are tokenized and prepared as the
Encoder\'s `""input_ids""` and Decoder\'s `""decoder_input_ids""`
respectively.

`""labels""` are shifted automatically to the left for language modeling
training.

Lastly, it is very important to remember to ignore the loss of the
padded labels. In ğŸ¤—Transformers this can be done by setting the label to
-100. Great, let\'s write down our mapping function then.

```python
encoder_max_length=512
decoder_max_length=128

def process_data_to_model_inputs(batch):
  # tokenize the inputs and labels
  inputs = tokenizer(batch[""article""], padding=""max_length"", truncation=True, max_length=encoder_max_length)
  outputs = tokenizer(batch[""highlights""], padding=""max_length"", truncation=True, max_length=decoder_max_length)

  batch[""input_ids""] = inputs.input_ids
  batch[""attention_mask""] = inputs.attention_mask
  batch[""labels""] = outputs.input_ids.copy()","What is the maximum length of the input IDs for the decoder in the model?

",128,huggingface/blog/blob/main/warm-starting-encoder-decoder.md,5.0,"The context clearly states that the maximum length of the input IDs for the decoder in the model is set to 128, which is a direct answer to the question. However, it's worth noting that this value is not directly related to the overall model architecture or its limitations (as mentioned with `bert-base-cased` having a limit of 512 tokens), but rather a specific choice made for this particular task.
",4.0,"This question is useful because understanding the maximum input length for the decoder is crucial when working with models like transformers, where padding tokens and truncation can significantly impact model performance. Knowing this limit helps developers decide on appropriate input sizes and handle inputs that exceed this limit.
",5.0,"The question refers to specific components of a model (decoder, input IDs) without requiring any additional context or information. It appears to be asking about a technical specification of a particular part of the model architecture.
"
"Share a dataset to the Hub

The [Hub](https://huggingface.co/datasets) is home to an extensive collection of community-curated and popular research datasets. We encourage you to share your dataset to the Hub to help grow the ML community and accelerate progress for everyone. All contributions are welcome; adding a dataset is just a drag and drop away!

Start by [creating a Hugging Face Hub account](https://huggingface.co/join) if you don't have one yet.

## Upload with the Hub UI

The Hub's web-based interface allows users without any developer experience to upload a dataset.

### Create a repository

A repository hosts all your dataset files, including the revision history, making storing more than one dataset version possible.

1. Click on your profile and select **New Dataset** to create a new dataset repository.
2. Pick a name for your dataset, and choose whether it is a public or private dataset. A public dataset is visible to anyone, whereas a private dataset can only be viewed by you or members of your organization.

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/create_repo.png""/>
</div>

### Upload dataset

1. Once you've created a repository, navigate to the **Files and versions** tab to add a file. Select **Add file** to upload your dataset files. We support many text, audio, and image data extensions such as `.csv`, `.mp3`, and `.jpg` among many others. For text data extensions like `.csv`, `.json`, `.jsonl`, and `.txt`, we recommend compressing them before uploading to the Hub (to `.zip` or `.gz` file extension for example).","What is the minimum developer experience required to upload a dataset using the Hub's web-based interface?
",None.,huggingface/datasets/blob/main/docs/source/upload_dataset.mdx,4.0,"The context clearly states that ""The Hub's web-based interface allows users without any developer experience to upload a dataset,"" which implies that no developer experience is required. However, it also mentions that text data extensions like "".csv"", "".json"", etc., are recommended to be compressed before uploading. This might cause some ambiguity, as it could be interpreted as requiring some level of technical knowledge to handle file compression.

",4.0,"This question seems straightforward and aimed at helping developers who want to use the Hugging Face Hub, which suggests that it would be useful for those familiar with the ecosystem but not necessarily experts. However, without more context or specifics about what ""upload a dataset"" entails (e.g., formatting requirements, data size limitations), one might argue its usefulness is limited by potential ambiguity.

",5.0,"The question clearly states what it's asking about (uploading a dataset) and mentions a specific tool (Hub's web-based interface), but doesn't require any additional context or information beyond that.

"
"--
title: Text Duplicates
emoji: ğŸ¤—
colorFrom: green
colorTo: purple
sdk: gradio
sdk_version: 3.0.2
app_file: app.py
pinned: false
tags:
- evaluate
- measurement
description: >-
  Returns the duplicate fraction of duplicate strings in the input.
---

# Measurement Card for Text Duplicates

## Measurement Description

The `text_duplicates` measurement returns the fraction of duplicated strings in the input data.

## How to Use

This measurement requires a list of strings as input:

```python
>>> data = [""hello sun"",""hello moon"", ""hello sun""]
>>> duplicates = evaluate.load(""text_duplicates"")
>>> results = duplicates.compute(data=data)
```

### Inputs
- **data** (list of `str`): The input list of strings for which the duplicates are calculated.

### Output Values
- **duplicate_fraction**(`float`): the fraction of duplicates in the input string(s).
- **duplicates_dict**(`list`): (optional) a list of tuples with the duplicate strings and the number of times they are repeated.

By default, this measurement outputs a dictionary containing the fraction of duplicates in the input string(s) (`duplicate_fraction`):
  )
```python
{'duplicate_fraction': 0.33333333333333337}
```

With the `list_duplicates=True` option, this measurement will also output a dictionary of tuples with duplicate strings and their counts.

```python
{'duplicate_fraction': 0.33333333333333337, 'duplicates_dict': {'hello sun': 2}}
```

Warning: the `list_duplicates=True` function can be memory-intensive for large datasets.

### Examples

Example with no duplicates

```python
>>> data = [""foo"", ""bar"", ""foobar""]
>>> duplicates = evaluate.load(""text_duplicates"")
>>> results = duplicates.compute(data=data)
>>> print(results)
{'duplicate_fraction': 0.0}
```","What is the default output of the `text_duplicates` measurement?
",A dictionary containing the fraction of duplicates in the input string(s).,huggingface/evaluate/blob/main/measurements/text_duplicates/README.md,5.0,"The question asks for the default output of the `text_duplicates` measurement, and based on the provided context, it is clear that the default output is a dictionary containing the fraction of duplicates in the input string(s). This is explicitly stated in the ""How to Use"" section of the Measurement Card. Additionally, an example is given where no duplicates are present, further confirming this information.
",4.0,"This question seems to be asking about a specific aspect of the Hugging Face ecosystem, namely the behavior of the `text_duplicates` measurement. The question appears to be focused on understanding the default output of this measurement, which suggests that the person asking is trying to understand how to use or troubleshoot their NLP application. Given that it's a very specific and technical question, I would rate it as 4 out of 5 in terms of usefulness for machine learning developers building NLP applications with the Hugging Face ecosystem.

",5.0,"The question clearly refers to a specific function or method (`text_duplicates` measurement) without any context, making it self-explanatory for someone familiar with its purpose and functionality.
"
"Dataset viewer

The dataset page includes a table with the contents of the dataset, arranged by pages of 100 rows. You can navigate between pages using the buttons at the bottom of the table.

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/dataset-viewer.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/dataset-viewer-dark.png""/>
</div>

## Inspect data distributions

At the top of the columns you can see the graphs representing the distribution of their data. This gives you a quick insight on how balanced your classes are, what are the range and distribution of numerical data and lengths of texts, and what portion of the column data is missing.

## Filter by value

If you click on a bar of a histogram from a numerical column, the dataset viewer will filter the data and show only the rows with values that fall in the selected range.
Similarly, if you select one class from a categorical column, it will show only the rows from the selected category.

## Search a word in the dataset

You can search for a word in the dataset by typing it in the search bar at the top of the table. The search is case-insensitive and will match any row containing the word. The text is searched in the columns of `string`, even if the values are nested in a dictionary or a list.

## Share a specific row

You can share a specific row by clicking on it, and then copying the URL in the address bar of your browser. For example https://huggingface.co/datasets/glue/viewer/mrpc/test?p=2&row=241 will open the dataset viewer on the MRPC dataset, on the test split, and on the 241st row.

## Access the parquet files","What is the purpose of a histogram in a numerical column?
",It gives you a quick insight on how balanced your classes are.,huggingface/hub-docs/blob/main/docs/hub/datasets-viewer.md,5.0,"The context describes a dataset viewer that includes a table with graphs representing data distributions. It specifically mentions histograms for numerical columns, allowing users to filter the data by selecting a range of values from the histogram bars. This suggests that the purpose of a histogram in a numerical column is to facilitate filtering and exploration of the data.

",4.0,"The provided question appears to be more relevant to data analysis or visualization rather than NLP, as histograms are typically used for visualizing distributions of continuous data. However, understanding how to work with various data types and their representations can still be useful for machine learning developers in the Hugging Face ecosystem, particularly when dealing with numerical features in datasets.

",5.0,"The question refers to a general concept in data analysis, making it clear what is being asked without requiring additional context or information.

"
"<Tip warning={true}>

Note that, despite our advice to use key-value caches, your LLM output may be slightly different when you use them. This is a property of the matrix multiplication kernels themselves -- you can read more about it [here](https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535).

</Tip>

#### 3.2.1 Multi-round conversation

The key-value cache is especially useful for applications such as chat where multiple passes of auto-regressive decoding are required. Let's look at an example.

```
User: How many people live in France?
Assistant: Roughly 75 million people live in France
User: And how many are in Germany?
Assistant: Germany has ca. 81 million inhabitants
```

In this chat, the LLM runs auto-regressive decoding twice:
  1. The first time, the key-value cache is empty and the input prompt is `""User: How many people live in France?""` and the model auto-regressively generates the text `""Roughly 75 million people live in France""` while increasing the key-value cache at every decoding step.
  2. The second time the input prompt is `""User: How many people live in France? \n Assistant: Roughly 75 million people live in France \n User: And how many in Germany?""`. Thanks to the cache, all key-value vectors for the first two sentences are already computed. Therefore the input prompt only consists of `""User: And how many in Germany?""`. While processing the shortened input prompt, it's computed key-value vectors are concatenated to the key-value cache of the first decoding. The second Assistant's answer `""Germany has ca. 81 million inhabitants""` is then auto-regressively generated with the key-value cache consisting of encoded key-value vectors of `""User: How many people live in France? \n Assistant: Roughly 75 million people live in France \n User: And how many are in Germany?""`.","In what kind of applications is a key-value cache especially useful?
",chat where multiple passes of auto-regressive decoding are required.,huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md,5.0,"The question is asking about the applications where a key-value cache is especially useful, and the given context mentions that it's particularly useful for multi-round conversations like chat. This implies that the correct answer would be related to scenarios where multiple passes of auto-regressive decoding are required.

",4.0,"This question appears to be relevant to machine learning developers, particularly those working with Hugging Face's transformers library, which can benefit from using a key-value cache for tasks such as text generation and language modeling. The question encourages the test-taker to think about scenarios where caching can improve performance and efficiency.
",5.0,"This question makes sense on its own without any additional context or background information. The term ""key-value cache"" is self-explanatory, and the question asks for general applicability, which implies that it's asking for examples or scenarios where such a cache would be particularly useful.

"
"Use with PyTorch

This document is a quick introduction to using `datasets` with PyTorch, with a particular focus on how to get
`torch.Tensor` objects out of our datasets, and how to use a PyTorch `DataLoader` and a Hugging Face `Dataset`
with the best performance.

## Dataset format

By default, datasets return regular python objects: integers, floats, strings, lists, etc.

To get PyTorch tensors instead, you can set the format of the dataset to `pytorch` using [`Dataset.with_format`]:

```py
>>> from datasets import Dataset
>>> data = [[1, 2],[3, 4]]
>>> ds = Dataset.from_dict({""data"": data})
>>> ds = ds.with_format(""torch"")
>>> ds[0]
{'data': tensor([1, 2])}
>>> ds[:2]
{'data': tensor([[1, 2],
         [3, 4]])}
```

<Tip>

A [`Dataset`] object is a wrapper of an Arrow table, which allows fast zero-copy reads from arrays in the dataset to PyTorch tensors.

</Tip>


To load the data as tensors on a GPU, specify the `device` argument:
```py
>>> import torch
>>> device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
>>> ds = ds.with_format(""torch"", device=device)
>>> ds[0]
{'data': tensor([1, 2], device='cuda:0')}
```

## N-dimensional arrays

If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.
In particular, a PyTorch formatted dataset outputs nested lists instead of a single tensor:

```py
>>> from datasets import Dataset
>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]
>>> ds = Dataset.from_dict({""data"": data})
>>> ds = ds.with_format(""torch"")
>>> ds[0]
{'data': [tensor([1, 2]), tensor([3, 4])]}
```

To get a single tensor, you must explicitly use the [`Array`] feature type and specify the shape of your tensors:","What is the default format that datasets return?
",regular python objects,huggingface/datasets/blob/main/docs/source/use_with_pytorch.mdx,5.0,"The context clearly states that by default, datasets return regular python objects such as integers, floats, strings, lists, etc. However, it also mentions how to set the format of the dataset to `pytorch` to get PyTorch tensors instead. Since the question asks for the default format without any conditions or clarifications, we can directly infer that the answer is what the context explicitly states: regular python objects.

",4.0,"This question appears to be very general and doesn't provide much context about what aspect of the default format they are referring to. However, understanding the default data format returned by datasets can be crucial for working with Hugging Face's ecosystem, particularly when using models or fine-tuning them on specific tasks.

",5.0,"This question makes sense on its own without any additional context, as it simply asks about a general property of datasets.
"
"![backend_pipeline](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/disaster-assets/production_pipeline.png)

Later on, we were asked if we could extract what earthquake survivors need from a given tweet. We were given data with multiple labels for multiple needs in a given tweet, and these needs could be shelter, food, or logistics, as it was freezing cold over there. Weâ€™ve started experimenting first with zero-shot experimentations with open-source NLI models on Hugging Face Hub and few-shot experimentations with closed-source generative model endpoints. We have tried [xlm-roberta-large-xnli](https://huggingface.co/joeddav/xlm-roberta-large-xnli) and [convbert-base-turkish-mc4-cased-allnli_tr](https://huggingface.co/emrecan/convbert-base-turkish-mc4-cased-allnli_tr). NLI models were particularly useful as we could directly infer with candidate labels and change the labels as data drift occurs, whereas generative models could have made up labels and cause mismatches when giving responses to the backend. We initially didnâ€™t have labeled data so anything would work.

In the end, we decided to fine-tune our own model as it would take roughly three minutes to fine-tune BERTâ€™s text classification head on a single GPU. We had a labelling effort to develop the dataset to train this model. We logged our experiments in the model cardâ€™s metadata so we could later come up with a leaderboard to keep track of which model should be deployed to production. For base model, we have tried [bert-base-turkish-uncased](https://huggingface.co/loodos/bert-base-turkish-uncased) and [bert-base-turkish-128k-cased](https://huggingface.co/dbmdz/bert-base-turkish-128k-cased) and realized they perform better than [bert-base-turkish-cased](https://huggingface.co/dbmdz/bert-base-turkish-cased). You can find our leaderboard [here](https://huggingface.co/spaces/deprem-ml/intent-leaderboard).","How long does it take to fine-tune BERT's text classification head on a single GPU?
",Three minutes.,huggingface/blog/blob/main/using-ml-for-disasters.md,5.0,"The context clearly states that it takes roughly three minutes to fine-tune BERT's text classification head on a single GPU, making the question easily answerable. However, there is no specific information about the computational resources or the dataset used for fine-tuning in this particular case.

",4.0,"This question seems relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as fine-tuning pre-trained models like BERT is a common task in NLP development. However, the answer depends heavily on various factors such as model size, dataset size, batch size, and specific hardware configuration, making it difficult to provide a universally applicable response.

",5.0,"This question makes sense by itself, as it refers to a specific model (BERT) and task (fine-tuning its text classification head), and is asking for a general piece of information that can be understood without any additional context. The mention of a single GPU does not imply any specific context or setting.
"
"ğŸ¤— Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.

It currently contains:

- **implementations of dozens of popular metrics**: the existing metrics cover a variety of tasks spanning from NLP to Computer Vision, and include dataset-specific metrics for datasets. With a simple command like `accuracy = load(""accuracy"")`, get any of these metrics ready to use for evaluating a ML model in any framework (Numpy/Pandas/PyTorch/TensorFlow/JAX).
- **comparisons and measurements**: comparisons are used to measure the difference between models and measurements are tools to evaluate datasets.
- **an easy way of adding new evaluation modules to the ğŸ¤— Hub**: you can create new evaluation modules and push them to a dedicated Space in the ğŸ¤— Hub with `evaluate-cli create [metric name]`, which allows you to see easily compare different metrics and their outputs for the same sets of references and predictions.

[ğŸ“ **Documentation**](https://huggingface.co/docs/evaluate/)

ğŸ” **Find a [metric](https://huggingface.co/evaluate-metric), [comparison](https://huggingface.co/evaluate-comparison), [measurement](https://huggingface.co/evaluate-measurement) on the Hub**

[ğŸŒŸ **Add a new evaluation module**](https://huggingface.co/docs/evaluate/)

ğŸ¤— Evaluate also has lots of useful features like:

- **Type checking**: the input types are checked to make sure that you are using the right input formats for each metric
- **Metric cards**: each metrics comes with a card that describes the values, limitations and their ranges, as well as providing examples of their usage and usefulness.
- **Community metrics:** Metrics live on the Hugging Face Hub and you can easily add your own metrics for your project or to collaborate with others.


# Installation

## With pip

ğŸ¤— Evaluate can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)

```bash
pip install evaluate
```

# Usage","How can I create new evaluation modules using the ğŸ¤— Evaluate library?
",You can create new evaluation modules and push them to a dedicated Space in the ğŸ¤— Hub with `evaluate-cli create [metric name]`.,huggingface/evaluate/blob/main/README.md,4.0,"The context provided is very informative about the capabilities of ğŸ¤— Evaluate, including its features such as type checking, metric cards, and community metrics. However, when specifically asking how to create new evaluation modules using the library, there is a direct link in the context to add a new evaluation module, which indicates that the process is straightforward but might require some technical expertise.

",4.0,"This question appears to be relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it pertains to evaluating model performance. The request for creating new evaluation modules suggests that the user is looking to extend or customize the existing functionality of the Evaluate library. However, without more context, it's unclear whether this question would lead to a detailed explanation of implementing custom metrics, integrating with specific models, or another aspect of NLP development.

",5.0,"The question clearly refers to the Hugging Face Evaluate library, but it does not rely on any additional context or specific information about the library's usage. It's a straightforward inquiry that should be understandable without prior knowledge of a particular project or setting.
"
"</Tip>

Since ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.


## Usage example

ByT5 works on raw UTF-8 bytes, so it can be used without a tokenizer:

```python
>>> from transformers import T5ForConditionalGeneration
>>> import torch

>>> model = T5ForConditionalGeneration.from_pretrained(""google/byt5-small"")

>>> num_special_tokens = 3
>>> # Model has 3 special tokens which take up the input ids 0,1,2 of ByT5.
>>> # => Need to shift utf-8 character encodings by 3 before passing ids to model.

>>> input_ids = torch.tensor([list(""Life is like a box of chocolates."".encode(""utf-8""))]) + num_special_tokens

>>> labels = torch.tensor([list(""La vie est comme une boÃ®te de chocolat."".encode(""utf-8""))]) + num_special_tokens

>>> loss = model(input_ids, labels=labels).loss
>>> loss.item()
2.66
```

For batched inference and training it is however recommended to make use of the tokenizer:

```python
>>> from transformers import T5ForConditionalGeneration, AutoTokenizer

>>> model = T5ForConditionalGeneration.from_pretrained(""google/byt5-small"")
>>> tokenizer = AutoTokenizer.from_pretrained(""google/byt5-small"")

>>> model_inputs = tokenizer(
...     [""Life is like a box of chocolates."", ""Today is Monday.""], padding=""longest"", return_tensors=""pt""
... )
>>> labels_dict = tokenizer(
...     [""La vie est comme une boÃ®te de chocolat."", ""Aujourd'hui c'est lundi.""], padding=""longest"", return_tensors=""pt""
... )
>>> labels = labels_dict.input_ids

>>> loss = model(**model_inputs, labels=labels).loss
>>> loss.item()
17.9
```

Similar to [T5](t5), ByT5 was trained on the span-mask denoising task. However,
since the model works directly on characters, the pretraining task is a bit
different. Let's corrupt some characters of the
input sentence `""The dog chases a ball in the park.""` and ask ByT5 to predict them
for us.","What was the pre-training task used for ByT5 similar to?
",The span-mask denoising task.,huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md,5.0,"The question asks about the pre-training task used for ByT5, which is directly mentioned in the context as similar to T5. The only difference highlighted is that ByT5 works on characters instead of subwords or tokens, but this does not seem to affect the nature of the task, which remains a span-mask denoising task.

",4.0,"This question seems relevant because it implies that the person asking is interested in understanding the pre-training task of ByT5, a model within the Hugging Face ecosystem. The fact that they mention a similarity with another model suggests they are looking for context or analogies to better comprehend how ByT5 was trained.
",4.0,"The question refers to ByT5, a specific model, and asks about its similarity with another pre-training task. It does not rely on external context or information beyond general knowledge of pre-training tasks.
"
"The use and prompting for the conversational use is very similar to using the base models:

```py
>>> import torch
>>> from transformers import IdeficsForVisionText2Text, AutoProcessor

>>> device = ""cuda"" if torch.cuda.is_available() else ""cpu""

>>> checkpoint = ""HuggingFaceM4/idefics-9b-instruct""
>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)
>>> processor = AutoProcessor.from_pretrained(checkpoint)

>>> prompts = [
...     [
...         ""User: What is in this image?"",
...         ""https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG"",
...         ""<end_of_utterance>"",

...         ""\nAssistant: This picture depicts Idefix, the dog of Obelix in Asterix and Obelix. Idefix is running on the ground.<end_of_utterance>"",

...         ""\nUser:"",
...         ""https://static.wikia.nocookie.net/asterix/images/2/25/R22b.gif/revision/latest?cb=20110815073052"",
...         ""And who is that?<end_of_utterance>"",

...         ""\nAssistant:"",
...     ],
... ]

>>> # --batched mode
>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=""pt"").to(device)
>>> # --single sample mode
>>> # inputs = processor(prompts[0], return_tensors=""pt"").to(device)

>>> # Generation args
>>> exit_condition = processor.tokenizer(""<end_of_utterance>"", add_special_tokens=False).input_ids
>>> bad_words_ids = processor.tokenizer([""<image>"", ""<fake_token_around_image>""], add_special_tokens=False).input_ids

>>> generated_ids = model.generate(**inputs, eos_token_id=exit_condition, bad_words_ids=bad_words_ids, max_length=100)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
>>> for i, t in enumerate(generated_text):
...     print(f""{i}:\n{t}\n"")
```","What is the default device to use when torch.cuda.is_available() returns True?
",cuda,huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md,4.0,"The context provided is a code snippet that sets up a model and processing pipeline using the Hugging Face Transformers library. It specifically mentions CUDA availability to determine whether to use a CUDA device or CPU for computations. However, it does not explicitly state what the default device is when `torch.cuda.is_available()` returns True. Given the common practices in deep learning and PyTorch, one can infer that the default action would be to use the CUDA device if available, but this inference requires some background knowledge.

",4.0,"The question is clear and specific about a technical detail of PyTorch's behavior, which is relevant to machine learning developers using the Hugging Face ecosystem.

",4.0,"The question clearly refers to a specific scenario where torch.cuda.is_available() returns True, implying that some context about using PyTorch with CUDA devices is assumed. However, this assumption is still quite general and most developers familiar with PyTorch would understand what's being asked without needing additional information beyond the basics of PyTorch setup and CUDA availability checks.

"
"The abstract from the paper is the following:

*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.*

This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).
The original code can be found [here](https://github.com/zhouhaoyi/Informer2020).

## Resources","What are the three distinctive characteristics of the Informer model designed for Long Sequence Time-Series Forecasting (LSTF)?
","A ProbSparse self-attention mechanism, self-attention distilling with dominating attention, and a generative style decoder.",huggingface/transformers/blob/main/docs/source/en/model_doc/informer.md,5.0,"The context provides a clear description of the Informer model's three distinctive characteristics, which are listed point by point in the abstract. These characteristics include (1) the ProbSparse self-attention mechanism, (2) the self-attention distilling highlighting dominating attention, and (3) the generative style decoder. The context also mentions that these characteristics were designed to address specific issues with traditional Transformer models, such as time complexity and memory usage.

",5.0,"This question appears to be specific, well-defined, and relevant to machine learning developers working with time-series forecasting using the Hugging Face ecosystem. The Informer model is a notable architecture in this domain, and understanding its key characteristics can help developers assess whether it's suitable for their projects.

",5.0,"The question makes sense on its own, as it refers to a specific model architecture (Informer) and task (Long Sequence Time-Series Forecasting), but does not require any additional context or information about the specific implementation or application of the model. It is clear that the question is asking for general characteristics of the Informer model.

"
"|      |                                                                            |[TheBloke/tulu-13B-GPTQ](https://huggingface.co/TheBloke/tulu-13B-GPTQ)|25          |7                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/tulu-13B-GPTQ/blob/main/LICENSE.txt)                 |                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/NewHope-GPTQ](https://huggingface.co/TheBloke/NewHope-GPTQ)|24          |23                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/NewHope-GPTQ/blob/main/LICENSE.txt)                  |                                                                                                                     |                                                                                   |","What is the name of the license for the model tulu-13B-GPTQ?
",llama-2-community-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides a clear link to the LICENSE.txt file associated with the model ""tulu-13B-GPTQ"", which suggests that the license information is publicly available. Additionally, the text mentions the ""llama-2-community-license"" as the type of license used for both this model and another one. This implies that the context has sufficient information to answer the question about the name of the license.

",4.0,"This question appears to be a straightforward query about the licensing terms associated with a specific model, which can be an important piece of information for machine learning developers working with the Hugging Face ecosystem. The answer would likely involve checking the official documentation or resources provided by the creators of the tulu-13B-GPTQ model.

",4.0,"The question appears to be self-contained, assuming that ""tulu-13B-GPTQ"" refers to a specific pre-trained language model. However, it lacks context about where or how this model is being discussed, such as within an API documentation, research paper, or project repository.

"
"### supervised learning

A form of model training that directly uses labeled data to correct and instruct model performance. Data is fed into the model being trained, and its predictions are compared to the known labels. The model updates its weights based on how incorrect its predictions were, and the process is repeated to optimize model performance.

## T

### Tensor Parallelism (TP)

Parallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of
having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets
processed separately and in parallel on different GPUs and the results are synced at the end of the processing step.
This is what is sometimes called horizontal parallelism, as the splitting happens on horizontal level.
Learn more about Tensor Parallelism [here](perf_train_gpu_many#tensor-parallelism).

### token

A part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a
punctuation symbol.

### token Type IDs

Some models' purpose is to do classification on pairs of sentences or question answering.

<Youtube id=""0u3ioSwev3s""/>

These require two different sequences to be joined in a single ""input_ids"" entry, which usually is performed with the
help of special tokens, such as the classifier (`[CLS]`) and separator (`[SEP]`) tokens. For example, the BERT model
builds its two sequence input as such:

```python
>>> # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]
```

We can use our tokenizer to automatically generate such a sentence by passing the two sequences to `tokenizer` as two
arguments (and not a list, like before) like this:

```python
>>> from transformers import BertTokenizer

>>> tokenizer = BertTokenizer.from_pretrained(""bert-base-cased"")
>>> sequence_a = ""HuggingFace is based in NYC""
>>> sequence_b = ""Where is HuggingFace based?""","What type of learning uses labeled data to correct and instruct model performance?
",Supervised learning.,huggingface/transformers/blob/main/docs/source/en/glossary.md,5.0,"The context clearly describes supervised learning as a form of model training that uses labeled data to correct and instruct model performance, which directly answers the question.

",5.0,"This question appears to be related to supervised learning, where models are trained on labeled data to improve their performance. The mention of ""correcting and instructing model performance"" suggests that the question is inquiring about a type of machine learning where the goal is to learn from labeled examples and adjust the model's behavior accordingly.

",5.0,"This question is clear and unambiguous, and it requires no additional context to understand what is being asked. The phrase ""labeled data"" explicitly indicates that the question is referring to supervised learning, where the model receives feedback on its predictions from labeled examples.

"
"- `dataset`: the dataset name, for example `glue` or `mozilla-foundation/common_voice_10_0`
- `config`: the configuration name, for example `cola`
- `split`: the split name, for example `train`

<inferencesnippet>
<python>
```python
import requests
headers = {""Authorization"": f""Bearer {API_TOKEN}""}
API_URL = ""https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train""
def query():
    response = requests.get(API_URL, headers=headers)
    return response.json()
data = query()
```
</python>
<js>
```js
import fetch from ""node-fetch"";
async function query(data) {
    const response = await fetch(
        ""https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train"",
        {
            headers: { Authorization: `Bearer ${API_TOKEN}` },
            method: ""GET""
        }
    );
    const result = await response.json();
    return result;
}
query().then((response) => {
    console.log(JSON.stringify(response));
});
```
</js>
<curl>
```curl
curl https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train \
        -X GET \
        -H ""Authorization: Bearer ${API_TOKEN}""
```
</curl>
</inferencesnippet>

The endpoint response is a JSON containing two keys:

- The [`features`](https://huggingface.co/docs/datasets/about_dataset_features) of a dataset, including the column's name and data type.
- The first 100 `rows` of a dataset and the content contained in each column of a specific row.

For example, here are the `features` and the first 100 `rows` of the `duorc`/`SelfRC` train split:","What is the API URL for querying the first rows of the duorc dataset with the SelfRC config on the train split?
",https://datasets-server.huggingface.co/first-rows?dataset=duorc&config=SelfRC&split=train,huggingface/datasets-server/blob/main/docs/source/first_rows.mdx,5.0,"The question asks for the API URL to query the first rows of the duorc dataset with the SelfRC config on the train split, which is directly answered by the context provided. The code snippet explicitly shows the API_URL variable containing the exact URL needed to make this request.
",5.0,"This question is useful because it directly asks for a specific piece of information that can be obtained from the Hugging Face datasets or transformers library. The answer would provide a concrete API URL, which could help developers quickly integrate the duorc dataset into their NLP applications with the SelfRC config and retrieve the required data efficiently.

",5.0,"The question clearly refers to a specific dataset (duorc) and a configuration (SelfRC), which are technical terms that require some background knowledge. However, the question is still self-contained and does not rely on any additional context beyond what can be inferred from the terms used.

"
"```python
import torch
from torch import nn
import evaluate

metric = evaluate.load(""mean_iou"")

def compute_metrics(eval_pred):
  with torch.no_grad():
    logits, labels = eval_pred
    logits_tensor = torch.from_numpy(logits)
    # scale the logits to the size of the label
    logits_tensor = nn.functional.interpolate(
        logits_tensor,
        size=labels.shape[-2:],
        mode=""bilinear"",
        align_corners=False,
    ).argmax(dim=1)

    pred_labels = logits_tensor.detach().cpu().numpy()
    # currently using _compute instead of compute
    # see this issue for more info: https://github.com/huggingface/evaluate/pull/328#issuecomment-1286866576
    metrics = metric._compute(
            predictions=pred_labels,
            references=labels,
            num_labels=len(id2label),
            ignore_index=0,
            reduce_labels=processor.do_reduce_labels,
        )

    # add per category metrics as individual key-value pairs
    per_category_accuracy = metrics.pop(""per_category_accuracy"").tolist()
    per_category_iou = metrics.pop(""per_category_iou"").tolist()

    metrics.update({f""accuracy_{id2label[i]}"": v for i, v in enumerate(per_category_accuracy)})
    metrics.update({f""iou_{id2label[i]}"": v for i, v in enumerate(per_category_iou)})

    return metrics
```

Finally, we can instantiate a `Trainer` object.


```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
```

Now that our trainer is set up, training is as simple as calling the `train` function. We don't need to worry about managing our GPU(s), the trainer will take care of that.


```python
trainer.train()
```

When we're done with training, we can push our fine-tuned model and the image processor to the Hub.","What mode is used for interpolation when scaling logits in the compute_metrics function?
",bilinear,huggingface/blog/blob/main/fine-tune-segformer.md,5.0,"The context clearly states that when scaling logits in the compute_metrics function, interpolation is performed using nn.functional.interpolate() with mode=""bilinear"". This is explicitly shown in the line of code where logits_tensor is scaled. There is no ambiguity or uncertainty in this answer.
",4.0,"This question seems to be asking about a specific detail of the Hugging Face ecosystem, which suggests that it may be useful for machine learning developers who are working with this technology and need to understand how to scale logits properly. The question is quite specific, so it's likely to be relevant only to those who have encountered this issue or are interested in understanding the inner workings of the compute_metrics function.
",4.0,"The question refers to a specific context or code (the `compute_metrics` function), which suggests that it's not entirely independent of external information. However, the terms ""mode"", ""interpolation"", and ""logits"" are technical and self-explanatory, making it clear what is being asked without needing further explanation.

"
"The next step is to install the libraries that we'll be using in this course. We'll use `pip` for the installation, which is the package manager for Python. In notebooks, you can run system commands by preceding them with the `!` character, so you can install the ğŸ¤— Transformers library as follows:

```
!pip install transformers
```

You can make sure the package was correctly installed by importing it within your Python runtime:

```
import transformers
```

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/install.gif"" alt=""A gif showing the result of the two commands above: installation and import"" width=""80%""/>
</div>

This installs a very light version of ğŸ¤— Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we'll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case:

```
!pip install transformers[sentencepiece]
```

This will take a bit of time, but then you'll be ready to go for the rest of the course!

## Using a Python virtual environment[[using-a-python-virtual-environment]]

If you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started.

Once you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly installed before proceeding to the next steps: `python --version`. This should print out the Python version now available on your system.","How do you install the ğŸ¤— Transformers library in a notebook?
",By running the command `!pip install transformers` followed by an import statement to verify installation.,huggingface/course/blob/main/chapters/en/chapter0/1.mdx,5.0,"The question asks for a very specific and straightforward action, which is installing the ğŸ¤— Transformers library in a notebook. The context provides clear instructions on how to do this using the `!pip install transformers` command, as well as additional information on how to make sure the package was correctly installed by importing it within the Python runtime. Furthermore, the context also mentions an alternative installation method that includes all required dependencies for various use cases.

",4.0,"The question seems straightforward and focused on installation of a specific library, which is a common task for machine learning developers. However, it lacks specificity about the environment (e.g., Colab, Kernels) or additional context that might affect how one installs the library. Nonetheless, the core information needed to guide someone through installing the ğŸ¤— Transformers library in a notebook can be directly answered.

",5.0,"The question can be understood independently of any specific context, as it mentions a well-known library (ğŸ¤— Transformers) and a common use case (installing a library in a notebook). The language used is straightforward, making it easy to understand without additional information.
"
"```python
import evaluate

metric = evaluate.load(""wer"")
```

We then simply have to define a function that takes our model
predictions and returns the WER metric. This function, called
`compute_metrics`, first replaces `-100` with the `pad_token_id`
in the `label_ids` (undoing the step we applied in the
data collator to ignore padded tokens correctly in the loss).
It then decodes the predicted and label ids to strings. Finally,
it computes the WER between the predictions and reference labels:

```python
def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with the pad_token_id
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # we do not want to group tokens when computing the metrics
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)

    return {""wer"": wer}
```

### Load a Pre-Trained Checkpoint

Now let's load the pre-trained Whisper `small` checkpoint. Again, this
is trivial through use of ğŸ¤— Transformers!

```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(""openai/whisper-small"")
```","What is the name of the metric used to compute the WER?
",wer,huggingface/blog/blob/main/fine-tune-whisper.md,5.0,"The context clearly shows the usage of a metric called ""wer"" (Word Error Rate) to compute the difference between model predictions and reference labels. It also mentions loading this metric using `evaluate.load(""wer"")` and utilizing it in the `compute_metrics` function.

",4.0,"This question is somewhat specific but relevant to NLP developers working with the Hugging Face ecosystem, particularly those using the Transformers library for speech recognition tasks. The answer would likely be ""Word Error Rate"" (WER), which is a standard metric in speech recognition evaluation.

",4.0,"The question asks for the specific metric used to calculate Word Error Rate (WER), which implies some context about the task or problem being addressed, but does not require additional information from a specific setting or document.
"
"## Support for LoRA

Fine-tuning diffusion models is expensive and out of reach for most consumer GPUs. We added the Low-Rank Adaptation ([LoRA](https://huggingface.co/papers/2106.09685)) technique to close this gap. With LoRA, which is a method for parameter-efficient fine-tuning, you can fine-tune large diffusion models faster and consume less memory. The resulting model weights are also very lightweight compared to the original model, so you can easily share your custom models. If you want to learn more, [our documentation](https://huggingface.co/docs/diffusers/main/en/training/lora) shows how to perform fine-tuning and inference on Stable Diffusion with LoRA.

In addition to LoRA, we support other [training techniques](https://huggingface.co/docs/diffusers/main/en/training/overview) for personalized generation, including DreamBooth, textual inversion, custom diffusion, and more!

## Torch 2.0 optimizations

PyTorch 2.0 [introduced support](https://pytorch.org/get-started/pytorch-2.0/#pytorch-2x-faster-more-pythonic-and-as-dynamic-as-ever) for `torch.compile()`and `scaled_dot_product_attention()`, a more efficient implementation of the attention mechanism. ğŸ¤—Â Diffusers [provides first-class support](https://huggingface.co/docs/diffusers/optimization/torch2.0) for these features resulting in massive speedups in inference latency, which can sometimes be more than twice as fast!

In addition to visual content (images, videos, 3D assets, etc.), we also added support for audio! Check out [the documentation](https://huggingface.co/docs/diffusers/using-diffusers/audio) to learn more.

## Community highlights

One of the most gratifying experiences of the past year has been seeing how the community is incorporating ğŸ¤— Diffusers into their projects. From adapting Low-rank adaptation (LoRA) for faster training of text-to-image models to building a state-of-the-art inpainting tool, here are a few of our favorite projects:","What is LoRA?
",A method for parameter-efficient fine-tuning.,huggingface/blog/blob/main/diffusers-turns-1.md,5.0,"The context clearly explains what LoRA is and how it works, mentioning that it's a method for parameter-efficient fine-tuning. It also provides additional information about LoRA being used in conjunction with other training techniques, but this is not necessary to answer the question.

",4.0,"LoRA stands for Low-Rank Adaptation, which is a technique used to adapt pre-trained language models such as those provided by the Hugging Face Transformers library to new tasks or datasets with minimal additional training data. It works by adding low-rank linear layers on top of the frozen weights of the pre-trained model, allowing for fine-tuning while preserving the original knowledge learned from the large-scale pre-training dataset.
",5.0,"The question ""What is LoRA?"" is a clear inquiry about the meaning or definition of the term LoRA, without any context that would imply additional information is needed to understand it. It is concise and directly asks for the explanation of a specific concept.
"
"!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# XLNet

<div class=""flex flex-wrap space-x-1"">
<a href=""https://huggingface.co/models?filter=xlnet"">
<img alt=""Models"" src=""https://img.shields.io/badge/All_model_pages-xlnet-blueviolet"">
</a>
<a href=""https://huggingface.co/spaces/docs-demos/xlnet-base-cased"">
<img alt=""Spaces"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"">
</a>
</div>

## Overview

The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
Quoc V. Le. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn
bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization
order.

The abstract from the paper is the following:","Who are the authors of the XLNet paper?
","Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",huggingface/transformers/blob/main/docs/source/en/model_doc/xlnet.md,4.0,"The context provides a brief overview of the XLNet model, including its authors and a link to the original research paper. However, the mention of ""the authors"" in general does not specify which exact individuals are being referred to. Nevertheless, upon clicking on the provided link to the paper's abstract, we can find the specific names mentioned as authors.
",4.0,"The question asks about the authors of a specific paper, which is relevant to NLP researchers working with Hugging Face models. Knowing the authors can provide context and help establish credibility within the field.
",5.0,"The question does not require any additional information to be understood, as it is a straightforward query about the authors of a specific research paper. The mention of ""XLNet"" clearly refers to the paper's title and provides sufficient context for understanding the question.
"
"#### æ›´å¤šå¤æ‚æ€§

è¿™é‡Œæœ‰ä¸€ä¸ªåº”ç”¨ç¨‹åºå¯ä»¥è®©ä½ æ„Ÿå—ä¸€ä¸‹`Blocks`çš„æ›´å¤šå¯èƒ½ï¼š

```python
import numpy as np
import gradio as gr

def flip_text(x):
    return x[::-1]

def flip_image(x):
    return np.fliplr(x)

with gr.Blocks() as demo:
    gr.Markdown(""Flip text or image files using this demo."")
    with gr.Tabs():
        with gr.TabItem(""Flip Text""):
            text_input = gr.Textbox()
            text_output = gr.Textbox()
            text_button = gr.Button(""Flip"")
        with gr.TabItem(""Flip Image""):
            with gr.Row():
                image_input = gr.Image()
                image_output = gr.Image()
            image_button = gr.Button(""Flip"")

    text_button.click(flip_text, inputs=text_input, outputs=text_output)
    image_button.click(flip_image, inputs=image_input, outputs=image_output)

demo.launch()
```

![`blocks_flipper` demo](../../demo/blocks_flipper/screenshot.gif)

è¿˜æœ‰å¾ˆå¤šäº‹æƒ…å¯ä»¥åšï¼æˆ‘ä»¬å°†åœ¨[ä½¿ç”¨blocksæ„å»º](https://gradio.app/building_with_blocks)éƒ¨åˆ†ä¸ºæ‚¨ä»‹ç»å¦‚ä½•åˆ›å»ºåƒè¿™æ ·å¤æ‚çš„ `Blocks` åº”ç”¨ç¨‹åºã€‚

æ­å–œä½ ï¼Œä½ ç°åœ¨å·²ç»ç†Ÿæ‚‰äº†Gradioçš„åŸºç¡€ä½¿ç”¨ï¼ğŸ¥³ å»æˆ‘ä»¬çš„[ä¸‹ä¸€ç« ](https://gradio.app/key_features) äº†è§£Gradioçš„æ›´å¤šåŠŸèƒ½ã€‚

## å¼€æºæ ˆ

Gradioæ˜¯ç”±è®¸å¤šå¾ˆæ£’çš„å¼€æºåº“æ„å»ºçš„ï¼Œè¯·ä¸€å¹¶æ”¯æŒå®ƒä»¬!

[<img src=""../huggingface_mini.svg"" alt=""huggingface"" height=40>](https://huggingface.co)
[<img src=""../python.svg"" alt=""python"" height=40>](https://www.python.org)
[<img src=""../fastapi.svg"" alt=""fastapi"" height=40>](https://fastapi.tiangolo.com)
[<img src=""../encode.svg"" alt=""encode"" height=40>](https://www.encode.io)
[<img src=""../svelte.svg"" alt=""svelte"" height=40>](https://svelte.dev)
[<img src=""../vite.svg"" alt=""vite"" height=40>](https://vitejs.dev)
[<img src=""../pnpm.svg"" alt=""pnpm"" height=40>](https://pnpm.io)
[<img src=""../tailwind.svg"" alt=""tailwind"" height=40>](https://tailwindcss.com)

## åè®®

Gradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.

## å¼•ç”¨

å¦å¤–è¯·å‚é˜…è®ºæ–‡ _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_ï¼Œå¦‚æœæ‚¨åœ¨å·¥ä½œä¸­ä½¿ç”¨Gradioè¯·å¼•ç”¨å®ƒã€‚","What is the license under which Gradio is licensed?
",Apache License 2.0,gradio-app/gradio/blob/main/readme_files/zh-cn/README.md,4.0,"The context provided discusses Gradio's features, usage, and open-source stack, but it does not explicitly mention the license under which Gradio is licensed. However, at the end of the text, it mentions that ""Gradio is licensed under the Apache License 2.0 found in the LICENSE file in the root directory of this repository."" This implies that the question about Gradio's license can be answered unambiguously by looking at the provided context.

",4.0,"This question appears to be directly related to using Gradio, a popular library for building interactive machine learning interfaces, within the Hugging Face ecosystem. Knowing the licensing terms of Gradio can be crucial for developers who plan to integrate or distribute applications built with it. The question's specificity about Gradio makes it relevant and useful for those working on NLP projects that might involve visualizing outputs or models in a more user-friendly way.

",5.0,"The question can be understood by itself without any additional context, as it directly asks about the licensing information of Gradio.
"
"<h4 align=""center"">
    <p>
        <a href=""https://github.com/huggingface/transformers/"">English</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_zh-hans.md"">ç®€ä½“ä¸­æ–‡</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_zh-hant.md"">ç¹é«”ä¸­æ–‡</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_ko.md"">í•œêµ­ì–´</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_es.md"">EspaÃ±ol</a> |
        <a href=""https://github.com/huggingface/transformers/blob/main/README_ja.md"">æ—¥æœ¬èª</a> |
        <b>à¤¹à¤¿à¤¨à¥à¤¦à¥€</b> |
        <a href=""https://github.com/huggingface/transformers//blob/main/README_te.md"">à°¤à±†à°²à±à°—à±</a> |
    </p>
</h4>

<h3 align=""center"">
    <p>Jax, PyTorch à¤”à¤° TensorFlow à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤¨à¥à¤¨à¤¤ à¤®à¤¶à¥€à¤¨ à¤²à¤°à¥à¤¨à¤¿à¤‚à¤—</p>
</h3>

<h3 align=""center"">
    <a href=""https://hf.co/course""><img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png""></a>
</h3>

ğŸ¤— Transformers 100 à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¾à¤  à¤µà¤°à¥à¤—à¥€à¤•à¤°à¤£, à¤¸à¥‚à¤šà¤¨à¤¾ à¤¨à¤¿à¤·à¥à¤•à¤°à¥à¤·à¤£, à¤ªà¥à¤°à¤¶à¥à¤¨ à¤‰à¤¤à¥à¤¤à¤°, à¤¸à¤¾à¤°à¤¾à¤‚à¤¶à¥€à¤•à¤°à¤£, à¤…à¤¨à¥à¤µà¤¾à¤¦, à¤ªà¤¾à¤  à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¹à¤œà¤¾à¤°à¥‹à¤‚ à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤•à¤¾ à¤‰à¤¦à¥à¤¦à¥‡à¤¶à¥à¤¯ à¤¸à¤¬à¤¸à¥‡ à¤‰à¤¨à¥à¤¨à¤¤ à¤à¤¨à¤à¤²à¤ªà¥€ à¤¤à¤•à¤¨à¥€à¤• à¤•à¥‹ à¤¸à¤­à¥€ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¥à¤²à¤­ à¤¬à¤¨à¤¾à¤¨à¤¾ à¤¹à¥ˆà¥¤

ğŸ¤— Transformers à¤¤à¥à¤µà¤°à¤¿à¤¤ à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤”à¤° à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤à¤ªà¥€à¤†à¤ˆ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤†à¤ª à¤•à¤¿à¤¸à¥€ à¤¦à¤¿à¤ à¤—à¤ à¤ªà¤¾à¤  à¤ªà¤° à¤à¤• à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤®à¥‰à¤¡à¤² à¤²à¥‡ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤‡à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤ªà¤° à¤ à¥€à¤• à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤‡à¤¸à¥‡ [à¤®à¥‰à¤¡à¤² à¤¹à¤¬](https://huggingface.co/models) à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤¸à¤®à¥à¤¦à¤¾à¤¯ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¸à¤¾à¤à¤¾ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤‡à¤¸à¥€ à¤¸à¤®à¤¯, à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤ªà¤¾à¤¯à¤¥à¤¨ à¤®à¥‰à¤¡à¥à¤¯à¥‚à¤² à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¸à¥à¤µà¤¤à¤‚à¤¤à¥à¤° à¤¹à¥ˆ, à¤œà¥‹ à¤¸à¤‚à¤¶à¥‹à¤§à¤¨ à¤”à¤° à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤…à¤¨à¥à¤¸à¤‚à¤§à¤¾à¤¨ à¤ªà¥à¤°à¤¯à¥‹à¤—à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¥à¤µà¤¿à¤§à¤¾à¤œà¤¨à¤• à¤¹à¥ˆà¥¤

ğŸ¤— Transformers à¤¤à¥€à¤¨ à¤¸à¤¬à¤¸à¥‡ à¤²à¥‹à¤•à¤ªà¥à¤°à¤¿à¤¯ à¤—à¤¹à¤¨ à¤¶à¤¿à¤•à¥à¤·à¤£ à¤ªà¥à¤¸à¥à¤¤à¤•à¤¾à¤²à¤¯à¥‹à¤‚ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆï¼š [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) â€” à¤”à¤° à¤‡à¤¸à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¨à¤¿à¤°à¥à¤¬à¤¾à¤§ à¤°à¥‚à¤ª à¤¸à¥‡ à¤à¤•à¥€à¤•à¥ƒà¤¤ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤†à¤ª à¤…à¤ªà¤¨à¥‡ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤¸à¥€à¤§à¥‡ à¤à¤• à¤¢à¤¾à¤‚à¤šà¥‡ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤¦à¥‚à¤¸à¤°à¥‡ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤²à¥‹à¤¡ à¤”à¤° à¤…à¤¨à¥à¤®à¤¾à¤¨ à¤²à¤—à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤","à¤•à¥à¤¯à¤¾ Transformers 100 à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ?
",à¤¹à¤¾à¤,huggingface/transformers/blob/main/README_hd.md,5.0,"à¤¯à¤¹ à¤¸à¤µà¤¾à¤² Transformers à¤ªà¤° à¤¹à¥€ à¤•à¥‡à¤‚à¤¦à¥à¤°à¤¿à¤¤ à¤¹à¥ˆ, à¤”à¤° à¤•Ğ¾Ğ½Ñ‚à¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤­à¥€ Transformers à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¹à¥ˆà¥¤ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤¸à¤µà¤¾à¤² à¤ªà¥‚à¤›à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤‡à¤¸ à¤¬à¤¾à¤¤ à¤•à¤¾ à¤‰à¤²à¥à¤²à¥‡à¤– à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ Transformers 100 à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤®à¥‡à¤‚ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤•à¤‚à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤®à¥‡à¤‚ à¤¯à¤¹à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥€ à¤—à¤ˆ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤²à¤¿à¤, à¤¯à¤¹ à¤¸à¤µà¤¾à¤² à¤•à¤¿à¤¸à¥€ à¤­à¥à¤°à¤® à¤¯à¤¾ à¤…à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤à¤¤à¤¾ à¤•à¥‡ à¤¬à¤¿à¤¨à¤¾ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

",4.0,"à¤¯à¤¹ à¤ªà¥à¤°à¤¶à¥à¤¨ Transformer à¤®à¥‰à¤¡à¤² à¤•à¥€ à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤“à¤‚ à¤ªà¤° à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¦à¥‡à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦à¤—à¤¾à¤° à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤–à¤¾à¤¸à¤•à¤° à¤‰à¤¨ à¤¨à¥Œà¤¸à¤¿à¤–à¤¿à¤ à¤¡à¥‡à¤µà¤²à¤ªà¤°à¥à¤¸ à¤•à¥‡ à¤²à¤¿à¤ à¤œà¥‹ Hugging Face Transformers à¤¸à¥‡ à¤ªà¤°à¤¿à¤šà¤¿à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¹ à¤ªà¥à¤°à¤¶à¥à¤¨ Transformer à¤®à¥‰à¤¡à¤² à¤•à¥€ à¤¸à¥€à¤®à¤¾à¤“à¤‚ à¤”à¤° à¤•à¥à¤·à¤®à¤¤à¤¾à¤“à¤‚ à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¸à¤¹à¤¾à¤¯à¤¤à¤¾ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤
",5.0,"à¤¯à¤¹ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤à¤• à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤¸à¤‚à¤ªà¥à¤°à¥‡à¤·à¤£ à¤®à¥‰à¤¡à¤² à¤•à¤¾ à¤‰à¤²à¥à¤²à¥‡à¤– à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¥‹ Transformers à¤•à¤¹à¤²à¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¸à¤®à¤à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤à¤• à¤ªà¤°à¤¿à¤šà¤¿à¤¤ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¯à¤¾ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤¸à¤¾à¤®à¤—à¥à¤°à¥€ à¤¤à¤• à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ à¤”à¤° à¤¯à¤¹ à¤®à¥‰à¤¡à¤² à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤ªà¥‚à¤›à¤¤à¤¾ à¤¹à¥ˆà¥¤
"
"``python
import argparse
import gc
import hashlib
import itertools
import logging
import math
import os
import threading
import warnings
from pathlib import Path
from typing import Optional
import psutil
import json

import torch
import torch.nn.functional as F
import torch.utils.checkpoint
from torch.utils.data import Dataset

import datasets
import diffusers
import transformers
from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import set_seed
from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel
from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline
from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker
from diffusers.optimization import get_scheduler
from diffusers.utils import check_min_version
from diffusers.utils.import_utils import is_xformers_available
from huggingface_hub import HfFolder, Repository, whoami
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm
from transformers import AutoTokenizer, PretrainedConfig, CLIPFeatureExtractor
from peft import PeftModel, LoraConfig, get_peft_model_state_dict, set_peft_model_state_dict

# Will error if the minimal version of diffusers is not installed. Remove at your own risks.
check_min_version(""0.10.0.dev0"")

logger = get_logger(__name__)


MODEL_NAME = ""CompVis/stable-diffusion-v1-4""  # ""stabilityai/stable-diffusion-2-1-base""
INSTANCE_PROMPT = ""a photo of sks dog""
base_path = ""/home/sourab/temp/""
```","What is the minimal version of diffusers required to be installed?
",0.10.0.dev0,huggingface/peft/blob/main/examples/lora_dreambooth/lora_dreambooth_inference.ipynb,5.0,"The context provides a clear indication that the minimal version of diffusers required to be installed is ""0.10.0.dev0"", as it explicitly checks for this version using the `check_min_version` function from the `diffusers.utils` module.

",4.0,"This question appears to be related to the technical requirements for using a specific library (diffusers) within the Hugging Face ecosystem, which is relevant to NLP application development. However, it's quite a narrow and specific query that might not cover broader or more complex aspects of NLP with Hugging Face.

",5.0,"This question can be understood without any additional context as it clearly states what is being asked, i.e., the minimal version of a specific library (diffusers). It does not require knowledge of any specific setting or project to understand that it's inquiring about a requirement for installation.

"
"### Python list of dictionaries

Load a list of Python dictionaries with [`~Dataset.from_list`]:

```py
>>> from datasets import Dataset
>>> my_list = [{""a"": 1}, {""a"": 2}, {""a"": 3}]
>>> dataset = Dataset.from_list(my_list)
```

### Python generator

Create a dataset from a Python generator with [`~Dataset.from_generator`]:

```py
>>> from datasets import Dataset
>>> def my_gen():
...     for i in range(1, 4):
...         yield {""a"": i}
...
>>> dataset = Dataset.from_generator(my_gen)
```

This approach supports loading data larger than available memory.

You can also define a sharded dataset by passing lists to `gen_kwargs`:

```py
>>> def gen(shards):
...     for shard in shards:
...         with open(shard) as f:
...             for line in f:
...                 yield {""line"": line}
...
>>> shards = [f""data{i}.txt"" for i in range(32)]
>>> ds = IterableDataset.from_generator(gen, gen_kwargs={""shards"": shards})
>>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer
>>> from torch.utils.data import DataLoader
>>> dataloader = DataLoader(ds.with_format(""torch""), num_workers=4)  # give each worker a subset of 32/4=8 shards
```

### Pandas DataFrame

Load Pandas DataFrames with [`~Dataset.from_pandas`]:

```py
>>> from datasets import Dataset
>>> import pandas as pd
>>> df = pd.DataFrame({""a"": [1, 2, 3]})
>>> dataset = Dataset.from_pandas(df)
```

<Tip>

For more details, check out the [how to load tabular datasets from Pandas DataFrames](tabular_load#pandas-dataframes) guide.

</Tip>

## Offline

Even if you don't have an internet connection, it is still possible to load a dataset. As long as you've downloaded a dataset from the Hub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline.","How many shards are used in the sharded dataset example?
",32,huggingface/datasets/blob/main/docs/source/loading.mdx,5.0,"The context provided describes how to create datasets in Python using various methods such as loading lists of dictionaries, generators, or Pandas DataFrames. However, when discussing sharded datasets specifically, it mentions that a sharded dataset can be created by passing lists to `gen_kwargs` and defines an example with 32 shards. Since the question asks for the number of shards used in the sharded dataset example, this information is explicitly stated in the context.

",4.0,"This question seems to be relevant to users who want to understand the specifics of Hugging Face's dataset library, specifically regarding the example of a sharded dataset. The answer could provide valuable insight into how to work with large datasets in the Hugging Face ecosystem.
",5.0,"This question is context-independent because it asks about a specific scenario (sharded dataset) without requiring any additional information or context. The phrase ""example"" suggests that the question refers to a particular instance or illustration of how to use sharding, but it does not depend on any external knowledge to be understood.

"
"Metric Card for CUAD

## Metric description

This metric wraps the official scoring script for version 1 of the [Contract Understanding Atticus Dataset (CUAD)](https://huggingface.co/datasets/cuad), which is a corpus of more than 13,000 labels in 510 commercial legal contracts that have been manually labeled to identify 41 categories of important clauses that lawyers look for when reviewing contracts in connection with corporate transactions.

The CUAD metric computes several scores: [Exact Match](https://huggingface.co/metrics/exact_match), [F1 score](https://huggingface.co/metrics/f1), Area Under the Precision-Recall Curve, [Precision](https://huggingface.co/metrics/precision) at 80% [recall](https://huggingface.co/metrics/recall) and Precision at 90% recall.

## How to use

The CUAD metric takes two inputs :


`predictions`, a list of question-answer dictionaries with the following key-values:
- `id`: the id of the question-answer pair as given in the references.
- `prediction_text`: a list of possible texts for the answer, as a list of strings depending on a threshold on the confidence probability of each prediction.


`references`: a list of question-answer dictionaries with the following key-values:
 - `id`: the id of the question-answer pair (the same as above).
 - `answers`: a dictionary *in the CUAD dataset format* with the following keys:
   - `text`: a list of possible texts for the answer, as a list of strings.
   - `answer_start`: a list of start positions for the answer, as a list of ints.

 Note that `answer_start` values are not taken into account to compute the metric.","How many labels does the Contract Understanding Atticus Dataset (CUAD) have?
","13,000.",huggingface/datasets/blob/main/metrics/cuad/README.md,5.0,"The context clearly states that the Contract Understanding Atticus Dataset (CUAD) has more than 13,000 labels in 510 commercial legal contracts, which is specifically mentioned as a ""corpus of more than 13,000 labels"". This unambiguously answers the question about the number of labels.

",5.0,"This question appears to be specific and well-defined, targeting a particular dataset used in NLP applications with the Hugging Face ecosystem. It implies that the respondent has knowledge of the CUAD dataset and its characteristics, which is relevant to machine learning developers working with this dataset.

",5.0,"The question refers to a specific dataset, CUAD, which has been mentioned without any context or background. However, based on general knowledge, it's clear that the question is asking for the number of labels associated with this dataset.

"
"Spaces make hosting and making your application accessible for others to use much more straightforward. You can use Spaces to host [Gradio](gradio.app/) and [Streamlit](https://streamlit.io/) applications, or you can use Spaces to [custom docker images](https://huggingface.co/docs/hub/spaces-sdks-docker). Using Gradio and Spaces in combination often means you can have an application created and hosted with access for others to use within minutes. You can use Spaces to host a Docker image if you want complete control over your application. There are also Docker templates that can give you quick access to a hosted version of many popular tools, including the [Argailla](https://argilla.io/) and [Label Studio](https://labelstud.io/) annotations tools.

## How can you use the Hugging Face Hub: finding relevant models on the Hub

There are many potential use cases in the GLAM sector where machine learning models can be helpful. Whilst some institutions may have the resources required to train machine learning models from scratch, you can use the Hub to find openly shared models that either already do what you want or are very close to your goal.

As an example, if you are working with a collection of digitized Norwegian documents with minimal metadata. One way to better understand what's in the collection is to use a Named Entity Recognition (NER) model. This model extracts entities from a text, for example, identifying the locations mentioned in a text. Knowing which entities are contained in a text can be a valuable way of better understanding what a document is about.","What are some popular tools that can be hosted on Spaces using Docker templates?
",Argailla and Label Studio annotations tools.,huggingface/blog/blob/main/hf-hub-glam-guide.md,4.0,"The context provides information on how to use Spaces to host various applications and tools, including Docker templates, but does not explicitly mention Docker templates for popular tools other than Argailla and Label Studio annotations. However, it mentions that there are Docker templates available for these purposes, implying that such templates exist for other tools as well.

",4.0,"This question appears to be related to Hugging Face's Spaces, which is a platform for deploying machine learning models. The mention of Docker templates suggests that the user is interested in running containers with pre-configured environments for NLP tasks. The question seems like it could lead to useful answers about popular tools and workflows for model deployment on Spaces.

",5.0,"The question refers to a specific platform (Spaces) and technology stack (Docker templates), but it does not require any additional context or information about a particular project, setting, or document. It is clear and self-contained.

"
"Example ignoring punctuation and capitalization, and everything matches:
```python
>>> predictions = [""does this sentence match??"",
...                     ""what about this sentence?""]
>>> references = [[""does this sentence match"", ""does this sentence match!?!""],
...             [""wHaT aBoUt ThIs SeNtEnCe?"", ""wHaT aBoUt ThIs SeNtEnCe?""]]
>>> ter = evaluate.load(""ter"")
>>> results = ter.compute(predictions=predictions,
...                         references=references,
...                         ignore_punct=True,
...                         case_sensitive=False)
>>> print(results)
{'score': 0.0, 'num_edits': 0, 'ref_length': 8.0}
```

Example ignoring punctuation and capitalization, but with an extra (incorrect) sample:
```python
>>> predictions = [""does this sentence match??"",
...                    ""what about this sentence?"",
...                    ""What did the TER metric user say to the developer?""]
>>> references = [[""does this sentence match"", ""does this sentence match!?!""],
...             [""wHaT aBoUt ThIs SeNtEnCe?"", ""wHaT aBoUt ThIs SeNtEnCe?""],
...             [""Your jokes are..."", ""...TERrible""]]
>>> ter = evaluate.load(""ter"")
>>> results = ter.compute(predictions=predictions,
...                         references=references,
...                         ignore_punct=True,
...                         case_sensitive=False)
>>> print(results)
{'score': 100.0, 'num_edits': 10, 'ref_length': 10.0}
```


## Limitations and Bias","What is the default behavior of the TER metric when computing similarities between predictions and references?
",ignore punctuation,huggingface/evaluate/blob/main/metrics/ter/README.md,4.0,"The context provides examples of computing the TER metric with different input parameters (ignore_punct=True, case_sensitive=False), but it does not explicitly state what happens when the metric encounters an extra sample that is not part of any reference. However, based on the provided results, it can be inferred that the default behavior is to include the extra sample in the computation and assign it a high edit distance (10) relative to its own length, resulting in a very low similarity score (100). This suggests that the TER metric treats extra samples as incorrect predictions.

",4.0,"This question appears to be specific and well-defined, targeting a particular aspect of the Hugging Face ecosystem. It likely requires knowledge of the TER (Translation Error Rate) metric and its implementation in the context of NLP tasks with the Hugging Face library. The answer should provide insight into how the metric behaves under default settings, which can be valuable for developers aiming to optimize their models' performance without delving into complex customizations.

",5.0,"The question makes sense without any additional information, as it clearly states what is being asked about (the default behavior of the TER metric) and refers to a specific concept that can be looked up in documentation.
"
"atasets and DataFrames equals love. Although the processing functions of Datasets will cover most the cases needed to train a model, there are times when youâ€™ll need to switch to a library like Pandas to access more powerful features or high-level APIs for visualisation. Fortunately, Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. In this video, we'll take a look at how we can quickly switch our data to Pandas DataFrames and back. As an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download our dataset from the Hub using the load_dataset() function, and you can see that the first element of the training set is an ordinary Python dictionary with various fields of interest. Now suppose that before we train any models, we'd like to explore the data a bit. For example we might be interested in knowing which legal area is most common or we might want to know how the languages are distributed across regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch to Pandas to get our answers! The way this works is by using the set_format() method, which will change the output format of the dataset from Python dictionaries to Pandas DataFrames. As you can see in this example, each row in the dataset is represented as a DataFrame, so we can slice the whole dataset to get a single DataFrame of the dataset. The way this works under the hood is that the Datasets library changes the magic __getitem__() method of the dataset. The __getitem__() method is a special method for Python containers that allows you to specify how indexing works. In this case, the __getitem__() method of the raw dataset starts off by returning Python dictionaries and then after applying set_format() we change __getitem__() to return DataFrames instead","What libraries is Datasets designed to be interoperable with?
","NumPy, PyTorch, TensorFlow, JAX, Pandas.",huggingface/course/blob/main/subtitles/en/raw/chapter5/03b_dataframes.md,5.0,"The context clearly states that Datasets is designed to be interoperable with libraries like Pandas, NumPy, PyTorch, TensorFlow, and JAX. This information directly addresses the question of which libraries Datasets is intended to work with.

",4.0,"This question shows that the person asking it has a basic understanding of the Hugging Face ecosystem, specifically the Datasets library. It also demonstrates an interest in exploring the capabilities and limitations of this library. A well-informed answer would provide information on how Datasets can be used alongside other popular libraries such as Transformers, Tokenizers, or PreTrained. This would help machine learning developers make informed decisions when building NLP applications.
",5.0,"The question makes sense without any additional context, it clearly asks about the compatibility of a specific library (Datasets) with other libraries. The term ""interoperable"" suggests that the answer will be a list of libraries, and the question is asking for that list.
"
"This page contains information on how to fine-prune pre-trained models such as `BERT` to obtain extremely sparse models with movement pruning. In contrast to magnitude pruning which selects weights that are far from 0, movement pruning retains weights that are moving away from 0.

For more information, we invite you to check out [our paper](https://arxiv.org/abs/2005.07683).
You can also have a look at this fun *Explain Like I'm Five* introductory [slide deck](https://www.slideshare.net/VictorSanh/movement-pruning-explain-like-im-five-234205241).

<div align=""center"">
<img src=""https://www.seekpng.com/png/detail/166-1669328_how-to-make-emmental-cheese-at-home-icooker.png"" width=""400"">
</div>

## Extreme sparsity and efficient storage

One promise of extreme pruning is to obtain extremely small models that can be easily sent (and stored) on edge devices. By setting weights to 0., we reduce the amount of information we need to store, and thus decreasing the memory size. We are able to obtain extremely sparse fine-pruned models with movement pruning: ~95% of the dense performance with ~5% of total remaining weights in the BERT encoder.

In [this notebook](https://github.com/huggingface/transformers/blob/main/examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb), we showcase how we can leverage standard tools that exist out-of-the-box to efficiently store an extremely sparse question answering model (only 6% of total remaining weights in the encoder). We are able to reduce the memory size of the encoder **from the 340MB (the original dense BERT) to 11MB**, without any additional training of the model (every operation is performed *post fine-pruning*). It is sufficiently small to store it on a [91' floppy disk](https://en.wikipedia.org/wiki/Floptical) ğŸ“!","What percentage of total remaining weights in the BERT encoder are required to obtain approximately 95% of the dense performance with movement pruning?
",5%,huggingface/transformers/blob/main/examples/research_projects/movement-pruning/README.md,5.0,"The question can be answered with a high degree of certainty, as the context provides a direct quote stating that approximately 95% of the dense performance is achieved with ~5% of total remaining weights in the BERT encoder.

",5.0,"This question is extremely useful because it directly addresses a specific technical detail about the Hugging Face ecosystem, specifically the BERT encoder and its pruning mechanism. The information sought by this question would be valuable for machine learning developers seeking to optimize their NLP applications' performance while reducing memory usage. It's also practical in nature, as it pertains to a concrete outcome (95% of dense performance) and a measurable parameter (percentage of total remaining weights), making the answer potentially actionable.

",4.0,"The question refers to a specific context or setting (the BERT encoder and movement pruning), but it does not require additional information from a particular document or environment. It is still clear what is being asked even without knowing the specifics of the BERT encoder, as long as one understands the general concept of movement pruning.

"
"<PipelineTag pipeline=""text-classification""/>

- [`LayoutLMv2ForSequenceClassification`] is supported by this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb).
- [Text classification task guide](../tasks/sequence_classification)

<PipelineTag pipeline=""token-classification""/>

- [`LayoutLMv3ForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3) and [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb).
- A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb) for how to perform inference with [`LayoutLMv2ForTokenClassification`] and a [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb) for how to perform inference when no labels are available with [`LayoutLMv2ForTokenClassification`].
- A [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb) for how to finetune [`LayoutLMv2ForTokenClassification`] with the ğŸ¤— Trainer.
- [Token classification task guide](../tasks/token_classification)

<PipelineTag pipeline=""question-answering""/>

- [`LayoutLMv2ForQuestionAnswering`] is supported by this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb).
- [Question answering task guide](../tasks/question_answering)","Which notebook is used for fine-tuning LayoutLMv2ForTokenClassification on FUNSD using HuggingFace Trainer?
",Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb,huggingface/transformers/blob/main/docs/source/en/model_doc/layoutlmv3.md,4.0,"The context provides information about the support for specific models and their respective notebooks, but it does not explicitly mention LayoutLMv2ForTokenClassification being used with HuggingFace Trainer on FUNSD. However, there is a notebook that fine-tunes LayoutLMv2ForTokenClassification on FUNSD using HuggingFace Trainer, which suggests that this configuration might be supported.

",4.0,"The question is specific and clear, asking about a particular task (fine-tuning LayoutLMv2ForTokenClassification on FUNSD) within the Hugging Face ecosystem. To answer this question accurately, one would need to have knowledge of the available notebooks in the Hugging Face Transformers library, as well as familiarity with the Trainer API. This type of detailed question is likely to be useful for machine learning developers who are working with NLP tasks and are looking for specific guidance on how to use a particular model or tool.

",5.0,"The question explicitly mentions specific entities (LayoutLMv2ForTokenClassification, FUNSD, and HuggingFace Trainer) that provide sufficient context for understanding the task at hand. However, it doesn't require any external information to be understood; all necessary details are contained within the question itself.

"
"### Step 2 - Test on AI-Platform Prediction

As the model is not a ""pure TensorFlow"" saved model but a checkpoint, and I couldn't turn it into a ""pure TensorFlow model"", I figured out that the example on [this page](https://cloud.google.com/ai-platform/prediction/docs/deploying-models) wouldn't work.
From there I saw that I could write some custom code, allowing me to load the `pipeline` instead of having to handle the model, which seemed is easier. I also learned that I could define a pre-prediction & post-prediction action, which could be useful in the future for pre- or post-processing the data for customers' needs.
I followed Google's guide but encountered an issue as the service is still in beta and everything is not stable. This issue is detailed [here](https://github.com/huggingface/transformers/issues/9926).


### Step 3 - Test on App Engine

I moved to Google's [App Engine](https://cloud.google.com/appengine) as it's a service that I am familiar with, but encountered an installation issue with TensorFlow due to a missing system dependency file. I then tried with PyTorch which worked with an F4_1G instance, but it couldn't handle more than 2 requests on the same instance, which isn't really great performance-wise.

### Step 4 - Test on Cloud Run

Lastly, I moved to [Cloud Run](https://cloud.google.com/run) with a docker image. I followed [this guide](https://cloud.google.com/run/docs/quickstarts/build-and-deploy#python) to get an idea of how it works. In Cloud Run, I could configure a higher memory and more vCPUs to perform the prediction with PyTorch. I ditched Tensorflow as PyTorch seems to load the model faster.


## Implementation of the serverless pipeline","What is the name of the service that the author encountered an installation issue with TensorFlow due to a missing system dependency file?
",App Engine,huggingface/blog/blob/main/how-to-deploy-a-pipeline-to-google-clouds.md,5.0,"The context provides information about various services used by the author for deploying models, including App Engine and Cloud Run. However, it specifically mentions an installation issue with TensorFlow due to a missing system dependency file when using App Engine. This directly answers the question.

",4.0,"This question appears to be relevant and specific, requiring knowledge about common issues that developers may encounter when using the Hugging Face ecosystem. However, it does not seem particularly complex or nuanced, which might limit its usefulness for machine learning developers seeking in-depth guidance on advanced NLP topics.

",5.0,"The question is clear and concise, and it can be understood without any additional information. It explicitly mentions the context (TensorFlow) and the problem encountered, making it easy to infer what service is being referred to.

"
"<gradio-app theme_mode=""light"" space=""simonduerr/ProteinMPNN""></gradio-app>

# Issues

If you encounter any issues with the integration of 3Dmol.js in Gradio/HF spaces, please open a discussion in [hf.space/simonduerr/3dmol.js](https://hf.space/simonduerr/3dmol.js/discussions).

If you have problems with 3Dmol.js configuration - you need to ask the developers, please, open a [3Dmol.js Issue](https://github.com/3dmol/3Dmol.js/issues) instead and describe your problem.","Where should I report issues with 3Dmol.js integration in Gradio/HF spaces?
",hf.space/simonduerr/3dmol.js,huggingface/blog/blob/main/spaces_3dmoljs.md,5.0,"The context explicitly states that if issues with 3Dmol.js integration in Gradio/HF spaces are encountered, one should open a discussion on the specific HF space dedicated to 3Dmol.js. This provides a clear and direct instruction for reporting such issues.
",4.0,"This question appears to be specific to a particular use case of integrating 3Dmol.js with Gradio and Hugging Face spaces, which suggests that it may not be broadly applicable or relevant to the general machine learning community. However, for developers working on NLP applications within this specific ecosystem, having access to support channels for troubleshooting integration issues can be very valuable.
",4.0,"The question requires additional context to understand what ""issues"" refers to, and it assumes familiarity with specific tools (Gradio/HF spaces) without providing any explanation. However, the main point of the question is clear: where to report problems.
"
"### 5. What does ""transfer learning"" mean?

<Question
	choices={[
		{
			text: ""Transferring the knowledge of a pretrained model to a new model by training it on the same dataset."",
			explain: ""No, that would be two versions of the same model.""
		},
		{
			text: ""Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights."",
			explain: ""Correct: when the second model is trained on a new task, it *transfers* the knowledge of the first model."",
			correct: true
		},
		{
			text: ""Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model."",
			explain: ""The architecture is just the way the model is built; there is no knowledge shared or transferred in this case.""
		}
	]}
/>

### 6. True or false? A language model usually does not need labels for its pretraining.

<Question
	choices={[
		{
			text: ""True"",
			explain: ""The pretraining is usually <em>self-supervised</em>, which means the labels are created automatically from the inputs (like predicting the next word or filling in some masked words)."",
			correct: true
		},
		{
			text: ""False"",
			explain: ""This is not the correct answer.""
		}
	]}
/>

### 7. Select the sentence that best describes the terms ""model"", ""architecture"", and ""weights"".","What does it mean to transfer the knowledge of a pretrained model to a new model?
",Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.,huggingface/course/blob/main/chapters/en/chapter1/10.mdx,5.0,"The context clearly explains what transfer learning means, providing a definition and examples to illustrate the concept. It also distinguishes between related but distinct ideas, such as training on the same dataset versus initializing with pre-trained weights.
",5.0,"This question is extremely relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as they often rely on pre-trained models such as BERT and RoBERTa. The concept of transferring knowledge from a pre-trained model to a new model is crucial in fine-tuning these models for specific downstream tasks. The answer would likely involve explaining the process of using a pre-trained model as a starting point for training, adapting it to a new task by adding or modifying layers, and leveraging the features learned by the pre-trained model.
",5.0,"This question is clear and self-contained, requiring no external context or information to understand what ""transfering the knowledge"" implies. It's a general concept that can be explained without referencing specific settings, models, or applications.

"
"## How to enable Hyperparameter search in example

Define the hyperparameter search space, different backends need different format.

For sigopt, see sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter), it's like following:
```py
>>> def sigopt_hp_space(trial):
...     return [
...         {""bounds"": {""min"": 1e-6, ""max"": 1e-4}, ""name"": ""learning_rate"", ""type"": ""double""},
...         {
...             ""categorical_values"": [""16"", ""32"", ""64"", ""128""],
...             ""name"": ""per_device_train_batch_size"",
...             ""type"": ""categorical"",
...         },
...     ]
```

For optuna, see optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py), it's like following:

```py
>>> def optuna_hp_space(trial):
...     return {
...         ""learning_rate"": trial.suggest_float(""learning_rate"", 1e-6, 1e-4, log=True),
...         ""per_device_train_batch_size"": trial.suggest_categorical(""per_device_train_batch_size"", [16, 32, 64, 128]),
...     }
```

Optuna provides multi-objective HPO. You can pass `direction` in `hyperparameter_search` and define your own compute_objective to return multiple objective values. The Pareto Front (`List[BestRun]`) will be returned in hyperparameter_search, you should refer to the test case `TrainerHyperParameterMultiObjectOptunaIntegrationTest` in [test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py). It's like following

```py
>>> best_trials = trainer.hyperparameter_search(
...     direction=[""minimize"", ""maximize""],
...     backend=""optuna"",
...     hp_space=optuna_hp_space,
...     n_trials=20,
...     compute_objective=compute_objective,
... )
```

For raytune, see raytune [object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html), it's like following:","What is the direction parameter in the hyperparameter_search method of Optuna used for?
",It is used to specify whether you want to minimize or maximize certain objectives.,huggingface/transformers/blob/main/docs/source/en/hpo_train.md,5.0,"The context clearly explains the usage of the `direction` parameter in the `hyperparameter_search` method of Optuna. It mentions that you can pass a list of directions, which will allow you to perform multi-objective hyperparameter tuning. Specifically, it states that if multiple objectives are optimized with different directions, the Pareto Front (List[BestRun]) will be returned.
",5.0,"This question appears to be related to the use of Optuna, a library for Bayesian optimization, within the context of Hugging Face's Transformers and other NLP applications. The direction parameter in the hyperparameter_search method is likely used to determine whether the search should maximize or minimize the objective function being optimized. This is a reasonable inquiry given that many machine learning developers use Optuna with Hugging Face models for hyperparameter tuning.

",5.0,"This question makes sense by itself because it specifically asks about a well-defined function (hyperparameter_search method) within a known library (Optuna), and the term ""direction"" is likely referring to one of the possible parameters in the function's signature. The question does not depend on any additional information or context beyond what is provided by the library's documentation.

"
"Model Card components

**Model Card Components** are special elements that you can inject directly into your Model Card markdown to display powerful custom components in your model page. These components are authored by us, feel free to share ideas about new Model Card component in [this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/17).

## The Gallery component

Add the `<Gallery />` component to your text-to-image model card to showcase your images generation.

For example,
```md

<Gallery />

## Model description

TintinIA is fine-tuned version of Stable-Diffusion-xl trained on 125 comics panels from Tintin album.

```


<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-gallery.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/models-gallery-dark.png""/>
</div>

The `<Gallery/>` component will use your Model Card [widget metadata](/docs/hub/models-widgets-examples#text-to-image) to display the images with each associated prompt.

```yaml
widget:
- text: ""drawing of tintin in a shop""
  output:
    url: ""images/shop.png""
- text: ""drawing of tintin watching rugby""
  output:
    url: ""images/rugby.png""
  parameters:
    negative_prompt: ""blurry""
- text: ""tintin working at the office""
  output:
    url: ""images/office.png""
```

> Hint: Support of Card Components through the GUI editor coming soon...","What is the name of the Model Card component that displays images generated by a text-to-image model?
",Gallery,huggingface/hub-docs/blob/main/docs/hub/model-cards-components.md,5.0,"The context provides a clear description of Model Card components and specifically mentions the Gallery component, which is used to display images generated by a text-to-image model. However, it does not explicitly mention what other types of Model Card components exist or what their specific functions are.

",4.0,"This question appears to be specific and targeted towards Hugging Face's Model Cards, which are used to provide additional metadata and information about models in the ecosystem. The fact that it mentions a specific type of image (generated by a text-to-image model) suggests that the question is likely relevant to developers working with this particular domain.
The question seems clear and unambiguous, making it easier for an expert in the field to answer accurately.

",5.0,"The question is clear and asks about a specific feature within the context of a broader framework (Model Cards), but it does not require any additional information beyond general knowledge of what Model Cards are or how they might function. It specifically references a component that is likely well-defined, making the question self-contained.
"
"<div class=""flex justify-center"">
    <img class=""rounded-xl"" src=""https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/robot_cat.png""/>
</div>

## Optimizations

Kandinsky is unique because it requires a prior pipeline to generate the mappings, and a second pipeline to decode the latents into an image. Optimization efforts should be focused on the second pipeline because that is where the bulk of the computation is done. Here are some tips to improve Kandinsky during inference.

1. Enable [xFormers](../optimization/xformers) if you're using PyTorch < 2.0:

```diff
  from diffusers import DiffusionPipeline
  import torch

  pipe = DiffusionPipeline.from_pretrained(""kandinsky-community/kandinsky-2-1"", torch_dtype=torch.float16)
+ pipe.enable_xformers_memory_efficient_attention()
```

2. Enable `torch.compile` if you're using PyTorch >= 2.0 to automatically use scaled dot-product attention (SDPA):

```diff
  pipe.unet.to(memory_format=torch.channels_last)
+ pipe.unet = torch.compile(pipe.unet, mode=""reduce-overhead"", fullgraph=True)
```

This is the same as explicitly setting the attention processor to use [`~models.attention_processor.AttnAddedKVProcessor2_0`]:

```py
from diffusers.models.attention_processor import AttnAddedKVProcessor2_0

pipe.unet.set_attn_processor(AttnAddedKVProcessor2_0())
```

3. Offload the model to the CPU with [`~KandinskyPriorPipeline.enable_model_cpu_offload`] to avoid out-of-memory errors:

```diff
  from diffusers import DiffusionPipeline
  import torch

  pipe = DiffusionPipeline.from_pretrained(""kandinsky-community/kandinsky-2-1"", torch_dtype=torch.float16)
+ pipe.enable_model_cpu_offload()
```

4. By default, the text-to-image pipeline uses the [`DDIMScheduler`] but you can replace it with another scheduler like [`DDPMScheduler`] to see how that affects the tradeoff between inference speed and image quality:

```py
from diffusers import DDPMScheduler
from diffusers import DiffusionPipeline","What is Kandinsky unique about in terms of pipeline requirements?
","It requires a prior pipeline to generate the mappings, and a second pipeline to decode the latents into an image.",huggingface/diffusers/blob/main/docs/source/en/using-diffusers/kandinsky.md,5.0,"The context clearly explains what makes Kandinsky unique in terms of pipeline requirements, specifically mentioning the need for a prior pipeline to generate mappings and a second pipeline to decode latents into an image. It also provides optimization tips for improving performance during inference.

",4.0,"The question is somewhat specific to a particular component (Kandinsky) within the Hugging Face ecosystem, which implies that it's related to a specific aspect or edge case. It requires knowledge of how Kandinsky operates and what sets it apart from other components in terms of pipeline requirements.
",5.0,"This question refers to a specific entity, Kandinsky, which appears to be a model or tool, without providing any context. However, it's clear that the question is asking for information about this entity's characteristics or properties, specifically regarding pipeline requirements.
"
"<Tip warning={true}>

You must keep the `diffusers` folder if you want to keep using the library.

</Tip>

Now you can easily update your clone to the latest version of ğŸ¤— Diffusers with the following command:

```bash
cd ~/diffusers/
git pull
```

Your Python environment will find the `main` version of ğŸ¤— Diffusers on the next run.

## Cache

Model weights and files are downloaded from the Hub to a cache which is usually your home directory. You can change the cache location by specifying the `HF_HOME` or `HUGGINFACE_HUB_CACHE` environment variables or configuring the `cache_dir` parameter in methods like [`~DiffusionPipeline.from_pretrained`].

Cached files allow you to run ğŸ¤— Diffusers offline. To prevent ğŸ¤— Diffusers from connecting to the internet, set the `HF_HUB_OFFLINE` environment variable to `True` and ğŸ¤— Diffusers will only load previously downloaded files in the cache.

```shell
export HF_HUB_OFFLINE=True
```

For more details about managing and cleaning the cache, take a look at the [caching](https://huggingface.co/docs/huggingface_hub/guides/manage-cache) guide.

## Telemetry logging

Our library gathers telemetry information during [`~DiffusionPipeline.from_pretrained`] requests.
The data gathered includes the version of ğŸ¤— Diffusers and PyTorch/Flax, the requested model or pipeline class,
and the path to a pretrained checkpoint if it is hosted on the Hugging Face Hub.
This usage data helps us debug issues and prioritize new features.
Telemetry is only sent when loading models and pipelines from the Hub,
and it is not collected if you're loading local files.

We understand that not everyone wants to share additional information,and we respect your privacy.
You can disable telemetry collection by setting the `DISABLE_TELEMETRY` environment variable from your terminal:

On Linux/MacOS:
```bash
export DISABLE_TELEMETRY=YES
```

On Windows:
```bash
set DISABLE_TELEMETRY=YES
```","What is the command to update a clone of ğŸ¤— Diffusers to the latest version?
",cd ~/diffusers/ && git pull,huggingface/diffusers/blob/main/docs/source/en/installation.md,5.0,"The context provides a clear instruction on how to update a clone of ğŸ¤— Diffusers to the latest version, which is ""cd ~/diffusers/ && git pull"". This command is explicitly mentioned in the provided text and is easy to understand. There are no ambiguous or unclear statements related to updating the clone.

",4.0,"This question appears to be useful because it directly asks about performing an update operation on a specific library (ğŸ¤— Diffusers) within the Hugging Face ecosystem, which is relevant to machine learning developers using this technology. The answer will likely involve providing a straightforward command or instruction that can be easily followed.
",4.0,"The question refers to an external entity (""ğŸ¤— Diffusers""), but it's clear what it's about, and the context can be inferred from the acronym. However, there might be multiple ways to update a clone of a repository, so the question could benefit from specifying whether it's asking for Git commands or other tools.
"
"### Examples

A minimal example of looks as follows:
```Python
qrel = {
    ""query"": [0],
    ""q0"": [""q0""],
    ""docid"": [""doc_1""],
    ""rel"": [2]
}
run = {
    ""query"": [0, 0],
    ""q0"": [""q0"", ""q0""],
    ""docid"": [""doc_2"", ""doc_1""],
    ""rank"": [0, 1],
    ""score"": [1.5, 1.2],
    ""system"": [""test"", ""test""]
}

trec_eval = evaluate.load(""trec_eval"")
results = trec_eval.compute(references=[qrel], predictions=[run])
results[""P@5""]
0.2
```

A more realistic use case with an examples from [`trectools`](https://github.com/joaopalotti/trectools):

```python
qrel = pd.read_csv(""robust03_qrels.txt"", sep=""\s+"", names=[""query"", ""q0"", ""docid"", ""rel""])
qrel[""q0""] = qrel[""q0""].astype(str)
qrel = qrel.to_dict(orient=""list"")

run = pd.read_csv(""input.InexpC2"", sep=""\s+"", names=[""query"", ""q0"", ""docid"", ""rank"", ""score"", ""system""])
run = run.to_dict(orient=""list"")

trec_eval = evaluate.load(""trec_eval"")
result = trec_eval.compute(run=[run], qrel=[qrel])
```

```python
result

{'runid': 'InexpC2',
 'num_ret': 100000,
 'num_rel': 6074,
 'num_rel_ret': 3198,
 'num_q': 100,
 'map': 0.22485930431817494,
 'gm_map': 0.10411523825735523,
 'bpref': 0.217511695914079,
 'Rprec': 0.2502547201167236,
 'recip_rank': 0.6646545943335417,
 'P@5': 0.44,
 'P@10': 0.37,
 'P@15': 0.34600000000000003,
 'P@20': 0.30999999999999994,
 'P@30': 0.2563333333333333,
 'P@100': 0.1428,
 'P@200': 0.09510000000000002,
 'P@500': 0.05242,
 'P@1000': 0.03198,
 'NDCG@5': 0.4101480395089769,
 'NDCG@10': 0.3806761417784469,
 'NDCG@15': 0.37819463408955706,
 'NDCG@20': 0.3686080836061317,
 'NDCG@30': 0.352474353427451,
 'NDCG@100': 0.3778329431025776,
 'NDCG@200': 0.4119129817248979,
 'NDCG@500': 0.4585354576461375,
 'NDCG@1000': 0.49092149290805653}
```

## Limitations and Bias
The `trec_eval` metric requires the inputs to be in the TREC run and qrel formats for predictions and references.


## Citation","What is the recall at rank 5 for the system ""test"" according to trec_eval?
",0.2,huggingface/evaluate/blob/main/metrics/trec_eval/README.md,5.0,"The context clearly explains how to use trec_eval to compute various metrics, including recall at rank. It provides a minimal example that demonstrates how to call the trec_eval.compute function with a reference (qrel) and a prediction (run), and then access the result using results[""P@5""]. This indicates that the question about recall at rank 5 for the system ""test"" can be answered by following these steps. Additionally, the context explains the limitations and bias of trec_eval, which is relevant to understanding how to use it correctly.

",4.0,"This question is useful because it requires knowledge of the TREC evaluation metrics, which are commonly used in NLP applications. The answer will likely involve understanding how to use the `trec_eval` tool and its output, which can be applied to various machine learning development scenarios. However, the question's specificity (focusing on recall at rank 5) might limit its general applicability to broader machine learning development tasks.

",5.0,"The question asks about a specific metric (recall at rank 5) related to a particular system (""test"") and tool (trec_eval), but it does not require any additional context beyond what is mentioned in the question. The terms ""recall"", ""rank 5"", ""system"", and ""trec_eval"" are clear enough that someone familiar with information retrieval evaluation would understand what is being asked.

"
"## Providing a bugfix

Please give a description of the circumstances that lead to the bug. If there is an existing issue, please link to it (e.g. â€œResolves #12345â€).

Ideally, when a bugfix is provided, it should be accompanied by a test for this bug. The test should fail with the current code and pass with the bugfix. Add a comment to the test that references the issue or PR. Without such a test, it is difficult to prevent regressions in the future.

## Adding a new fine-tuning method

New parameter-efficient fine-tuning methods are developed all the time. If you would like to add a new, promising method to PEFT, please follow these steps.

**Requirements**

1. Please add a link to the source (usually a paper) of the method.
2. Some evidence should be provided that there is general interest in using the method. We will not add new methods that are freshly published but without evidence that there is demand for it.
3. Ideally, we want to not only add the implementation of the new method, but also examples (notebooks, scripts), documentation, and an extensive test suite that proves that the method works with a variety of tasks. However, this can be very daunting. Therefore, it is also acceptable to only provide the implementation and at least one working example. Documentation and tests can be added in follow up PRs.

**Steps**

Before you start to implement the new method, please open an issue on GitHub with your proposal. That way, the maintainers can give you some early feedback.

When implementing the method, it makes sense to look for existing implementations that already exist as a guide. Moreover, when you structure your code, please take inspiration from the other PEFT methods. For example, if your method is similar to LoRA, it makes sense to structure your code similarly or even re-use some functions or classes where it makes sense (but donâ€™t overdo it, some code duplication is okay).","What should be included when adding a new fine-tuning method to PEFT?
",A link to the source of the method and evidence of general interest in using it.,huggingface/peft/blob/main/docs/source/developer_guides/contributing.md,5.0,"The context clearly outlines the steps and requirements for adding a new fine-tuning method to PEFT. It specifies what information should be provided (link to source, evidence of interest), what implementation details are expected (structure of code, examples), and even provides guidance on how to approach the task (looking at existing implementations, reusing similar functions or classes). Therefore, it is clear that a new fine-tuning method can be added to PEFT with specific requirements and guidelines provided in the context.

",4.0,"The question shows an understanding of the context, specifically asking about PEFT (Prompt Engineering for Transfer) which is related to the Hugging Face ecosystem. However, it lacks specificity and clarity regarding what exactly should be included when adding a new fine-tuning method, making it somewhat broad and open-ended.

",5.0,"The question appears to be self-contained, referring only to PEFT (Parallel Evolutionary Fine Tuning), which is an open concept. There's no mention of specific context or settings that would require additional information to understand the question.

"
"## TFWav2Vec2ForSequenceClassification

[[autodoc]] TFWav2Vec2ForSequenceClassification
    - call

## TFWav2Vec2ForCTC

[[autodoc]] TFWav2Vec2ForCTC
    - call

</tf>
<jax>

## FlaxWav2Vec2Model

[[autodoc]] FlaxWav2Vec2Model
    - __call__

## FlaxWav2Vec2ForCTC

[[autodoc]] FlaxWav2Vec2ForCTC
    - __call__

## FlaxWav2Vec2ForPreTraining

[[autodoc]] FlaxWav2Vec2ForPreTraining
    - __call__

</jax>
</frameworkcontent>","What is the name of the model class that is a JAX implementation of Wav2Vec2.
",FlaxWav2Vec2Model,huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2.md,5.0,"The question is about the JAX implementation of Wav2Vec2, and the context provides information on different model classes in JAX. However, only one class is mentioned that seems to be related to Wav2Vec2, which is FlaxWav2Vec2Model.

",4.0,"The question is relatively specific and points to a particular aspect of the Hugging Face ecosystem, indicating an interest in exploring JAX-based models for speech processing. This suggests a moderate level of specificity and relevance to NLP tasks.

",5.0,"The question is clear and unambiguous, asking for the specific model class implementing Wav2Vec2 using JAX. It does not rely on any additional context beyond the information provided in the question itself.
"
"While the model is loading, you might notice the following warning:

```bash
Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']
- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some others (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.","What happens when initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture?
",Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification.,huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md,5.0,"The warning explicitly mentions that some weights of the model checkpoint were not used and some others were newly initialized when initializing VideoMAEForVideoClassification from a model trained on another task or with another architecture. This suggests that some pre-trained weights will be discarded, while new ones are randomly initialized for certain layers (in this case, the classifier layer).
",4.0,"This question is likely to be useful as it requires an understanding of how Hugging Face's AutoModel class handles loading pre-trained checkpoints, and specifically what happens when there are inconsistencies between the loaded checkpoint and the initialized model. It may involve understanding the configuration options available for VideoMAEForVideoClassification and how they interact with the loaded checkpoint.

",5.0,"This question assumes basic knowledge of deep learning models, model initialization, and video classification tasks. However, it does not depend on additional information specific to a particular context or document, making it understandable by itself.

"
"To get the top-5 predictions class names:

```py
>>> # Get imagenet class mappings
>>> url, filename = (""https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"", ""imagenet_classes.txt"")
>>> urllib.request.urlretrieve(url, filename)
>>> with open(""imagenet_classes.txt"", ""r"") as f:
...     categories = [s.strip() for s in f.readlines()]

>>> # Print top categories per image
>>> top5_prob, top5_catid = torch.topk(probabilities, 5)
>>> for i in range(top5_prob.size(0)):
...     print(categories[top5_catid[i]], top5_prob[i].item())
>>> # prints class names and probabilities like:
>>> # [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]
```

Replace the model name with the variant you want to use, e.g. `regnety_002`. You can find the IDs in the model summaries at the top of this page.

To extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.

## How do I finetune this model?

You can finetune any of the pre-trained models just by changing the classifier (the last layer).

```py
>>> model = timm.create_model('regnety_002', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)
```
To finetune on your own dataset, you have to write a training loop or adapt [timm's training
script](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.

## How do I train this model?

You can follow the [timm recipe scripts](../scripts) for training a new model afresh.

## Citation

```BibTeX
@misc{radosavovic2020designing,
      title={Designing Network Design Spaces},
      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr DollÃ¡r},
      year={2020},
      eprint={2003.13678},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```","How do I finetune a pre-trained model?
",Just by changing the classifier (the last layer).,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/regnety.mdx,5.0,"The context provides a clear example of how to finetune a pre-trained model by changing the classifier (the last layer) using `timm.create_model`. It also mentions that one has to write a training loop or adapt Timm's training script to use their own dataset. Additionally, it points to Timm recipe scripts for training a new model from scratch.

",5.0,"This question is useful because it's a common task in NLP, and there are many use cases where finetuning a pre-trained model is necessary. The Hugging Face ecosystem provides various tools to make this process easier, such as the `Trainer` class in the Transformers library.

",5.0,"The question is clear and makes sense without any additional context, as it asks about a general process (finetuning) that can be applied to various models.
"
"1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.
1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre DÃ©fossez.
1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.
1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.
1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (from Huawei Noahâ€™s Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.
1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.
1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.","Who is the author of the paper ""mT5: A massively multilingual pre-trained text-to-text transformer""?
","Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.",huggingface/transformers/blob/main/README_zh-hant.md,4.0,"The context provides a list of papers and their corresponding authors, but it does not explicitly mention the author of the paper ""mT5: A massively multilingual pre-trained text-to-text transformer"". However, there is an entry in the list that links to this paper with its authors. Therefore, one can easily find the answer by looking at the provided information.

",4.0,"This question appears to be directly related to the topic of NLP applications with the Hugging Face ecosystem, as the paper mentioned in the question (mT5) is a model built using the Transformers library from Hugging Face. The author of this paper might have some insights or experiences that could be relevant and helpful for machine learning developers working on similar projects.
",5.0,"This question can be understood independently without any additional context, as it simply asks for the author of a specific research paper.
"
"## Load Dataset


```python
from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice[""train""] = load_dataset(dataset_name, language_abbr, split=""train+validation"", use_auth_token=True)
common_voice[""test""] = load_dataset(dataset_name, language_abbr, split=""test"", use_auth_token=True)

print(common_voice)
```


```python
common_voice = common_voice.remove_columns(
    [""accent"", ""age"", ""client_id"", ""down_votes"", ""gender"", ""locale"", ""path"", ""segment"", ""up_votes""]
)

print(common_voice)
```

## Prepare Feature Extractor, Tokenizer and Data


```python
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)
```


```python
from transformers import WhisperTokenizer

tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)
```


```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)
```

### Prepare Data


```python
print(common_voice[""train""][0])
```

Since
our input audio is sampled at 48kHz, we need to _downsample_ it to
16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.

We'll set the audio inputs to the correct sampling rate using dataset's
[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)
method. This operation does not change the audio in-place,
but rather signals to `datasets` to resample audio samples _on the fly_ the
first time that they are loaded:


```python
from datasets import Audio

common_voice = common_voice.cast_column(""audio"", Audio(sampling_rate=16000))
```

Re-loading the first audio sample in the Common Voice dataset will resample
it to the desired sampling rate:


```python
print(common_voice[""train""][0])
```","What is the sampling rate expected by the Whisper model?
",16kHz,huggingface/peft/blob/main/examples/int8_training/peft_bnb_whisper_large_v2_training.ipynb,4.0,"The context provides information about loading a dataset, removing columns, preparing feature extractors and tokenizers, and downsampling audio samples from 48kHz to 16kHz. However, it does not explicitly mention the sampling rate expected by the Whisper model until the last code snippet, where it mentions that 16kHz is the sampling rate expected by the Whisper model.

",4.0,"This question seems to be specific to the Whisper model, which is a speech recognition model part of the Hugging Face Transformers library. The sampling rate for audio data in the context of speech recognition is typically related to the quality and resolution of the input audio signal. Without more information about the Whisper model's requirements or common practices in speech recognition, it's difficult to provide a precise answer. However, this question could be useful if someone was looking into implementing the Whisper model with specific audio data that has varying sampling rates and needs guidance on how to properly configure the model.

",5.0,"The question refers to a specific model (Whisper) and asks for a specific parameter (sampling rate), which can be looked up independently of any additional context.
"
"model_8bit = AutoModelForCausalLM.from_pretrained(""facebook/opt-350m"", load_in_8bit=True, torch_dtype=torch.float32)
model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

Once a model is quantized to 8-bit, you can't push the quantized weights to the Hub unless you're using the latest version of Transformers and bitsandbytes. If you have the latest versions, then you can push the 8-bit model to the Hub with the [`~PreTrainedModel.push_to_hub`] method. The quantization config.json file is pushed first, followed by the quantized model weights.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(""bigscience/bloom-560m"", device_map=""auto"", load_in_8bit=True)
tokenizer = AutoTokenizer.from_pretrained(""bigscience/bloom-560m"")

model.push_to_hub(""bloom-560m-8bit"")
```

</hfoption>
<hfoption id=""4-bit"">

Quantizing a model in 4-bit reduces your memory-usage by 4x, and for large models, set `device_map=""auto""` to efficiently use the GPUs available:

```py
from transformers import AutoModelForCausalLM

model_4bit = AutoModelForCausalLM.from_pretrained(""bigscience/bloom-1b7"", device_map=""auto"", load_in_4bit=True)
```

By default, all the other modules such as `torch.nn.LayerNorm` are converted to `torch.float16`. You can change the data type of these modules with the `torch_dtype` parameter if you want:

```py
import torch
from transformers import AutoModelForCausalLM

model_4bit = AutoModelForCausalLM.from_pretrained(""facebook/opt-350m"", load_in_4bit=True, torch_dtype=torch.float32)
model_4bit.model.decoder.layers[-1].final_layer_norm.weight.dtype
```

Once a model is quantized to 4-bit, you can't push the quantized weights to the Hub.

</hfoption>
</hfoptions>

<Tip warning={true}>

Training with 8-bit and 4-bit weights are only supported for training *extra* parameters.

</Tip>

You can check your memory footprint with the `get_memory_footprint` method:","What is the dtype of model_8bit.model.decoder.layers[-1].final_layer_norm.weight?
",torch.float32,huggingface/transformers/blob/main/docs/source/en/quantization.md,5.0,"The context provides information about quantizing models to 8-bit or 4-bit, but it does not explicitly mention the dtype of specific model weights. However, the provided code snippet shows that when loading a model in 8-bit mode with `torch_dtype=torch.float32`, the weight is indeed of type `float32`. This suggests that when using 8-bit quantization with a custom torch dtype, the weights will be of that custom dtype.

",4.0,"This question seems to be about retrieving a specific attribute from a pre-trained language model using the Hugging Face Transformers library. The question is clear and concise, but it assumes a certain level of familiarity with the library's API and the structure of the models. A machine learning developer building NLP applications with this ecosystem would need to know how to access weights in a transformer decoder layer.

",4.0,"This question asks about a specific attribute (dtype) of a particular object (model_8bit.model.decoder.layers[-1].final_layer_norm.weight) within a model, indicating it relies on prior knowledge or context to be understood.

"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Load adapters

[[open-in-colab]]

There are several [training](../training/overview) techniques for personalizing diffusion models to generate images of a specific subject or images in certain styles. Each of these training methods produces a different type of adapter. Some of the adapters generate an entirely new model, while other adapters only modify a smaller set of embeddings or weights. This means the loading process for each adapter is also different.

This guide will show you how to load DreamBooth, textual inversion, and LoRA weights.

<Tip>

Feel free to browse the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer), [LoRA the Explorer](https://huggingface.co/spaces/multimodalart/LoraTheExplorer), and the [Diffusers Models Gallery](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery) for checkpoints and embeddings to use.

</Tip>

## DreamBooth

[DreamBooth](https://dreambooth.github.io/) finetunes an *entire diffusion model* on just several images of a subject to generate images of that subject in new styles and settings. This method works by using a special word in the prompt that the model learns to associate with the subject image. Of all the training methods, DreamBooth produces the largest file size (usually a few GBs) because it is a full checkpoint model.","What type of adapter generates an entirely new model?
",DreamBooth,huggingface/diffusers/blob/main/docs/source/en/using-diffusers/loading_adapters.md,5.0,"The context describes several adapters for personalizing diffusion models, including DreamBooth, which ""finetunes an entire diffusion model"" and thus generates a new model. This information directly addresses the question of what type of adapter generates an entirely new model.
",4.0,"The question appears to be asking about adapters used in the Hugging Face ecosystem, specifically seeking information on a particular type of adapter that generates a completely new model. This suggests the respondent has some familiarity with NLP and the Hugging Face tools but needs clarification or details about specific types of adapters.
",5.0,"This question can be understood without any additional context or background information. The term ""adapter"" might require knowledge of specific domains, but the concept of generating a new model from it is clear and unambiguous.
"
"Some Spaces might have environment variables that you may need to set up. In these cases, the duplicate workflow will auto-populate the public Variables from the source Space, and give you a warning about setting up the Secrets. The duplicated Space will use a free CPU hardware by default, but you can later upgrade if needed.

## Networking

If your Space needs to make any network requests, you can make requests through the standard HTTP and HTTPS ports (80 and 443) along with port 8080. Any requests going to other ports will be blocked.

## Lifecycle management

On free hardware, your Space will ""go to sleep"" and stop executing after a period of time if unused. If you wish for your Space to run indefinitely, consider [upgrading to a paid hardware](./spaces-gpus). You can also manually pause your Space from the **Settings** tab. A paused Space stops executing until manually restarted by its owner.
Paused time is not billed.

## Helper environment variables

In some cases, you might be interested in having programmatic access to the Space author or repository name. This feature is particularly useful when you expect users to duplicate your Space. To help with this, Spaces exposes different environment variables at runtime. Given a Space [`osanseviero/i-like-flan`](https://huggingface.co/spaces/osanseviero/i-like-flan):

* `CPU_CORES`: 4
* `MEMORY`: 15Gi
* `SPACE_AUTHOR_NAME`: osanseviero
* `SPACE_REPO_NAME`: i-like-flan
* `SPACE_TITLE`: I Like Flan (specified in the README file)
* `SPACE_ID`: `osanseviero/i-like-flan`
* `SPACE_HOST`: `osanseviero-i-like-flan.hf.space`

In case [OAuth](./spaces-oauth) is enabled for your Space, the following variables will also be available:","What are the default HTTP and HTTPS ports that can be used to make network requests in a Space?
",80 and 443.,huggingface/hub-docs/blob/main/docs/hub/spaces-overview.md,5.0,"The context provides information about network requests in a Space, specifically mentioning that standard HTTP and HTTPS ports (80 and 443) along with port 8080 can be used. This directly answers the question of which default ports are allowed for making network requests.

",4.0,"The question seems to be about the technical details of making network requests within the Hugging Face Spaces platform, which is not directly related to NLP or machine learning. However, it's still relevant for developers working with Hugging Face ecosystems who might need to make API calls or interact with external services.

",5.0,"The question does not refer to any specific setting or context, but rather asks for general information about port numbers commonly used for network requests. It assumes that the respondent is familiar with standard networking protocols.

"
"|Opened|[HUB_PR](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1/discussions/9)                                                                            |[sambanovasystems/BLOOMChat-176B-v1](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1)|10075       |352                      |              bloomchat-v1.0                                                                               |                                                                                              |[LICENSE](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1/blob/main/LICENSE)             |                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/Llama-2-7B-Chat-GGML](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)|7835        |560                      |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/LICENSE)                  |                                                                                                                     |                                                                                   |","What is the name of the license used by sambanovasystems/BLOOMChat-176B-v1?
",bloomchat-v1.0,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provided is a table that lists various Hugging Face models, including sambanovasystems/BLOOMChat-176B-v1. One of the columns in the table shows the license used by each model. Specifically, for sambanovasystems/BLOOMChat-176B-v1, the license is listed as ""bloomchat-v1.0"". There is also a link to the LICENSE file associated with this model, which likely contains more information about the terms of use.

",4.0,"This question appears to be related to a specific dataset or model in the Hugging Face ecosystem, and asking about its license is an important aspect of understanding how to use it. A machine learning developer might find this information useful if they are considering using this dataset for their project.

",5.0,"This question can be understood without any additional context. It's a straightforward query asking for information about a specific GitHub repository, and it provides enough details (the repository name) to identify the entity in question.
"
"--
title: seqeval
emoji: ğŸ¤—
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  seqeval is a Python framework for sequence labeling evaluation.
  seqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labeling and so on.

  This is well-tested by using the Perl script conlleval, which can be used for
  measuring the performance of a system that has processed the CoNLL-2000 shared task data.

  seqeval supports following formats:
  IOB1
  IOB2
  IOE1
  IOE2
  IOBES

  See the [README.md] file at https://github.com/chakki-works/seqeval for more information.
---

# Metric Card for seqeval

## Metric description

seqeval is a Python framework for sequence labeling evaluation. seqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labeling and so on.


## How to use

Seqeval produces labelling scores along with its sufficient statistics from a source against one or more references.

It takes two mandatory arguments:

`predictions`: a list of lists of predicted labels, i.e. estimated targets as returned by a tagger.

`references`: a list of lists of reference labels, i.e. the ground truth/target values.

It can also take several optional arguments:

`suffix` (boolean): `True` if the IOB tag is a suffix (after type) instead of a prefix (before type), `False` otherwise. The default value is `False`, i.e. the IOB tag is a prefix (before type).

`scheme`: the target tagging scheme, which can be one of [`IOB1`, `IOB2`, `IOE1`, `IOE2`, `IOBES`, `BILOU`]. The default value is `None`.

`mode`: whether to count correct entity labels with incorrect I/B tags as true positives or not. If you want to only count exact matches, pass `mode=""strict""` and a specific `scheme` value. The default is `None`.","What are the supported tagging schemes in seqeval?
","IOB1, IOB2, IOE1, IOE2, IOBES, BILOU",huggingface/evaluate/blob/main/metrics/seqeval/README.md,4.0,"The context provides information about the supported tagging schemes in seqeval, specifically listing IOB1, IOB2, IOE1, IOE2, IOBES as formats that are supported by seqeval. However, it does not explicitly state what these formats mean or how they differ from each other, which might cause some ambiguity for a user who is not familiar with tagging schemes.

",5.0,"The question is useful because it directly asks about a specific feature of the SeqEval library, which is part of the Hugging Face ecosystem. This suggests that the person asking the question has already chosen to use this library and wants to understand its capabilities, making it relevant to developers working with NLP applications in this context.

",5.0,"The question explicitly mentions the tool or library being used (seqeval), but does not refer to any specific context, data, or model. It is a clear and direct inquiry about the capabilities of the mentioned tool.
"
"## Overview

BLOOM's architecture is very similar to [GPT3](https://en.wikipedia.org/wiki/GPT-3) with a few added improvements as will be discussed later in this article.

The model was trained on [Jean Zay](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html), the French government-funded super computer that is managed by GENCI and installed at [IDRIS](http://www.idris.fr/), the national computing center for the French National Center for Scientific Research (CNRS). The compute was generously donated to the project by GENCI (grant 2021-A0101012475).

The following hardware was used during the training:

- GPUs: 384 NVIDIA A100 80GB GPUs (48 nodes) + 32 spare gpus
- 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links
- CPU: AMD EPYC 7543 32-Core Processor
- CPU memory: 512GB per node
- GPU memory: 640GB per node
- Inter-node connect: Omni-Path Architecture (OPA) w/ non-blocking fat tree
- NCCL-communications network: a fully dedicated subnet
- Disc IO network: GPFS shared with other nodes and users

Checkpoints:

- [main checkpoints](https://huggingface.co/bigscience/bloom)
- each checkpoint with fp32 optim states and bf16+fp32 weights is 2.3TB - just the bf16 weights are 329GB.

Datasets:

- 46 Languages in 1.5TB of deduplicated massively cleaned up text, converted into 350B unique tokens
- Vocabulary size of the model is 250,680 tokens
- For full details please see [The BigScience Corpus A 1.6TB Composite Multilingual Dataset](https://openreview.net/forum?id=UoEw6KigkUn)

The training of the 176B BLOOM model occurred over Mar-Jul 2022 and took about 3.5 months to complete (approximately 1M compute hours).

## Megatron-DeepSpeed

The 176B BLOOM model has been trained using [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed), which is a combination of 2 main technologies:","What was the name of the French government-funded supercomputer used for training the BLOOM model?
",Jean Zay,huggingface/blog/blob/main/bloom-megatron-deepspeed.md,5.0,"The question can be answered unambiguously with the context because it directly mentions that the French government-funded supercomputer used for training the BLOOM model is called ""Jean Zay"". This information is explicitly stated in the first paragraph of the provided text.

",4.0,"This question appears to be relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it involves a specific detail about the BLOOM model's training process. The answer is likely related to the infrastructure and resources used for training large language models like BLOOM.
",4.0,"The question refers to a specific event (training the BLOOM model) and assumes knowledge of a particular instance of a French government-funded supercomputer. While it doesn't explicitly mention a context, it implies that there is a specific project or event being referred to.
"
"# Print top categories per image
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
# prints class names and probabilities like:
# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]
```

Replace the model name with the variant you want to use, e.g. `regnetx_002`. You can find the IDs in the model summaries at the top of this page.

To extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.

## How do I finetune this model?
You can finetune any of the pre-trained models just by changing the classifier (the last layer).
```python
model = timm.create_model('regnetx_002', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)
```
To finetune on your own dataset, you have to write a training loop or adapt [timm's training
script](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.

## How do I train this model?

You can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.

## Citation

```BibTeX
@misc{radosavovic2020designing,
      title={Designing Network Design Spaces},
      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr DollÃ¡r},
      year={2020},
      eprint={2003.13678},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```","How many pre-trained models can be finetuned by changing the classifier?
",any of the pre-trained models,huggingface/pytorch-image-models/blob/main/docs/models/regnetx.md,5.0,"The context clearly explains how to finetune a pre-trained model by changing the classifier, and it also mentions that any of the pre-trained models can be finetuned in this way. This suggests that the question is answerable with high certainty.

",4.0,"This question is quite generic and doesn't provide much context about the specific Hugging Face ecosystem tools or techniques, which makes it difficult to give a precise answer. However, it's still related to NLP development with the Hugging Face ecosystem, and understanding how many pre-trained models can be fine-tuned by changing the classifier is an important aspect of model selection and customization in NLP.

",5.0,"The question makes sense without any additional context, as it only refers to a general concept of fine-tuning pre-trained models. It doesn't require knowledge of a specific model or implementation.
"
"### How can I add a new library to the Inference API?

If you're interested in adding your library, please reach out to us! Read about it in [Adding a Library Guide](./models-adding-libraries).","Where should I look for instructions on how to add my library to the Inference API?
",Adding a Library Guide.,huggingface/hub-docs/blob/main/docs/hub/models-libraries.md,5.0,"The context provides clear instructions on how to proceed for someone who wants to add their library to the Inference API. It explicitly mentions reaching out and reading the ""Adding a Library Guide"", which likely contains step-by-step instructions on where to look for further information.

",4.0,"The question is asking about a specific task related to integrating a custom library with the Hugging Face Inference API, which is a crucial step in deploying models. The Hugging Face ecosystem has extensive documentation and resources available for this purpose. Therefore, it's likely that this question can be resolved by looking through the official documentation or community support channels, such as the GitHub repository issues or the forums.
",4.0,"The question assumes prior knowledge of what the Inference API is and that it has some form of user documentation, but the phrase ""Inference API"" itself provides sufficient context. However, the question implies a lack of specific knowledge about where such instructions would be located within this context.
"
"```
sudo apt update && sudo apt upgrade -y
wget https://repo.radeon.com/amdgpu-install/5.4.3/ubuntu/jammy/amdgpu-install_5.4.50403-1_all.deb
sudo apt-get install ./amdgpu-install_5.4.50403-1_all.deb
sudo amdgpu-install --usecase=hiplibsdk,rocm,dkms
sudo amdgpu-install --list-usecase
sudo reboot
```

**1.2 ROCm installation verification**
```
rocm-smi
sudo rocminfo
```
**1.3 Docker image pull and run a Docker container**

The following uses Pytorch2.0 on ROCm5.4.2. Please use the
appropriate docker image according to your target ROCm and Pytorch
version: <https://hub.docker.com/r/rocm/pytorch/tags>
```
docker pull rocm/pytorch:rocm5.4.2_ubuntu20.04_py3.8_pytorch_2.0.0_preview

sudo docker run --device=/dev/kfd --device=/dev/dri --group-add video \
--shm-size=8g --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
--ipc=host -it --name vicuna_test -v ${PWD}:/workspace -e USER=${USER} \
rocm/pytorch:rocm5.4.2_ubuntu20.04_py3.8_pytorch_2.0.0_preview
```
**2 Model** **quantization and Model inference (Inside the docker)**

You can either download quantized Vicuna-13b model from Huggingface or
quantize the floating-point model. Please check out **Appendix - GPTQ
model quantization** if you want to quantize the floating-point model.

**2.1 Download the quantized Vicuna-13b model**

Use download-model.py script from the following git repo.
```
git clone https://github.com/oobabooga/text-generation-webui.git
cd text-generation-webui
python download-model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g
```
2.  **Running the Vicuna 13B GPTQ Model on AMD GPU**
```
git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda
cd GPTQ-for-LLaMa
python setup_cuda.py install
```
These commands will compile and link HIPIFIED CUDA-equivalent kernel
binaries to","What is the name of the Docker image used for running PyTorch 2.0 on ROCm 5.4.2?
",rocm/pytorch:rocm5.4.2_ubuntu20.04_py3.8_pytorch_2.0.0_preview,huggingface/blog/blob/main/chatbot-amd-gpu.md,4.0,"The context provided is a guide for installing ROCm on Ubuntu and running PyTorch 2.0 on ROCm using Docker. It mentions the command to pull and run a Docker container with the specific image, but it does not explicitly mention the name of the Docker image used for running PyTorch 2.0 on ROCm 5.4.2. However, based on the provided command `docker pull rocm/pytorch:rocm5.4.2_ubuntu20.04_py3.8_pytorch_2.0.0_preview`, it can be inferred that the name of the Docker image is ""rocm/pytorch"" with a specific tag.

",4.0,"The provided question is not very specific or detailed, but it still targets a particular technology stack (PyTorch 2.0 and ROCm) which is relevant to the Hugging Face ecosystem that often utilizes deep learning frameworks like PyTorch.

",5.0,"The question does not refer to a specific context or setting, and it only mentions technical terms that are well-defined in the relevant documentation (Docker image, PyTorch, ROCm). This suggests that an operator with access to such documentation could understand what is being asked without additional information.
"
"`--target_feature_extractor_sampling_rate` resamples audio to target feature extractor's sampling rate (16kHz).

`--max_duration_in_seconds=""15""` filters out examples whose audio is longer than the specified limit,
which helps with capping GPU memory usage.


### DeepSpeed Integration

To learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration).

But to get started quickly all you need is to install:
```
pip install deepspeed
```
and then use the default configuration files in this directory:

* `ds_config_wav2vec2_zero2.json`
* `ds_config_wav2vec2_zero3.json`

Here are examples of how you can use DeepSpeed:

(edit the value for `--num_gpus` to match the number of GPUs you have)

ZeRO-2:

```
PYTHONPATH=../../../src deepspeed --num_gpus 2 \
run_asr.py \
--output_dir=output_dir --num_train_epochs=2 --per_device_train_batch_size=2 \
--per_device_eval_batch_size=2 --evaluation_strategy=steps --save_steps=500 --eval_steps=100 \
--logging_steps=5 --learning_rate=5e-4 --warmup_steps=3000 \
--model_name_or_path=patrickvonplaten/wav2vec2_tiny_random_robust \
--dataset_name=hf-internal-testing/librispeech_asr_dummy --dataset_config_name=clean \
--train_split_name=validation --validation_split_name=validation --orthography=timit \
--preprocessing_num_workers=1 --group_by_length --freeze_feature_extractor --verbose_logging \
--deepspeed ds_config_wav2vec2_zero2.json
```

For ZeRO-2 with more than 1 gpu you need to use (which is already in the example configuration file):
```
    ""zero_optimization"": {
        ...
        ""find_unused_parameters"": true,
        ...
    }
```

ZeRO-3:","What is the target sampling rate of the feature extractor?
",16kHz.,huggingface/transformers/blob/main/examples/research_projects/wav2vec2/README.md,5.0,"The context mentions a specific option `--target_feature_extractor_sampling_rate` which is used to resample audio to the target sampling rate of the feature extractor, and it's mentioned that the target sampling rate is 16kHz. There are no other relevant information about sampling rates in the context.
",4.0,"This question seems to be relevant to NLP developers working with the Hugging Face ecosystem, specifically those using their pre-trained models or fine-tuning them for specific tasks. The target sampling rate of a feature extractor could impact model performance and training efficiency.
The question appears to be straightforward, asking for a specific setting within the Hugging Face framework.

",5.0,"The question clearly asks about a specific property (target sampling rate) of an entity (feature extractor), without any implicit context or references. It's clear what the question is asking, even to someone unfamiliar with the topic.

"
"By deleting components you aren't using and flushing the GPU VRAM, you should be able to run [`PixArtAlphaPipeline`] with under 8GB GPU VRAM.

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/pixart/8bits_cat.png)

If you want a report of your memory-usage, run this [script](https://gist.github.com/sayakpaul/3ae0f847001d342af27018a96f467e4e).

<Tip warning={true}>

Text embeddings computed in 8-bit can impact the quality of the generated images because of the information loss in the representation space caused by the reduced precision. It's recommended to compare the outputs with and without 8-bit.

</Tip>

While loading the `text_encoder`, you set `load_in_8bit` to `True`. You could also specify `load_in_4bit` to bring your memory requirements down even further to under 7GB.

## PixArtAlphaPipeline

[[autodoc]] PixArtAlphaPipeline
	- all
	- __call__","What is the recommended approach when comparing outputs from PixArtAlphaPipeline with and without 8-bit text embeddings?
",It's recommended to compare the outputs in both 8-bit and non-8-bit configurations.,huggingface/diffusers/blob/main/docs/source/en/api/pipelines/pixart.md,4.0,"The question is clearly answerable with the given context, as it directly addresses a piece of advice provided in the tip. However, the context only provides a recommendation to compare outputs without explicitly stating how this comparison should be done or what approach is recommended.

",4.0,"This question shows a clear understanding of the specific pipeline being used (PixArtAlphaPipeline) and a desire to optimize its performance by comparing different approaches. The mention of 8-bit text embeddings suggests that the user is aware of some nuances in the pipeline's functionality, which makes their question more informed and relevant.

However, without knowing the full context or the expected outcome, it's difficult to determine whether this approach will be particularly useful for machine learning developers building NLP applications with the Hugging Face ecosystem. The question could benefit from additional details about what specific aspects of performance are being compared (e.g., speed, accuracy) and how they relate to real-world applications.

",5.0,"This question can be understood without additional context because it specifies a particular pipeline (PixArtAlphaPipeline) and mentions a specific feature comparison. The use of technical terms like ""outputs"", ""embeddings"" suggests that the person asking the question is familiar with the context, but the question itself remains clear and unambiguous.

"
"Metrics:
      Top 1 Accuracy: 80.96%
      Top 5 Accuracy: 95.38%
- Name: resnest50d_1s4x24d
  In Collection: ResNeSt
  Metadata:
    FLOPs: 5686764544
    Parameters: 25680000
    File Size: 103045531
    Architecture:
    - 1x1 Convolution
    - Convolution
    - Dense Connections
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Connection
    - Softmax
    - Split Attention
    Tasks:
    - Image Classification
    Training Techniques:
    - AutoAugment
    - DropBlock
    - Label Smoothing
    - Mixup
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    Training Resources: 64x NVIDIA V100 GPUs
    ID: resnest50d_1s4x24d
    LR: 0.1
    Epochs: 270
    Layers: 50
    Dropout: 0.2
    Crop Pct: '0.875'
    Momentum: 0.9
    Batch Size: 8192
    Image Size: '224'
    Weight Decay: 0.0001
    Interpolation: bicubic
  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/resnest.py#L229
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-resnest/resnest50_fast_1s4x24d-d4a4f76f.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 81.0%
      Top 5 Accuracy: 95.33%
- Name: resnest50d_4s2x40d
  In Collection: ResNeSt
  Metadata:
    FLOPs: 5657064720
    Parameters: 30420000
    File Size: 122133282
    Architecture:
    - 1x1 Convolution
    - Convolution
    - Dense Connections
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Connection
    - Softmax
    - Split Attention
    Tasks:
    - Image Classification
    Training Techniques:
    - AutoAugment
    - DropBlock
    - Label Smoothing
    - Mixup
    - SGD with Momentum
    - Weight Decay
    Training Data:
    - ImageNet
    Training Resources: 64x NVIDIA V100 GPUs
    ID: resnest50d_4s2x40d
    LR: 0.1
    Epochs: 270
    Layers: 50
    Dropout: 0.2
    Crop Pct: '0.875'
    Momentum: 0.9","How many layers does the ResNeSt model resnest50d_1s4x24d have?
",50,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/resnest.mdx,5.0,"The context provides detailed metadata about the ResNeSt model, including its architecture and configuration. Specifically, it lists ""Architecture"" as a section with various components, but does not explicitly mention the number of layers in each component. However, further down the context, there is a section titled ""Metadata"" which includes an entry labeled ""Layers: 50"". This suggests that the ResNeSt model has 50 layers.

",4.0,"This question appears to be quite specific, asking about a particular model architecture within the Hugging Face ecosystem. It seems likely that someone would ask this question if they were trying to implement or understand the ResNeSt model with this specific configuration. The answer is also relatively straightforward, making it easy for others to find and use as reference.

",4.0,"This question can be understood on its own because it refers to a specific model (ResNeSt) and a particular version of that model (resnest50d_1s4x24d). The phrase ""How many layers"" is also clear and unambiguous. However, the question does not provide any additional context about what kind of layers are being referred to (e.g., convolutional, recurrent, etc.). Nevertheless, based on general knowledge of deep learning models, it can be inferred that the question is asking for the number of convolutional or neural network layers in the model.

"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Configuration classes for TFLite export

## Base classes

[[autodoc]] exporters.tflite.TFLiteConfig
    - inputs
    - outputs
    - generate_dummy_inputs

## Middle-end classes

[[autodoc]] exporters.tflite.config.TextEncoderTFliteConfig

[[autodoc]] exporters.tflite.config.VisionTFLiteConfig","Under what license is the software distributed?
","Apache License, Version 2.0",huggingface/optimum/blob/main/docs/source/exporters/tflite/package_reference/configuration.mdx,5.0,"The context clearly states that the software is distributed under the Apache License, Version 2.0, and provides a link to the license terms. This information is explicitly mentioned in a section labeled ""Licensed under the Apache License, Version 2.0"".
",4.0,"The question is straightforward and relevant to understanding the terms of use for Hugging Face's ecosystem. Knowing the licensing details can be important for machine learning developers who want to integrate or modify Hugging Face models into their projects, as it affects how they can distribute and share their work.
",5.0,"This question makes sense by itself because it only refers to general concepts of software distribution, without specifying a particular context or application. The question is clear and doesn't require any additional information to be understood.
"
"|      |                                                                            |[valurank/distilroberta-mbfc-bias](https://huggingface.co/valurank/distilroberta-mbfc-bias)|18          |2                        |                                                                                              |                                                                                              |[LICENSE](https://huggingface.co/valurank/distilroberta-mbfc-bias/blob/main/LICENSE)               |                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/llama-2-70b-Guanaco-QLoRA-GGUF](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-GGUF)|15          |3                        |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-GGUF/blob/main/LICENSE.txt)|                                                                                                                     |                                                                                   |","What is the license of the model ""TheBloke/llama-2-70b-Guanaco-QLoRA-GGUF""?
",llama-2-community-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides a direct link to the license of the model ""TheBloke/llama-2-70b-Guanaco-QLoRA-GGUF"", as well as a brief description of it, making it easy to identify and access the license information.

",4.0,"This question appears to be quite specific and relevant to a particular model within the Hugging Face ecosystem. Knowing the license of a model is crucial for users to understand their rights and obligations when using, modifying, or distributing that model. The question suggests the user wants to ensure they are complying with any applicable terms and conditions.
",5.0,"This question can be understood without any prior knowledge of a specific context or additional information, as it directly asks for the licensing details of a particular model. The mention of a model's name and attributes (""TheBloke/llama-2-70b-Guanaco-QLoRA-GGUF"") is sufficient to understand the question.
"
"!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# DePlot

## Overview

DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.

The abstract of the paper states the following:","What is the title of the paper where DePlot was proposed?
",[DePlot: One-shot visual language reasoning by plot-to-table translation],huggingface/transformers/blob/main/docs/source/en/model_doc/deplot.md,4.0,"The context provides a clear reference to the paper where DePlot was proposed, along with its title and authors. However, it does not explicitly state what DePlot is or its purpose. It seems that the author intended to provide additional information about DePlot in a separate section, but only provided a link to an external source (arXiv) for further reading.

",4.0,"This question seems to be relevant to the Hugging Face ecosystem, specifically related to a research paper that proposes a specific library or tool (DePlot). The answer could potentially point to a resource that provides more information on how to apply this tool in NLP applications.
",5.0,"This question can be understood without any additional context or information, as it only refers to a specific concept (DePlot) and asks for a factual piece of information related to it.
"
"--
title: ""Code Llama: Llama 2 learns to code""
thumbnail: /blog/assets/160_codellama/thumbnail.jpg
authors:
- user: philschmid
- user: osanseviero
- user: pcuenq
- user: lewtun
- user: lvwerra
- user: loubnabnl
- user: ArthurZ
- user: joaogante
---

# Code Llama: Llama 2 learns to code


## Introduction

Code Llama is a family of state-of-the-art, open-access versions of [Llama 2](https://huggingface.co/blog/llama2) specialized on code tasks, and weâ€™re excited to release integration in the Hugging Face ecosystem! Code Llama has been released with the same permissive community license as Llama 2 and is available for commercial use.

Today, weâ€™re excited to release:

- Models on the Hub with their model cards and license
- Transformers integration
- Integration with Text Generation Inference for fast and efficient production-ready inference
- Integration with Inference Endpoints
- Integration with VS Code extension
- Code benchmarks

Code LLMs are an exciting development for software engineers because they can boost productivity through code completion in IDEs, take care of repetitive or annoying tasks like writing docstrings, or create unit tests.

## Table of Contents

  - [Introduction](#introduction)
  - [Table of Contents](#table-of-contents)
  - [Whatâ€™s Code Llama?](#whats-code-llama)
  - [How to use Code Llama?](#how-to-use-code-llama)
    - [Demo](#demo)
    - [Transformers](#transformers)
      - [A Note on dtypes](#a-note-on-dtypes)
      - [Code Completion](#code-completion)
      - [Code Infilling](#code-infilling)
      - [Conversational Instructions](#conversational-instructions)
      - [4-bit Loading](#4-bit-loading)
    - [Using text-generation-inference and Inference Endpoints](#using-text-generation-inference-and-inference-endpoints)
    - [Using VS Code extension](#using-vs-code-extension)
  - [Evaluation](#evaluation)
  - [Additional Resources](#additional-resources)

## Whatâ€™s Code Llama?","How is Code Llama licensed?
",It has the same permissive community license as Llama 2 and is available for commercial use.,huggingface/blog/blob/main/codellama.md,4.0,"The context clearly states that Code Llama has been released with a ""permissive community license"" and is available for commercial use, indicating the licensing terms. However, it does not explicitly state the name of the license.

",4.0,"This question is likely to be useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it pertains to one of their competitors. Understanding the licensing terms of a tool like Code Llama can provide insight into how its use and distribution are restricted, which could inform decisions about integrating similar tools in NLP pipelines.
",5.0,"The question explicitly mentions ""Code Llama"" which implies a specific entity or model, but it does not reference any particular context or setup. This suggests that the question can be understood without additional information beyond what's contained in the question itself.
"
"1. **Do a brain dump of everything you know the toolâ€™s capable of**: For Sentence Transformers this includes generating sentence embeddings, comparing sentences, [retrieve and re-rank for complex search tasks](https://www.sbert.net/examples/applications/retrieve_rerank/README.html), clustering, and searching for similar documents with [semantic search](https://www.sbert.net/examples/applications/semantic-search/README.html).
2. **Reflect on some interesting data sources:** Thereâ€™s a huge collection of datasets on the [Hugging Face Hub](https://huggingface.co/datasets), or you can also consult lists like [awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets) for some inspiration. You can often find interesting data in unexpected places â€“ your municipality, for example, may have an [open data portal](https://opendata.vancouver.ca/pages/home/). Youâ€™re going to spend a decent amount of time working with your data, so you may as well pick datasets that excite you!
3. **Pick a *secondary* tool that youâ€™re somewhat comfortable with:** Why limit your experience to learning one tool at a time? [â€œDistributed practiceâ€](https://senecalearning.com/en-GB/blog/top-10-most-effective-learning-strategies/) (a.k.a. â€œspaced repetitionâ€) means spreading your learning across multiple sessions, and itâ€™s been proven to be an effective strategy for learning new material. One way to actively do this is by practicing new skills even in situations where theyâ€™re not the main learning focus. If youâ€™ve recently picked up a new tool, this is a great opportunity to multiply your learning potential by battle-testing your skills. I recommend only including one secondary tool in your Rocket Launch projects.","What are some complex search tasks that Sentence Transformers can be used for?
",Retrieve and re-rank for complex search tasks.,huggingface/blog/blob/main/your-first-ml-project.md,5.0,"The context provides clear information about Sentence Transformers' capabilities, specifically mentioning the ability to retrieve and re-rank for complex search tasks through a specific example on the Hugging Face website. This implies that Sentence Transformers can be used for various complex search tasks.
",4.0,"This question appears to be useful as it explores the capabilities of Sentence Transformers, a popular library within the Hugging Face ecosystem. A detailed response would help developers understand the breadth and depth of applications possible with this technology, potentially leading to more effective adoption and utilization in NLP projects.

",5.0,"This question is clear and self-contained, asking about a specific application of Sentence Transformers. It requires no additional context or information beyond general knowledge of what Sentence Transformers are and what they are capable of.

"
"CodeParrot ğŸ¦œ
<p align=""center"">
    <img src=""https://huggingface.co/datasets/lvwerra/repo-images/raw/main/code-highlighting-streamlit.png"" alt=""drawing"" width=""350""/>
</p>

## What is this about?
This is an open-source effort to train and evaluate code generation models. CodeParrot ğŸ¦œ is a GPT-2 model trained from scratch on Python code. The highlights of this project are:
- initialize and train a GPT-2 language model from scratch for code generation
- train a custom tokenizer adapted for Python code
- clean and deduplicate a large (>100GB) dataset with `datasets`
- train with `accelerate` on multiple GPUs using data parallelism and mixed precision
- continuously push checkpoints to the hub with `huggingface_hub`
- stream the dataset with `datasets` during training to avoid disk bottlenecks
- apply the `code_eval` metric in `datasets` to evaluate on [OpenAI's _HumanEval_ benchmark](https://huggingface.co/datasets/openai_humaneval)
- showcase examples for downstream tasks with code models in [examples](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot/examples) folder:
    - Algorithmic complexity prediction
    - Code generation from english text
    - Code explanation

## Installation
To install the dependencies simply run the following command:
```bash
pip install -r requirements.txt
```

To reproduce the results you can follow the scripts in the following sections. Note that we don't always show all possible arguments to the scripts. To get the full list of arguments with descriptions you can run the following command on any script:

```bash
python scripts/some_script.py --help
```

Before you run any of the scripts make sure you are logged in and can push to the hub:

```bash
huggingface-cli login
```

Additionally, sure you have git-lfs installed. You can find instructions for how to install it [here](https://git-lfs.github.com/).","What is CodeParrot trained on?
",Python code,huggingface/transformers/blob/main/examples/research_projects/codeparrot/README.md,5.0,"The context clearly states that CodeParrot is a GPT-2 model trained from scratch on Python code, and provides details about the training process and evaluation metrics used.

",4.0,"This question appears to be asking about the training data of CodeParrot, which is a model likely used for code analysis or generation tasks. The information required to answer this question is relatively specific and might be relevant to developers working with code-related NLP models in the Hugging Face ecosystem.

",4.0,"The question refers to a specific model called CodeParrot, which suggests that it has a particular dataset or training process associated with it. Without knowing what CodeParrot is, an operator might struggle to understand the context of this question.
"
"Finally, you should also check that **all** required weights are
initialized and print out all checkpoint weights that were not used for
initialization to make sure the model is correctly converted. It is
completely normal, that the conversion trials fail with either a wrong
shape statement or wrong name assignment. This is most likely because
either you used incorrect parameters in `[camelcase name of model]Config()`, have a
wrong architecture in the ğŸ¤— Transformers implementation, you have a bug
in the `init()` functions of one of the components of the ğŸ¤— Transformers
implementation or you need to transpose one of the checkpoint weights.

This step should be iterated with the previous step until all weights of
the checkpoint are correctly loaded in the Transformers model. Having
correctly loaded the checkpoint into the ğŸ¤— Transformers implementation,
you can then save the model under a folder of your choice
`/path/to/converted/checkpoint/folder` that should then contain both a
`pytorch_model.bin` file and a `config.json` file:

```python
model.save_pretrained(""/path/to/converted/checkpoint/folder"")
```

[TODO FILL: Here the mentor should add very specific information on what exactly has to be done for the conversion of this model]
[...]
[...]

**7. Implement the forward pass**

Having managed to correctly load the pretrained weights into the ğŸ¤—
Transformers implementation, you should now make sure that the forward
pass is correctly implemented. In [Get familiar with the original
repository](#34-run-a-pretrained-checkpoint-using-the-original-repository),
you have already created a script that runs a forward pass of the model
using the original repository. Now you should write an analogous script
using the ğŸ¤— Transformers implementation instead of the original one. It
should look as follows:

[TODO FILL: Here the model name might have to be adapted, *e.g.*, maybe [camelcase name of model]ForConditionalGeneration instead of [camelcase name of model]Model]","What files should be present in a converted checkpoint folder?
",pytorch_model.bin and config.json,huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md,4.0,"The context provides information on how to load a pre-trained checkpoint into the ğŸ¤— Transformers implementation and save it under a specific folder. However, it does not explicitly state what files should be present in the converted checkpoint folder beyond mentioning the `pytorch_model.bin` file and the `config.json` file.

",4.0,"This question is useful because it indicates that the user has gone through the process of converting their model to a specific format, but is now unsure about the expected file structure. It shows a good understanding of the conversion process and highlights a potential issue that may be relevant for other users who are working with Hugging Face models.
",5.0,"This question is clear and unambiguous without any need for additional context. It asks about the expected contents of a specific type of file, which implies that the reader already knows what a ""converted checkpoint folder"" refers to.

"
"</Tip>


When inheriting from a middle-end class, look for the one handling the same modality / category of models as the one you
are trying to support.

### Example: Adding support for BERT

Since BERT is an encoder-based model for text, its configuration inherits from the middle-end class [`~optimum.exporters.onnx.config.TextEncoderOnnxConfig`].
In `optimum/exporters/onnx/model_configs.py`:

```python
# This class is actually in optimum/exporters/onnx/config.py
class TextEncoderOnnxConfig(OnnxConfig):
    # Describes how to generate the dummy inputs.
    DUMMY_INPUT_GENERATOR_CLASSES = (DummyTextInputGenerator,)

class BertOnnxConfig(TextEncoderOnnxConfig):
    # Specifies how to normalize the BertConfig, this is needed to access common attributes
    # during dummy input generation.
    NORMALIZED_CONFIG_CLASS = NormalizedTextConfig
    # Sets the absolute tolerance to when validating the exported ONNX model against the
    # reference model.
    ATOL_FOR_VALIDATION = 1e-4

    @property
    def inputs(self) -> Dict[str, Dict[int, str]]:
        if self.task == ""multiple-choice"":
            dynamic_axis = {0: ""batch_size"", 1: ""num_choices"", 2: ""sequence_length""}
        else:
            dynamic_axis = {0: ""batch_size"", 1: ""sequence_length""}
        return {
            ""input_ids"": dynamic_axis,
            ""attention_mask"": dynamic_axis,
            ""token_type_ids"": dynamic_axis,
        }
```

First let's explain what `TextEncoderOnnxConfig` is all about. While most of the features are already implemented in `OnnxConfig`,
this class is modality-agnostic, meaning that it does not know what kind of inputs it should handle. The way input generation is
handled is via the `DUMMY_INPUT_GENERATOR_CLASSES` attribute, which is a tuple of [`~optimum.utils.input_generators.DummyInputGenerator`]s.
Here we are making a modality-aware configuration inheriting from `OnnxConfig` by specifying
`DUMMY_INPUT_GENERATOR_CLASSES = (DummyTextInputGenerator,)`.","What is the purpose of `TextEncoderOnnxConfig`?
",It is a modality-agnostic class that does not know what kind of inputs it should handle.,huggingface/optimum/blob/main/docs/source/exporters/onnx/usage_guides/contribute.mdx,4.0,"The context provides information about the purpose of `TextEncoderOnnxConfig` but does not directly state it. However, it explains that this class is used to generate dummy inputs for text-based models and how it can be inherited from in specific cases like BERT.

",4.0,"This question is somewhat useful because it shows an interest in understanding the specifics of Hugging Face's NLP ecosystem, but a deeper look reveals that this class is related to ONNX (Open Neural Network Exchange) and is likely used for model deployment or inference. The fact that it specifically asks about `TextEncoderOnnxConfig` suggests that the questioner might be working with text-based models and wants to optimize them for production use.

",5.0,"The question refers to a specific class name (`TextEncoderOnnxConfig`) but does not require any additional context beyond this. It can be understood that the question is asking for a general description or explanation of what this class does.
"
"### Distributional Statistics
**To measure the language patterns in the dataset**

*This begins to answer questions like â€œHow does the language behave in this dataset?â€*

- Adherence to [Zipfâ€™s law](https://en.wikipedia.org/wiki/Zipf%27s_law), which provides measurements of how closely the distribution over words in the dataset fits to the expected distribution of words in natural language.

![image](https://user-images.githubusercontent.com/14205986/144266979-9a5bfea2-c7b8-46fb-9749-e90ee0e5e20e.png)

  You can use this to figure out whether your dataset represents language as it tends to behave in the natural world or if there are things that are more unnatural about it.  If youâ€™re someone who enjoys optimization, then you can view the alpha value this widget calculates as a value to get as close as possible to 1 during dataset development.  Further details on alpha values following Zipfâ€™s law in different languages is available here.

  In general, an alpha greater than 2 or a minimum rank greater than 10 (take with a grain of salt) means that your distribution is relatively unnatural for natural language. This can be a sign of mixed artefacts in the dataset, such as HTML markup. You can use this information to clean up your dataset or to guide you in determining how further language you add to the dataset should be distributed.

### Comparison statistics
*This begins to answer questions like â€œWhat kinds of topics, biases, and associations are in this dataset?â€*","What is Zipf's law used for?
",To provide measurements of how closely the distribution over words in the dataset fits to the expected distribution of words in natural language.,huggingface/blog/blob/main/data-measurements-tool.md,5.0,"The context discusses Zipf's law as a tool for measuring adherence to expected language distribution patterns in a dataset. It mentions using the alpha value calculated from the law to determine whether the dataset represents natural language behavior or if it has unnatural characteristics, such as mixed artefacts like HTML markup.

",4.0,"Zipf's law is a principle in linguistics that describes how frequency distributions of words or other units in language follow a power-law distribution. This concept can be applied to NLP tasks, such as natural language processing and machine learning model development. However, the specific utility of Zipf's law in these areas requires a deeper understanding of its applications.

",5.0,"The question is clear and concise, asking about the purpose or application of Zipf's law. It does not require any additional information or context to understand what it is referring to.
"
"--
title: MAUVE
emoji: ğŸ¤—
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  MAUVE is a measure of the statistical gap between two text distributions, e.g., how far the text written by a model is the distribution of human text, using samples from both distributions.

  MAUVE is obtained by computing Kullbackâ€“Leibler (KL) divergences between the two distributions in a quantized embedding space of a large language model. It can quantify differences in the quality of generated text based on the size of the model, the decoding algorithm, and the length of the generated text. MAUVE was found to correlate the strongest with human evaluations over baseline metrics for open-ended text generation.

---

# Metric Card for MAUVE

## Metric description

MAUVE is a measure of the gap between neural text and human text. It is computed using the [Kullbackâ€“Leibler (KL) divergences](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the two distributions of text in a quantized embedding space of a large language model. MAUVE can identify differences in quality arising from model sizes and decoding algorithms.

This metric is a wrapper around the [official implementation](https://github.com/krishnap25/mauve) of MAUVE.

For more details, consult the [MAUVE paper](https://arxiv.org/abs/2102.01454).

## How to use

The metric takes two lists of strings of tokens separated by spaces: one representing `predictions` (i.e. the text generated by the model) and the second representing `references` (a reference text for each prediction):

```python
from evaluate import load
mauve = load('mauve')
predictions = [""hello world"", ""goodnight moon""]
references = [""hello world"",  ""goodnight moon""]
mauve_results = mauve.compute(predictions=predictions, references=references)
```

It also has several optional arguments:","What is the official implementation of MAUVE available on?
",https://github.com/krishnap25/mauve,huggingface/evaluate/blob/main/metrics/mauve/README.md,4.0,"The context provides a detailed description of the MAUVE metric, its definition, and its implementation. However, it does not explicitly mention where the official implementation is available, only providing a link to a GitHub repository in the metric card section.

",4.0,"This question appears to be useful because it shows an interest in a specific tool (MAUVE) that is likely relevant to NLP developers working with the Hugging Face ecosystem. The question specifically asks for the ""official implementation"" which suggests they are looking for a reliable and authoritative source of information, rather than just any implementation.

",5.0,"The question assumes familiarity with the project or tool called ""MAUVE"" and its availability, but it does not rely on any specific context or information outside of a general knowledge about software implementations. It's clear that the question is asking for a general piece of information related to MAUVE.
"
"Typically from here, one could either throw all of this into a python script or run it on a Jupyter Notebook.

However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play

## PyTorch Distributed Data Parallelism

As the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.

To convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

First a `setup` and a `cleanup` function must be declared. This will open up a processing group that all of the compute processes can communicate through

> Note: for this section of the tutorial it should be assumed these are sent in python script files. Later on a launcher using Accelerate will be discussed that removes this necessity

```python
import os
import torch.distributed as dist

def setup(rank, world_size):
    ""Sets up the process group and configuration for PyTorch Distributed Data Parallelism""
    os.environ[""MASTER_ADDR""] = 'localhost'
    os.environ[""MASTER_PORT""] = ""12355""

    # Initialize the process group
    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)

def cleanup():
    ""Cleans up the distributed environment""
    dist.destroy_process_group()
```

The last piece of the puzzle is *how do I send my data and model to another GPU?*","What must be defined before converting code to work within a PyTorch Distributed Data Parallelism setup?
",A few setup configurations.,huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md,5.0,"To answer this question unambiguously with the given context, one needs to understand that the context provides information on how to set up a distributed training environment using PyTorch Distributed Data Parallelism. The key piece of information needed to convert code to work within this setup is the definition of certain setup configurations, specifically the `setup` and `cleanup` functions, as detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html). These functions are responsible for setting up a processing group that all compute processes can communicate through. Therefore, to answer this question correctly, one must identify these setup configurations as essential for distributed training.

",5.0,"To convert code to work within a PyTorch Distributed Data Parallelism (DDP) setup, several definitions are required. Specifically, one of the crucial things that need to be defined is the distributed data parallel wrapper around your model and/or optimizer using DDP's `DataParallel` or `DistributedDataParallel` module. Additionally, before running distributed training, it's essential to define the device IDs for each process using `torch.cuda.set_device()`. Furthermore, the world size (number of processes) must be defined in the command used to launch the PyTorch process with `python -m torch.distributed.launch`. This ensures that all processes have a unique rank and can communicate properly.

",5.0,"The question refers to a specific technical concept (PyTorch Distributed Data Parallelism) but does not require any additional context beyond basic knowledge of deep learning frameworks and distributed computing. It's clear what the question is asking without needing more information.

"
"Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word ""to"", isn't the representation of only the ""to"" word. It also takes into account the words around it, which we call the â€œcontextâ€.As in, it looks to the left context, the word on the left of the one we're studying (here the word ""Welcome"") and the context on the right (here the word ""NYC"") and outputs a value for the word, within its context. It is therefore a contextualized value. One could say that the vector of 768 values holds the ""meaning"" of that word in the text. How it does this is thanks to the self-attention mechanism. The self-attention mechanism relates to different positions (or different words) in a single sequence, in order to compute a representation of that sequence. As we've seen before, this means that the resulting representation of a word has been affected by other words in the sequence. We won't dive into the specifics here, but we'll offer some further readings if you want to get a better understanding at what happens under the hood. So when should one use an encoder? Encoders can be used as standalone models in a wide variety of tasks. For example BERT, arguably the most famous transformer model, is a standalone encoder model and at the time of release, beat the state of the art in many sequence classification tasks, question answering tasks, and masked language modeling, to only cite a few. The idea is that encoders are very powerful at extracting vectors that carry meaningful information about a sequence. This vector can then be handled down the road by additional layers of neurons to make sense of them","What is the dimension of each word vector in the base BERT model?
",768,huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md,5.0,"The context clearly explains that each word vector in the base BERT model has a dimension of 768, and provides an example to illustrate how this works.

",4.0,"The question seems to be about obtaining basic information about a specific model within the Hugging Face ecosystem, which suggests that it could be useful for someone looking to get started with using BERT. However, the fact that the question is very specific and does not involve any complex concepts or scenarios implies that it might not provide a lot of insight into how to use the model effectively.

",5.0,"The question makes sense on its own without requiring any additional context or information about a specific setting. It clearly refers to a well-known model (BERT) and asks for a specific characteristic of that model (the dimension of each word vector).

"
"[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-xl.svg)](https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE)
[![Sign in with Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/sign-in-with-huggingface-xl-dark.svg)](https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE)","What is the scope of authorization provided by Hugging Face OAuth?
",openid profile,huggingface/hub-docs/blob/main/docs/hub/oauth.md,4.0,"The context clearly shows the authorization URL provided by Hugging Face OAuth, which includes specific scopes (openid and profile). However, it does not explicitly mention the scope of authorization. Nevertheless, based on the common usage of such OAuth URLs, we can infer that the scope is related to user authentication and identification.

",4.0,"This question seems to be focused on a specific aspect of the Hugging Face ecosystem, which suggests that the user has some knowledge about the topic and wants to dig deeper. The question is well-defined and related to a particular feature of the platform (OAuth authorization), making it clear what the user is looking for.

However, without more context or information about the potential answers, it's difficult to assess the practical utility of this question in real-world scenarios. Nevertheless, understanding OAuth authorization scope can be essential for users who plan to use Hugging Face services with external systems, so the question has some relevance.

",5.0,"The question refers to a specific entity (Hugging Face OAuth) without requiring any additional context, making it self-contained. However, understanding the exact scope of authorization might necessitate knowledge of the OAuth system and Hugging Face's implementation details.

"
"Examples of such models are [THUDM/chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) and [mosaicml/mpt-30b](https://huggingface.co/mosaicml/mpt-30b).

To export custom models, a dictionary `custom_onnx_configs` needs to be passed to [`~optimum.exporters.onnx.main_export`], with the ONNX config definition for all the subparts of the model to export (for example, encoder and decoder subparts). The example below allows to export `mosaicml/mpt-7b` model:

```python
from optimum.exporters.onnx import main_export

from transformers import AutoConfig

from optimum.exporters.onnx.config import TextDecoderOnnxConfig
from optimum.utils import NormalizedTextConfig, DummyPastKeyValuesGenerator
from typing import Dict


class MPTDummyPastKeyValuesGenerator(DummyPastKeyValuesGenerator):
    """"""
    MPT swaps the two last dimensions for the key cache compared to usual transformers
    decoder models, thus the redefinition here.
    """"""
    def generate(self, input_name: str, framework: str = ""pt""):
        past_key_shape = (
            self.batch_size,
            self.num_attention_heads,
            self.hidden_size // self.num_attention_heads,
            self.sequence_length,
        )
        past_value_shape = (
            self.batch_size,
            self.num_attention_heads,
            self.sequence_length,
            self.hidden_size // self.num_attention_heads,
        )
        return [
            (
                self.random_float_tensor(past_key_shape, framework=framework),
                self.random_float_tensor(past_value_shape, framework=framework),
            )
            for _ in range(self.num_layers)
        ]

class CustomMPTOnnxConfig(TextDecoderOnnxConfig):
    DUMMY_INPUT_GENERATOR_CLASSES = (MPTDummyPastKeyValuesGenerator,) + TextDecoderOnnxConfig.DUMMY_INPUT_GENERATOR_CLASSES
    DUMMY_PKV_GENERATOR_CLASS = MPTDummyPastKeyValuesGenerator","What is the name of a model that swaps the two last dimensions for the key cache compared to usual transformers decoder models?
",MPT,huggingface/optimum/blob/main/docs/source/exporters/onnx/usage_guides/export_a_model.mdx,5.0,"The context clearly mentions that the model ""swaps the two last dimensions for the key cache compared to usual transformers decoder models"", which implies it is referring to a specific model architecture. Furthermore, the context includes examples of such models and provides code snippets related to these models, specifically mentioning ""mosaicml/mpt-30b"". Given this information, it seems that the question is asking about a model that is directly referenced in the provided context.

",4.0,"This question seems specific and technical, likely related to a particular implementation or design choice within transformer models. It requires knowledge of how different transformer architectures are structured, particularly regarding their decoder layers and key caches.

",4.0,"The question explicitly mentions the context of ""usual transformers decoder models"" which implies a certain level of background knowledge or information about transformer models. However, the core concept of swapping the two last dimensions for the key cache is specific and can be understood independently.

"
"## LEDConfig

[[autodoc]] LEDConfig

## LEDTokenizer

[[autodoc]] LEDTokenizer
    - build_inputs_with_special_tokens
    - get_special_tokens_mask
    - create_token_type_ids_from_sequences
    - save_vocabulary

## LEDTokenizerFast

[[autodoc]] LEDTokenizerFast

## LED specific outputs

[[autodoc]] models.led.modeling_led.LEDEncoderBaseModelOutput

[[autodoc]] models.led.modeling_led.LEDSeq2SeqModelOutput

[[autodoc]] models.led.modeling_led.LEDSeq2SeqLMOutput

[[autodoc]] models.led.modeling_led.LEDSeq2SeqSequenceClassifierOutput

[[autodoc]] models.led.modeling_led.LEDSeq2SeqQuestionAnsweringModelOutput

[[autodoc]] models.led.modeling_tf_led.TFLEDEncoderBaseModelOutput

[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqModelOutput

[[autodoc]] models.led.modeling_tf_led.TFLEDSeq2SeqLMOutput

<frameworkcontent>
<pt>

## LEDModel

[[autodoc]] LEDModel
    - forward

## LEDForConditionalGeneration

[[autodoc]] LEDForConditionalGeneration
    - forward

## LEDForSequenceClassification

[[autodoc]] LEDForSequenceClassification
    - forward

## LEDForQuestionAnswering

[[autodoc]] LEDForQuestionAnswering
    - forward

</pt>
<tf>

## TFLEDModel

[[autodoc]] TFLEDModel
    - call

## TFLEDForConditionalGeneration

[[autodoc]] TFLEDForConditionalGeneration
    - call

</tf>
</frameworkcontent>","What are the methods available for LEDTokenizer?
","build_inputs_with_special_tokens, get_special_tokens_mask, create_token_type_ids_from_sequences, save_vocabulary.",huggingface/transformers/blob/main/docs/source/en/model_doc/led.md,5.0,"The context provides a clear and detailed documentation of the LEDTokenizer class, including its methods such as build_inputs_with_special_tokens, get_special_tokens_mask, create_token_type_ids_from_sequences, and save_vocabulary. This information is easily accessible in the ""## LEDTokenizer"" section of the provided context.
",4.0,"This question appears to be related to the Hugging Face Transformers library, specifically the LEDTokenizer class. A thorough understanding of the available methods for this tokenizer could be helpful for developers who want to use it in their NLP applications.

However, without more context or information about what the questioner is trying to accomplish (e.g., are they having trouble with a specific task, or do they just want to know what options are available?), it's difficult to determine how useful this question would be.

",5.0,"The question asks about the methods available for a specific class, LEDTokenizer. This implies that there's an underlying assumption of existing documentation or knowledge about this class, but it doesn't explicitly mention any context other than the class name itself.

"
"<!--
Type: model-index
Collections:
- Name: ResNet-D
  Paper:
    Title: Bag of Tricks for Image Classification with Convolutional Neural Networks
    URL: https://paperswithcode.com/paper/bag-of-tricks-for-image-classification-with
Models:
- Name: resnet101d
  In Collection: ResNet-D
  Metadata:
    FLOPs: 13805639680
    Parameters: 44570000
    File Size: 178791263
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    Tasks:
    - Image Classification
    Training Data:
    - ImageNet
    ID: resnet101d
    Crop Pct: '0.94'
    Image Size: '256'
    Interpolation: bicubic
  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/resnet.py#L716
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet101d_ra2-2803ffab.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 82.31%
      Top 5 Accuracy: 96.06%
- Name: resnet152d
  In Collection: ResNet-D
  Metadata:
    FLOPs: 20155275264
    Parameters: 60210000
    File Size: 241596837
    Architecture:
    - 1x1 Convolution
    - Batch Normalization
    - Bottleneck Residual Block
    - Convolution
    - Global Average Pooling
    - Max Pooling
    - ReLU
    - Residual Block
    - Residual Connection
    - Softmax
    Tasks:
    - Image Classification
    Training Data:
    - ImageNet
    ID: resnet152d
    Crop Pct: '0.94'
    Image Size: '256'
    Interpolation: bicubic
  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/resnet.py#L724
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet152d_ra2-5cac0439.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet","How many parameters does the ResNet101D model have?
",44570000,huggingface/pytorch-image-models/blob/main/hfdocs/source/models/resnet-d.mdx,5.0,"The context provides a detailed description of the ResNet101D model, including its architecture, metadata, and results. Specifically, it mentions that the model has 44,570,000 parameters. This information is directly related to the question being asked, making it easy to determine the answer.

",4.0,"This question appears to be asking about the specific architecture of a particular model, which might be relevant to machine learning developers working with the Hugging Face ecosystem. However, without knowing more context or details about the model's implementation within the Hugging Face framework (e.g., if it's been adapted or fine-tuned in any way), providing an accurate count of parameters could be challenging. Nonetheless, understanding parameter counts is crucial for tasks such as hyperparameter tuning and computational resource planning.
",5.0,"This question is clear and specific about the model being asked about, and it's a simple factual query that can be answered based on general knowledge of the ResNet101D architecture.
"
"# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors=""pt"")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors=""tf"")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors=""np"")
```

## Special tokens[[special-tokens]]

If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:

```py
sequence = ""I've been waiting for a HuggingFace course my whole life.""

model_inputs = tokenizer(sequence)
print(model_inputs[""input_ids""])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```

```python out
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```

One token ID was added at the beginning, and one at the end. Let's decode the two sequences of IDs above to see what this is about:

```py
print(tokenizer.decode(model_inputs[""input_ids""]))
print(tokenizer.decode(ids))
```

```python out
""[CLS] i've been waiting for a huggingface course my whole life. [SEP]""
""i've been waiting for a huggingface course my whole life.""
```

The tokenizer added the special word `[CLS]` at the beginning and the special word `[SEP]` at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don't add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you.

## Wrapping up: From tokenizer to model[[wrapping-up-from-tokenizer-to-model]]","What type of tensors does the TensorFlow version of the tokenizer return?
",TensorFlow tensors.,huggingface/course/blob/main/chapters/en/chapter2/6.mdx,5.0,"The question is clearly answerable based on the context provided, which explicitly states that ""Returns TensorFlow tensors"" when using `return_tensors=""tf""` in the `tokenizer()` function. Additionally, the example code snippets demonstrate how to use different return tensor types, including TensorFlow tensors.

",4.0,"This question appears to be relevant to machine learning developers using NLP applications with the Hugging Face ecosystem, specifically those working with the TensorFlow backend. The question seeks information about the data structure returned by the tokenizer, which is a fundamental concept in building NLP models.
",5.0,"The question clearly refers to a specific framework (TensorFlow) and tool (tokenizer), but it does not require any additional information to understand what type of tensors are being asked about. It's asking for a general characteristic of a particular implementation, which can be determined by documentation or existing knowledge.
"
"```python
pip install transformers torch
```

exporting our checkpoint with `export`

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# load model and tokenizer
model_id = ""distilbert-base-uncased-finetuned-sst-2-english""
model = AutoModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
dummy_model_input = tokenizer(""This is a sample"", return_tensors=""pt"")

# export
torch.onnx.export(
    model,
    tuple(dummy_model_input.values()),
    f=""torch-model.onnx"",
    input_names=['input_ids', 'attention_mask'],
    output_names=['logits'],
    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},
                  'attention_mask': {0: 'batch_size', 1: 'sequence'},
                  'logits': {0: 'batch_size', 1: 'sequence'}},
    do_constant_folding=True,
    opset_version=13,
)
```

### Export with `transformers.onnx`  (mid-level)

[transformers.onnx](https://huggingface.co/docs/transformers/serialization#exporting-a-model-to-onnx) enables you to convert model checkpoints to an ONNX graph by leveraging configuration objects. That way you donâ€™t have to provide the complex configuration for `dynamic_axes` etc.

Youâ€™ll first need to install some dependencies:

```python
pip install transformers[onnx] torch
```

Exporting our checkpoint with the `transformers.onnx`.

```python
from pathlib import Path
import transformers
from transformers.onnx import FeaturesManager
from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification

# load model and tokenizer
model_id = ""distilbert-base-uncased-finetuned-sst-2-english""
feature = ""sequence-classification""
model = AutoModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)","What command is used to install the dependencies required for export with `transformers.onnx`.
",pip install transformers[onnx] torch,huggingface/blog/blob/main/convert-transformers-to-onnx.md,4.0,"The question is answerable because the context provides information about installing dependencies for export with `transformers.onnx`, and it also shows a specific command to install these dependencies. However, this command does not match the one in the original context that mentions `pip install transformers torch`. Therefore, there might be some ambiguity, but the correct command is still clearly stated in another part of the context.

",4.0,"This question appears to be about using the Hugging Face Transformers library with ONNX, a popular open standard for representing deep learning models. The question is specific and asks about a particular task (installing dependencies for export), which suggests that it might be relevant to someone working on building NLP applications with this ecosystem.
",5.0,"The question refers to a specific library (`transformers.onnx`) but does not require any additional information about the context or setting. It is clear that the question is asking for a general command that can be used in various situations, making it context-independent.

"
"<p align=""center"">
    <a href=""https://circleci.com/gh/huggingface/transformers"">
        <img alt=""Build"" src=""https://img.shields.io/circleci/build/github/huggingface/transformers/main"">
    </a>
    <a href=""https://github.com/huggingface/transformers/blob/main/LICENSE"">
        <img alt=""GitHub"" src=""https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"">
    </a>
    <a href=""https://huggingface.co/docs/transformers/index"">
        <img alt=""Documentation"" src=""https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"">
    </a>
    <a href=""https://github.com/huggingface/transformers/releases"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/huggingface/transformers.svg"">
    </a>
    <a href=""https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"">
        <img alt=""Contributor Covenant"" src=""https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"">
    </a>
    <a href=""https://zenodo.org/badge/latestdoi/155220641""><img src=""https://zenodo.org/badge/155220641.svg"" alt=""DOI""></a>
</p>","What is the license of the transformers GitHub repository?
",MIT License.,huggingface/transformers/blob/main/README_ru.md,5.0,"The context provides a direct link to the LICENSE file of the transformers GitHub repository, which indicates that it is likely possible to determine the license of the repository by clicking on this link. Additionally, there is a GitHub shield displaying the license of the repository, which suggests that the license information is publicly available and easily accessible.
",5.0,"The question is useful because it shows an interest in understanding the intellectual property and usage rights associated with a specific open-source library, which is relevant to machine learning developers. Knowing the license of the repository can help them assess whether they are allowed to use the library for their projects without restrictions.
",5.0,"The question clearly refers to a specific entity (the ""transformers"" GitHub repository) but provides enough context for an operator to understand that it's asking about the licensing information of this repository. No additional information is required to comprehend the meaning of the question.

"
"|Opened|[HUB_PR](https://huggingface.co/huggyllama/llama-7b/discussions/7)|[huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)|319220      |200                      |llama-license                                                                                 |https://huggingface.co/huggyllama/llama-7b/blob/main/LICENSE                                  |[LICENSE](https://huggingface.co/huggyllama/llama-7b/blob/main/LICENSE)                            |                                                                                                                     |                                                                                   |
|      |                                                                            |[TheBloke/MythoMax-L2-13B-GPTQ](https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ)|183096      |69                       |llama-2-community-license                                                                     |https://ai.meta.com/llama/license/                                                            |[LICENSE.txt](https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ/blob/main/LICENSE.txt)          |                                                                                                                     |                                                                                   |","What is the license type of the model huggyllama/llama-7b?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md,5.0,"The context provides a table listing models with their respective license types, and one of the models listed is ""huggyllama/llama-7b"". Additionally, there is a direct link to the model's LICENSE file on GitHub. With this information, it is possible to determine the license type of the model by clicking on the link or checking the table directly.

",4.0,"This question is useful because it asks about a specific model, allowing me to provide information on its licensing details. However, without knowing the context or whether the person asking the question has access to the model's repository, I cannot rate this question as extremely high.

",5.0,"This question can be understood independently without any additional context. It refers to a specific model (huggyllama/llama-7b) and asks for its license type, which is a clear and objective fact that can be looked up in the model's documentation or metadata.

"
"## Preprocess the data

To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model.
[`AutoImageProcessor`] takes care of processing image data to create `pixel_values`, `pixel_mask`, and
`labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:

- `image_mean = [0.485, 0.456, 0.406 ]`
- `image_std = [0.229, 0.224, 0.225]`

These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial
to replicate when doing inference or finetuning a pre-trained image model.

Instantiate the image processor from the same checkpoint as the model you want to finetune.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = ""facebook/detr-resnet-50""
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

Before passing the images to the `image_processor`, apply two preprocessing transformations to the dataset:
- Augmenting images
- Reformatting annotations to meet DETR expectations

First, to make sure the model does not overfit on the training data, you can apply image augmentation with any data augmentation library. Here we use [Albumentations](https://albumentations.ai/docs/) ...
This library ensures that transformations affect the image and update the bounding boxes accordingly.
The ğŸ¤— Datasets library documentation has a detailed [guide on how to augment images for object detection](https://huggingface.co/docs/datasets/object_detection),
and it uses the exact same dataset as an example. Apply the same approach here, resize each image to (480, 480),
flip it horizontally, and brighten it:

```py
>>> import albumentations
>>> import numpy as np
>>> import torch","What is the size that each image should be resized to for image augmentation?
","(480, 480)",huggingface/transformers/blob/main/docs/source/en/tasks/object_detection.md,5.0,"The context provides information about pre-processing data for a DETR model, specifically using the `AutoImageProcessor` class from the transformers library. It mentions normalizing images with mean and standard deviation values, but it also discusses image augmentation using Albumentations. However, the specific detail about resizing each image to (480, 480) is mentioned as part of the augmentation process. Therefore, one can clearly infer that for image augmentation, each image should be resized to (480, 480).

",4.0,"This question is moderately useful because it is a common issue in image processing, but the context of the Hugging Face ecosystem and NLP applications might not be directly related. However, knowing the optimal image size is crucial for many computer vision tasks, including those involved in transformer-based models.
",5.0,"The question refers to a specific task within image processing or deep learning, but it does not require any additional context beyond general knowledge of image augmentation techniques. The term ""image augmentation"" and the concept of resizing images are widely understood in this field.

"
"url = ""https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png""


def download_image(url):
    image = PIL.Image.open(requests.get(url, stream=True).raw)
    image = PIL.ImageOps.exif_transpose(image)
    image = image.convert(""RGB"")
    return image

image = download_image(url)
prompt = ""wipe out the lake""
num_inference_steps = 20
image_guidance_scale = 1.5
guidance_scale = 10

edited_image = pipe(prompt,
    image=image,
    num_inference_steps=num_inference_steps,
    image_guidance_scale=image_guidance_scale,
    guidance_scale=guidance_scale,
    generator=generator,
).images[0]
edited_image.save(""edited_image.png"")
```

An example model repo obtained using this training script can be found
here - [sayakpaul/instruct-pix2pix](https://huggingface.co/sayakpaul/instruct-pix2pix).

We encourage you to play with the following three parameters to control
speed and quality during performance:

* `num_inference_steps`
* `image_guidance_scale`
* `guidance_scale`

Particularly, `image_guidance_scale` and `guidance_scale` can have a profound impact
on the generated (""edited"") image (see [here](https://twitter.com/RisingSayak/status/1628392199196151808?s=20) for an example).

If you're looking for some interesting ways to use the InstructPix2Pix training methodology, we welcome you to check out this blog post: [Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd).

## Stable Diffusion XL

There's an equivalent `train_instruct_pix2pix_sdxl.py` script for [Stable Diffusion XL](https://huggingface.co/papers/2307.01952). Please refer to the docs [here](./README_sdxl.md) to learn more.","What is the name of the model repository that can be found here - https://huggingface.co/sayakpaul/instruct-pix2pix?
",instruct-pix2pix.,huggingface/diffusers/blob/main/examples/instruct_pix2pix/README.md,4.0,"The context provides information about a model repository named ""instruct-pix2pix"" and its usage, but it does not explicitly mention the name of the model repository that can be found at the given URL. However, by reading the provided example model repo obtained using this training script, we can infer that the name of the model repository is indeed related to the one mentioned in the context.

",4.0,"This question is useful because it asks about a specific Hugging Face model repository, which suggests that the person asking wants to access or utilize the model. A correct answer would provide the name of the repository, allowing the user to navigate to the repository and explore its contents.
",5.0,"The question asks for a specific piece of information, namely the name of the model repository. However, it provides a clear and relevant context with a URL pointing to the exact location of the repository on Hugging Face. This context is not ambiguous or vague, making it possible to understand what the question is about without additional information.

"
"```py
>>> from transformers import pipeline

>>> classifier = pipeline(""image-classification"", model=""my_awesome_food_model"")
>>> classifier(image)
[{'score': 0.31856709718704224, 'label': 'beignets'},
 {'score': 0.015232225880026817, 'label': 'bruschetta'},
 {'score': 0.01519392803311348, 'label': 'chicken_wings'},
 {'score': 0.013022331520915031, 'label': 'pork_chop'},
 {'score': 0.012728818692266941, 'label': 'prime_rib'}]
```

You can also manually replicate the results of the `pipeline` if you'd like:

<frameworkcontent>
<pt>
Load an image processor to preprocess the image and return the `input` as PyTorch tensors:

```py
>>> from transformers import AutoImageProcessor
>>> import torch

>>> image_processor = AutoImageProcessor.from_pretrained(""my_awesome_food_model"")
>>> inputs = image_processor(image, return_tensors=""pt"")
```

Pass your inputs to the model and return the logits:

```py
>>> from transformers import AutoModelForImageClassification

>>> model = AutoModelForImageClassification.from_pretrained(""my_awesome_food_model"")
>>> with torch.no_grad():
...     logits = model(**inputs).logits
```

Get the predicted label with the highest probability, and use the model's `id2label` mapping to convert it to a label:

```py
>>> predicted_label = logits.argmax(-1).item()
>>> model.config.id2label[predicted_label]
'beignets'
```
</pt>
</frameworkcontent>

<frameworkcontent>
<tf>
Load an image processor to preprocess the image and return the `input` as TensorFlow tensors:

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained(""MariaK/food_classifier"")
>>> inputs = image_processor(image, return_tensors=""tf"")
```

Pass your inputs to the model and return the logits:

```py
>>> from transformers import TFAutoModelForImageClassification

>>> model = TFAutoModelForImageClassification.from_pretrained(""MariaK/food_classifier"")
>>> logits = model(**inputs).logits
```","What is the highest scoring label in the classification result?
",beignets,huggingface/transformers/blob/main/docs/source/en/tasks/image_classification.md,5.0,"The question is clearly answerable with the given context because it directly corresponds to the information provided in the classification result, which includes a list of labels along with their corresponding scores. To determine the highest scoring label, one can simply look for the label with the maximum score.

",4.0,"This question appears to be relevant to machine learning developers working with NLP applications, particularly those using the Hugging Face ecosystem. It seems like it could be related to understanding or debugging a model's performance, which is an essential aspect of developing NLP models.

",5.0,"The question clearly references a specific type of output (classification result) without specifying any particular context or setting, making it self-contained.
"
"### Published Resources

* [`apple/ml-stable-diffusion`](https://github.com/apple/ml-stable-diffusion), by Apple. Conversion and inference library for Swift (and Python).
* [`huggingface/swift-coreml-diffusers`](https://github.com/huggingface/swift-coreml-diffusers). Hugging Face demo app, built on top of Apple's package.
* [Stable Diffusion XL 1.0 base (Core ML version)](https://huggingface.co/apple/coreml-stable-diffusion-xl-base). Model ready to run using the repos above and other third-party apps.
* [Stable Diffusion XL 1.0 base, with mixed-bit palettization (Core ML)](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization/blob/main/coreml-stable-diffusion-mixed-bit-palettization_original_compiled.zip). Same model as above, with UNet quantized with an effective palettization of 4.5 bits (on average).
* [Additional UNets with mixed-bit palettizaton](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization/tree/main/unet-mbp-sdxl-1-base).
* [Mixed-bit palettization recipes](https://huggingface.co/apple/coreml-stable-diffusion-mixed-bit-palettization/tree/main/recipes), pre-computed for popular models and ready to use.
* [`mixed_bit_compression_pre_analysis.py`](https://github.com/apple/ml-stable-diffusion/python_coreml_stable_diffusion/mixed_bit_compression_pre_analysis.py). Script to run mixed-bit analysis and recipe generation.
* [`mixed_bit_compression_apply.py`](https://github.com/apple/ml-stable-diffusion/python_coreml_stable_diffusion/mixed_bit_compression_apply.py). Script to apply recipes computed during the analysis phase.","What is the name of Apple's conversion and inference library for Swift (and Python)?
",ml-stable-diffusion,huggingface/blog/blob/main/stable-diffusion-xl-coreml.md,5.0,"The context clearly mentions ""Apple's conversion and inference library for Swift (and Python)"" as `apple/ml-stable-diffusion`, which directly answers the question. There is no ambiguity in the information provided, and the relevant link is given.
",4.0,"This question appears to be about a specific product or technology used in the Hugging Face ecosystem, but it doesn't seem directly related to NLP applications. However, since Hugging Face does provide Swift and Python bindings for their models and libraries, this question might still have some relevance to developers working with the Hugging Face ecosystem.

",5.0,"The question refers to a specific product from Apple, but it does not rely on external context to understand what ""conversion and inference library"" means. Additionally, mentioning both Swift and Python makes it clear that the question is about a library with multi-language support.
"
"```python
cls_token_id = tokenizer.token_to_id(""<cls>"")
sep_token_id = tokenizer.token_to_id(""<sep>"")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

The template looks like this:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=""$A:0 <sep>:0 <cls>:2"",
    pair=""$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2"",
    special_tokens=[(""<sep>"", sep_token_id), (""<cls>"", cls_token_id)],
)
```

And we can test it works by encoding a pair of sentences:

```python
encoding = tokenizer.encode(""Let's test this tokenizer..."", ""on a pair of sentences!"")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', ""'"", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair',
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

Finally, we add a `Metaspace` decoder:

```python
tokenizer.decoder = decoders.Metaspace()
```

and we're done with this tokenizer! We can save the tokenizer like before, and wrap it in a `PreTrainedTokenizerFast` or `XLNetTokenizerFast` if we want to use it in ğŸ¤— Transformers. One thing to note when using `PreTrainedTokenizerFast` is that on top of the special tokens, we need to tell the ğŸ¤— Transformers library to pad on the left:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token=""<s>"",
    eos_token=""</s>"",
    unk_token=""<unk>"",
    pad_token=""<pad>"",
    cls_token=""<cls>"",
    sep_token=""<sep>"",
    mask_token=""<mask>"",
    padding_side=""left"",
)
```

Or alternatively:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```","What is the token ID of the special ""<sep>"" token in this tokenizer?
",1,huggingface/course/blob/main/chapters/en/chapter6/8.mdx,5.0,"The context explicitly assigns the token ID of ""<sep>"" to `sep_token_id` which is then printed as 1. Later, in the template definition, it is mentioned that special tokens include (""<sep>"", sep_token_id), where sep_token_id is also used with a value of 1.

",4.0,"This question is useful because it shows an understanding of how to work with specific tokens in a tokenizer, which can be relevant for tasks such as token-level processing or fine-tuning models on datasets that require precise control over input sequences.

",4.0,"The question makes sense on its own without any additional context, but it's still dependent on knowing that ""this tokenizer"" refers to a specific instance or object. However, the key concept of ""<sep>"" being a special token is widely understood within the domain of natural language processing and tokenization.
"
"Community Examples

> **For more information about community pipelines, please have a look at [this issue](https://github.com/huggingface/diffusers/issues/841).**

**Community** examples consist of both inference and training examples that have been added by the community.
Please have a look at the following table to get an overview of all community examples. Click on the **Code Example** to get a copy-and-paste ready code example that you can try out.
If a community doesn't work as expected, please open an issue and ping the author on it.","What link should I look at for more information about community pipelines?
",https://github.com/huggingface/diffusers/issues/841,huggingface/diffusers/blob/main/examples/community/README.md,5.0,"The context provides explicit information about where to find more details about community pipelines, making it clear that the link mentioned in the context is relevant for answering this question.
",4.0,"The question appears to be specific and relevant to the Hugging Face ecosystem, as it asks about a particular resource (community pipelines) that might be useful for machine learning developers. However, the question is somewhat vague in that it doesn't specify what kind of information the user is looking for or how they plan to use community pipelines.
",4.0,"The question asks about a specific type of resource (community pipelines) that can be found through links, but it doesn't reference any particular document or context. It implies familiarity with community pipelines and the idea of looking for more information via links.

"
"### 3. Suppose you try to run the following code, which throws an error:

```py
from transformers import GPT3ForSequenceClassification

# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)
# ---------------------------------------------------------------------------
# ImportError                               Traceback (most recent call last)
# /var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_30848/333858878.py in <module>
# ----> 1 from transformers import GPT3ForSequenceClassification

# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)
```

Which of the following might be a good choice for the title of a forum topic to ask for help?

<Question
	choices={[
		{
			text: ""<code>ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)</code>"",
			explain: ""Including the last line of the traceback can be descriptive, but this is better reserved for the main body of the topic. Try again!""
		},
		{
			text: ""Problem with <code>from transformers import GPT3ForSequenceClassification</code>"",
			explain: ""Try again -- although this provides useful information, it's probably best reserved for the main body of the text."",
		},
		{
			text: ""Why can't I import <code>GPT3ForSequenceClassification</code>?"",
			explain: ""Good choice! This title is concise and gives the reader a clue about what might be wrong (i.e., that GPT-3 is not supported in ğŸ¤— Transformers)."",
			correct: true
		},
		{
			text: ""Is GPT-3 supported in ğŸ¤— Transformers?"",
			explain: ""Good one! Using questions as topic titles is a great way to communicate the problem to the community."",
			correct: true
		}
	]}
/>","What might be a good choice for the title of a forum topic to ask for help with an error importing GPT3ForSequenceClassification from transformers?
","""Is GPT-3 supported in ğŸ¤— Transformers?"" or ""Why can't I import GPT3ForSequenceClassification?""",huggingface/course/blob/main/chapters/en/chapter8/7.mdx,5.0,"The question can be clearly and unambiguously answered based on the context, which describes an error importing GPT3ForSequenceClassification from transformers. The context provides a specific error message, indicating that the import is failing due to the non-existence of GPT3ForSequenceClassification in the transformers library.

",4.0,"This question is useful because it is specific and relevant to the Hugging Face ecosystem, particularly the Transformers library. It demonstrates a real-world problem that developers might encounter while working with GPT-3 models for sequence classification tasks.

",5.0,"The question mentions specific technical concepts (GPT3, sequence classification, and Transformers library), but it's clear that the question is asking about a general scenario where someone is trying to import a model and needs help. The context of the error is not explicitly mentioned, implying that the question can be understood without any additional information.

"
"Model Cards

<Tip>

[New! Try our experimental Model Card Creator App](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool)

</Tip>

## What are Model Cards?

Model cards are files that accompany the models and provide handy information. Under the hood, model cards are simple Markdown files with additional metadata. Model cards are essential for discoverability, reproducibility, and sharing! You can find a model card as the `README.md` file in any model repo.

The model card should describe:
- the model
- its intended uses & potential limitations, including biases and ethical considerations as detailed in [Mitchell, 2018](https://arxiv.org/abs/1810.03993)
- the training params and experimental info (you can embed or link to an experiment tracking platform for reference)
- which datasets were used to train your model
- your evaluation results

The model card template is available [here](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md).

## Model card metadata

A model repo will render its `README.md` as a model card. The model card is a [Markdown](https://en.wikipedia.org/wiki/Markdown) file, with a [YAML](https://en.wikipedia.org/wiki/YAML) section at the top that contains metadata about the model.

The metadata you add to the model card supports discovery and easier use of your model. For example:

* Allowing users to filter models at https://huggingface.co/models.
* Displaying the model's license.
* Adding datasets to the metadata will add a message reading `Datasets used to train:` to your model card and link the relevant datasets, if they're available on the Hub.

Dataset, metric, and language identifiers are those listed on the [Datasets](https://huggingface.co/datasets), [Metrics](https://huggingface.co/metrics) and [Languages](https://huggingface.co/languages) pages.


### Adding metadata to your model card","What is a model card in machine learning.
",A model card is a file that accompanies a model and provides handy information.,huggingface/hub-docs/blob/main/docs/hub/model-cards.md,5.0,"The context provides a clear definition of what a Model Card is in machine learning, including its purpose, content, and structure. It also explains the importance of Model Cards for discoverability, reproducibility, and sharing models. Additionally, it provides examples of metadata that can be added to a model card and how it supports easier use of models.

",5.0,"A model card is a document that provides information about a machine learning model, such as its training data, evaluation metrics, and potential biases. It's an essential tool for transparency and accountability in AI development. In the context of NLP with the Hugging Face ecosystem, a model card can be particularly useful for understanding the strengths and limitations of a specific model, making it easier to choose the right model for a particular task or application.
",5.0,"A model card is an informative document or metadata that provides information about a machine learning model, including its development process, training data, evaluation metrics, and intended use cases. This type of documentation helps to increase transparency and trustworthiness of the model.
"
"The combination of these secret variables gives you the following setup options:

1. *I want to avoid that anyone without the API keys adds, deletes, or updates datasets using the Python client*: You need to setup `ADMIN_PASSWORD` and `ADMIN_API_KEY`.
2. *Additionally, I want to avoid that the `argilla` username deletes datasets from the UI*: You need to setup `ANNOTATOR_PASSWORD` and use the `argilla` generated API key with the Python Client (check your Space logs). This option might be interesting if you want to control dataset management but want anyone to browse your datasets using the `argilla` user.
3. *Additionally, I want to avoid that anyone without password browses my datasets with the `argilla` user*: You need to setup `ANNOTATOR_PASSWORD`. In this case, you can use the `argilla` generated API key and/or `ADMIN_API_KEY` values with the Python Client depending on your needs for dataset deletion rights.

Additionally, the `LOAD_DATASETS` will let you configure the sample datasets that will be pre-loaded. The default value is `single` and the supported values for this variable are:
    1. `single`: Load single datasets for TextClassification task.
    2. `full`: Load all the sample datasets for NLP tasks (TokenClassification, TextClassification, Text2Text)
    3. `none`: No datasets being loaded.


## How to upload data

Once your Argilla Space is running:

1. You need to find the **Space Direct URL under the ""Embed this Space""** option (top right, see screenshot below).
2. This URL gives you access to a full-screen Argilla UI for data labelling. The **Direct URL is the api_url parameter** for connecting the argilla Python client in order to read and write data programmatically.
3. You are now ready to **upload your first dataset into Argilla**.

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-argilla-embed-space.png""/>
</div>","What are the supported values for the `LOAD_DATASETS` variable?
","single, full, none.",huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-argilla.md,5.0,"The context explicitly mentions the supported values for the `LOAD_DATASETS` variable, which are `single`, `full`, and `none`. This information is clearly stated without any ambiguity or need for further clarification.
",4.0,"This question appears to be very specific and related to a particular configuration option in the Hugging Face ecosystem. It shows a clear understanding of the concept and asks for a concrete piece of information, which is likely to have a direct impact on the development process. Given that the variable `LOAD_DATASETS` seems to be related to loading datasets for training models, this question might indicate a practical issue or misunderstanding in using Hugging Face's dataset integration features.
",5.0,"The question clearly mentions a specific variable name, `LOAD_DATASETS`, which implies that it's asking about its possible values without referencing any particular context or external information.
"
"Transformer-based encoder-decoder models are the result of years of
research on _representation learning_ and _model architectures_. This
notebook provides a short summary of the history of neural
encoder-decoder models. For more context, the reader is advised to read
this awesome [blog
post](https://ruder.io/a-review-of-the-recent-history-of-nlp/) by
Sebastion Ruder. Additionally, a basic understanding of the
_self-attention architecture_ is recommended. The following blog post by
Jay Alammar serves as a good refresher on the original Transformer model
[here](http://jalammar.github.io/illustrated-transformer/).

At the time of writing this notebook, ğŸ¤—Transformers comprises the
encoder-decoder models *T5*, *Bart*, *MarianMT*, and *Pegasus*, which
are summarized in the docs under [model
summaries](https://huggingface.co/transformers/model_summary.html#sequence-to-sequence-models).

The notebook is divided into four parts:

-   **Background** - *A short history of neural encoder-decoder models
    is given with a focus on RNN-based models.*
-   **Encoder-Decoder** - *The transformer-based encoder-decoder model
    is presented and it is explained how the model is used for
    inference.*
-   **Encoder** - *The encoder part of the model is explained in
    detail.*
-   **Decoder** - *The decoder part of the model is explained in
    detail.*

Each part builds upon the previous part, but can also be read on its
own.

## **Background**

Tasks in natural language generation (NLG), a subfield of NLP, are best
expressed as sequence-to-sequence problems. Such tasks can be defined as
finding a model that maps a sequence of input words to a sequence of
target words. Some classic examples are *summarization* and
*translation*. In the following, we assume that each word is encoded
into a vector representation. \\(n\\) input words can then be represented as
a sequence of \\(n\\) input vectors:

$$\mathbf{X}_{1:n} = \{\mathbf{x}_1, \ldots, \mathbf{x}_n\}.$$","What are the four encoder-decoder models that comprise Hugging Face's Transformers at the time of writing this notebook?
","T5, Bart, MarianMT, and Pegasus.",huggingface/blog/blob/main/encoder-decoder.md,5.0,"The context clearly mentions that Hugging Face's Transformers comprise four encoder-decoder models at the time of writing this notebook. It then lists these models as *T5*, *Bart*, *MarianMT*, and *Pegasus* and provides a link to their model summaries in the docs.

",5.0,"This question seems to be asking for a specific piece of information about Hugging Face's Transformers library, which is likely to be relevant to machine learning developers building NLP applications with the Hugging Face ecosystem. The question appears to be well-defined and easily answerable based on publicly available knowledge.

",4.0,"The question refers to a specific framework (Hugging Face's Transformers) and a particular context (a Jupyter notebook), but it does not require knowledge about the content or purpose of the notebook. However, it assumes that the reader is familiar with Hugging Face's Transformers and its architecture.
"
"Also, make sure that you uncomment the cells corresponding to save the preprocessing files and trained model weights to your drive. Otherwise, you might lose a trained model if you google crashes. You should change the name of your model from `wav2vec2-large-xlsr-turkish-demo` to `wav2vec2-large-xlsr-{your_favorite_name}`.

Those cells correspond to:

```python
# processor.save_pretrained(""/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo"")
```

and the line:

```python
  output_dir=""/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo"",
```

further below (which should already be uncommented).

Having finished the training you should find the following files/folders under the folder `wav2vec2-large-xlsr-{your_favorite_name}` in your google drive:

- `preprocessor_config.json` - the parameters of the feature extractor
- `special_tokens_map.json` - the special token map of the tokenizer
- `tokenizer_config.json` - the parameters of the tokenizer
- `vocab.json` - the vocabulary of the tokenizer
- `checkpoint-{...}/` - the saved checkpoints saved during training. Each checkpoint should contain the files: `config.json`, `optimizer.pt`, `pytorch_model.bin`, `scheduler.pt`, `training_args.bin`. The files `config.json` and `pytorch_model.bin` define your model.

If you are happy with your training results it is time to upload your model!
Download the following files to your local computer: **`preprocessor_config.json`, `special_tokens_map.json`, `tokenizer_config.json`, `vocab.json`, `config.json`, `pytorch_model.bin`**. Those files fully define a XLSR-Wav2Vec2 model checkpoint.

Awesome you have successfully trained a XLSR-Wav2Vec2 model ğŸ˜. Now you can jump to the section [""How to upload my trained checkpoint""](#how-to-upload-my-trained-checkpoint)

### Local machine","What files define a XLSR-Wav2Vec2 model checkpoint?
","preprocessor_config.json, special_tokens_map.json, tokenizer_config.json, vocab.json, config.json, pytorch_model.bin",huggingface/transformers/blob/main/examples/research_projects/wav2vec2/FINE_TUNE_XLSR_WAV2VEC2.md,5.0,"The context explicitly mentions that the files `config.json` and `pytorch_model.bin` define a XLSR-Wav2Vec2 model checkpoint. It also lists the other files (`preprocessor_config.json`, `special_tokens_map.json`, `tokenizer_config.json`, `vocab.json`) that are required to fully define the model, but it is clear that these additional files are needed in addition to the two mentioned.

",4.0,"The question asks about specific details related to Hugging Face's model checkpoints, which are relevant to NLP developers using the ecosystem. It's a concrete and technical inquiry that would help in understanding how to work with XLSR-Wav2Vec2 models.
",4.0,"The question explicitly mentions a specific type of model (XLSR-Wav2Vec2), which allows it to be understood without additional context. However, there might be some ambiguity in what exactly is being asked - whether it's about the training files or the saved model files. Nevertheless, it's clear that the question refers to a particular type of model checkpoint.

"
"Here's what you should keep in mind: any components created under a `Column` (this is also the default) will be laid out vertically. Any component created under a `Row` will be laid out horizontally, similar to the [flexbox model in web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox).

Finally, you can also create tabs for your demo by using the `with gradio.Tabs()` context manager. Within this context, you can create multiple tabs by specifying `with gradio.TabItem(name_of_tab):` children. Any component created inside of a `with gradio.TabItem(name_of_tab):` context appears in that tab.

Now let's add a `flip_image()` function to our demo and add a new tab that flips images. Below is an example with 2 tabs and also uses a Row:

```py
import numpy as np
import gradio as gr

demo = gr.Blocks()


def flip_text(x):
    return x[::-1]


def flip_image(x):
    return np.fliplr(x)


with demo:
    gr.Markdown(""Flip text or image files using this demo."")
    with gr.Tabs():
        with gr.TabItem(""Flip Text""):
            with gr.Row():
                text_input = gr.Textbox()
                text_output = gr.Textbox()
            text_button = gr.Button(""Flip"")
        with gr.TabItem(""Flip Image""):
            with gr.Row():
                image_input = gr.Image()
                image_output = gr.Image()
            image_button = gr.Button(""Flip"")

    text_button.click(flip_text, inputs=text_input, outputs=text_output)
    image_button.click(flip_image, inputs=image_input, outputs=image_output)

demo.launch()
```","What is the default layout for components created in Gradio by default?
",Vertical.,huggingface/course/blob/main/chapters/en/chapter9/7.mdx,5.0,"The context clearly states that the default layout for components is a Column, which lays out components vertically. This information directly answers the question about what the default layout for components created in Gradio by default.

",4.0,"The question asks about the default behavior of a specific library (Gradio) when creating components. This type of information is often useful for developers who are using the Hugging Face ecosystem, as it can impact how they design and implement their NLP applications.
",5.0,"This question refers to a specific framework (Gradio) and asks about its default behavior, which can be determined without additional context. The mention of ""default"" provides enough information to understand what is being asked.
"
"Using SpanMarker at Hugging Face

[SpanMarker](https://github.com/tomaarsen/SpanMarkerNER) is a framework for training powerful Named Entity Recognition models using familiar encoders such as BERT, RoBERTa and DeBERTa. Tightly implemented on top of the ğŸ¤— Transformers library, SpanMarker can take good advantage of it. As a result, SpanMarker will be intuitive to use for anyone familiar with Transformers.

## Exploring SpanMarker in the Hub

You can find `span_marker` models by filtering at the left of the [models page](https://huggingface.co/models?library=span-marker).

All models on the Hub come with these useful features:
1. An automatically generated model card with a brief description.
2. An interactive widget you can use to play with the model directly in the browser.
3. An Inference API that allows you to make inference requests.

## Installation

To get started, you can follow the [SpanMarker installation guide](https://tomaarsen.github.io/SpanMarkerNER/install.html). You can also use the following one-line install through pip:

```
pip install -U span_marker
```

## Using existing models

All `span_marker` models can easily be loaded from the Hub.

```py
from span_marker import SpanMarkerModel

model = SpanMarkerModel.from_pretrained(""tomaarsen/span-marker-bert-base-fewnerd-fine-super"")
```

Once loaded, you can use [`SpanMarkerModel.predict`](https://tomaarsen.github.io/SpanMarkerNER/api/span_marker.modeling.html#span_marker.modeling.SpanMarkerModel.predict) to perform inference.","How do I install SpanMarker using pip?
",`pip install -U span_marker`,huggingface/hub-docs/blob/main/docs/hub/span_marker.md,5.0,"The context directly provides the installation command for SpanMarker using pip, which makes it clear and unambiguous how to install the package. Additionally, the question is explicitly answered in the context by mentioning ""the following one-line install through pip:"" followed by the exact command.
",4.0,"This question shows a clear intent to learn about installing a specific library (SpanMarker) within the Hugging Face ecosystem, which is relevant to NLP developers. The simplicity of the question allows for a direct and concise answer.
",5.0,"The question asks about installing a specific package (SpanMarker) using a common Python package manager (pip), without referencing any external context or information. This makes it clear and understandable by itself.
"
"## How to convert a model into its `BetterTransformer` format?

### Step 1: Identifying the source layer to change

First, go to `optimum/bettertransformer/__init__.py` and you'll see the dictionary `BetterTransformerManager.MODEL_MAPPING`. This should contain the mapping between a model type, and the `Tuple[str, BetterTransformerBaseLayer]` composed of the name of the `nn.Module` that can be converted to its `BetterTransformer` equivalent, and effectively the equivalent `BetterTransformer` layer class.

Let us try to do it step by step for `Bert`, first we need to identify the layers that needs to be replaced:
```python
>>> from transformers import AutoModel","What is the name of the dictionary in optimum/bettertransformer/__init__.py that contains the mapping between a model type and its BetterTransformer equivalent?
",BetterTransformerManager.MODEL_MAPPING,huggingface/optimum/blob/main/docs/source/bettertransformer/tutorials/contribute.mdx,4.0,"The context provides a clear description of how to find the mapping between a model type and its BetterTransformer equivalent, including referencing the dictionary 'BetterTransformerManager.MODEL_MAPPING' in optimum/bettertransformer/__init__.py. However, it does not explicitly mention the name of this dictionary.
",4.0,"The question appears to be specific to a particular library (Optimum/BetterTransformer) and requires knowledge of its internal implementation. This suggests that the question may not have broad applicability to the general topic of NLP with Hugging Face ecosystem. However, for developers working specifically with Optimum/BetterTransformer, this information could be crucial.
",4.0,"The question refers to specific code files (`optimum/bettertransformer/__init__.py`) which implies that the context is somewhat tied to this particular file. However, the question itself is clear and unambiguous, suggesting that an operator with access to documentation could understand what is being asked without needing additional information.
"
"|      |      |[TheBloke/CodeFuse-CodeLlama-34B-GGUF](https://huggingface.co/TheBloke/CodeFuse-CodeLlama-34B-GGUF)                                                |14          |8       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/CodeFuse-CodeLlama-34B-GGUF/blob/main/LICENSE.txt)                        |                                                                                                    |             |
|      |      |[TheBloke/Llama2-13B-MegaCode2-OASST-GPTQ](https://huggingface.co/TheBloke/Llama2-13B-MegaCode2-OASST-GPTQ)                                        |14          |3       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/Llama2-13B-MegaCode2-OASST-GPTQ/blob/main/LICENSE.txt)                    |                                                                                                    |             |
|      |      |[camelids/llama-7b-fp16-safetensors](https://huggingface.co/camelids/llama-7b-fp16-safetensors)                                                    |13          |1       |llama-license            |https://huggingface.co/camelids/llama-7b-fp16-safetensors/blob/main/LICENSE        |[LICENSE](https://huggingface.co/camelids/llama-7b-fp16-safetensors/blob/main/LICENSE)                                  |                                                                                                    |             |
|      |      |[michaelfeil/ct2fast-Llama-2-7b-chat-hf](https://huggingface.co/michaelfeil/ct2fast-Llama-2-7b-chat-hf)                                            |13          |3       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/michaelfeil/ct2fast-Llama-2-7b-chat-hf/blob/main/LICENSE.txt)                      |                                                                                                    |             |","What is the license name used by the model ""camelids/llama-7b-fp16-safetensors""?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,5.0,"The context provided contains a table with information about various models, including the model ""camelids/llama-7b-fp16-safetensors"". In this table, there is a specific row dedicated to this model, which clearly states that it uses an ""LLaMA license"". This directly answers the question.

",4.0,"This question appears to be seeking information about a specific pre-trained model within the Hugging Face ecosystem. The question is precise, which suggests that it has a clear answer and does not require additional context to resolve. However, without knowing the actual content or purpose of the question, it's hard to assess its potential impact on machine learning developers working with NLP applications using this ecosystem. Assuming the model in question uses an open-source license commonly found in this field (e.g., Apache 2.0), and given that understanding the licensing terms is crucial for developers who aim to use or modify such models, I would rate this question as useful.

",5.0,"This question refers to a specific model name, but the information needed to understand the question (i.e., the model's metadata) can be looked up independently of any additional context.
"
"Supported Transformers & Diffusers Tasks

Inference Endpoints offers out-of-the-box support for Machine Learning tasks from the Transformers, Sentence-Transformers and Diffusers libraries. Below is a table of Hugging Face managed supported tasks for Inference Endpoint. These tasks don't require any form of code or [â€œcustom containerâ€](/docs/inference-endpoints/guides/docs/guides/custom_container) to deploy an Endpoint.
If you want to customize any of the tasks below, or want to write your own custom task, check out the [â€œCreate your own inference handlerâ€](/docs/inference-endpoints/guides/custom_handler) section for more information.","What libraries does Inference Endpoints offer out-of-the-box support for?
","Transformers, Sentence-Transformers and Diffusers.",huggingface/hf-endpoints-documentation/blob/main/docs/source/supported_tasks.mdx,5.0,"The context clearly states that Inference Endpoints offers out-of-the-box support for Machine Learning tasks from the Transformers, Sentence-Transformers, and Diffusers libraries. This directly answers the question about which libraries are supported, making it easy to determine the correct answer without any ambiguity.
",5.0,"This question is useful because it directly pertains to a specific feature of the Hugging Face ecosystem, namely the Inference Endpoints. It shows that the user is interested in leveraging this feature and needs information on what libraries are supported, which is an important aspect of building NLP applications with the Hugging Face platform.

",4.0,"The question clearly refers to a specific product or service (Inference Endpoints) without any additional context, making it understandable by itself. However, it's implied that some knowledge about the product is necessary to understand what ""out-of-the-box support"" means in this context.

"
"!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# XLM-RoBERTa

<div class=""flex flex-wrap space-x-1"">
<a href=""https://huggingface.co/models?filter=xlm-roberta"">
<img alt=""Models"" src=""https://img.shields.io/badge/All_model_pages-xlm--roberta-blueviolet"">
</a>
<a href=""https://huggingface.co/spaces/docs-demos/xlm-roberta-base"">
<img alt=""Spaces"" src=""https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"">
</a>
</div>

## Overview

The XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook's
RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl
data.

The abstract from the paper is the following:","Who proposed the XLM-RoBERTa model?
",Alexis Conneau,huggingface/transformers/blob/main/docs/source/en/model_doc/xlm-roberta.md,4.0,"The context provides a clear citation to a research paper where the XLM-RoBERTa model was proposed. However, it does not explicitly mention the authors' names within the context itself, but rather in the citation. Despite this, the citation directly links to an external source (arXiv) that can be easily accessed to find the answer.

",4.0,"This question appears to be focused on a specific aspect of NLP, namely the origin of the XLM-RoBERTa model. The question can potentially lead to discussions about the model's architecture, pre-training objectives, and evaluation metrics, which could be useful for machine learning developers building NLP applications with the Hugging Face ecosystem.

",5.0,"The question is clear and concise, asking directly about the origin of a specific model. It does not require any additional information to understand.
"
"**Finding the best model for your task**<br/>
Suppose you know exactly what your task is and you want to find the right model for the job. You can check out the leaderboard for a dataset representative of your task, which aggregates all the results. Thatâ€™s great! And what if that fancy new model youâ€™re interested in isnâ€™t on the [leaderboard](https://huggingface.co/spaces/autoevaluate/leaderboards) yet for that dataset? Simply run an evaluation for it, without leaving the Hub.

**Evaluating models on your brand new dataset**<br/>
Now what if you have a brand spanking new dataset that you want to run baselines on? You can upload it to the Hub and evaluate as many models on it as you like. No code required. Whatâ€™s more, you can be sure that the way you are evaluating these models on your dataset is exactly the same as how theyâ€™ve been evaluated on other datasets.

**Evaluating your model on many other related datasets**<br/>
Or suppose you have a brand new question answering model, trained on SQuAD? There are hundreds of different question answering datasets to evaluate on :scream: You can pick the ones you are interested in and evaluate your model, directly from the Hub.

## Ecosystem

![The Hugging Face Ecosystem and Evaluation on the Hub](/blog/assets/82_eval_on_the_hub/ecosystem.png)
<figcaption><center><i>Evaluation on the Hub fits neatly into the Hugging Face ecosystem.</i></center></figcaption>

Evaluation on the Hub is meant to make your life easier. But of course, thereâ€™s a lot happening in the background. What we really like about Evaluation on the Hub: it fits so neatly into the existing Hugging Face ecosystem, we almost had to do it. Users start on dataset pages, from where they can launch evaluations or see leaderboards. The model evaluation submission interface and the leaderboards are regular Hugging Face Spaces. The evaluation backend is powered by AutoTrain, which opens up a PR on the Hub for the given modelâ€™s model card.","What is the name of the service that powers the evaluation backend on the Hub?
",AutoTrain,huggingface/blog/blob/main/eval-on-the-hub.md,4.0,"The context describes Evaluation on the Hub as a service that allows users to evaluate models without leaving the platform, but it doesn't explicitly mention the name of the service powering the evaluation backend. However, it does mention AutoTrain as a part of the ecosystem and states that the evaluation backend is powered by AutoTrain, which opens up a PR on the Hub for the given model's model card.

",4.0,"This question appears to be a specific inquiry about the technical infrastructure behind the Hugging Face Hub, which suggests it may not be directly related to the application or implementation of NLP models within machine learning development. However, understanding such underlying services can provide context for developers navigating the ecosystem.
",4.0,"The question asks about the name of a specific service powering an evaluation backend, but it doesn't specify which backend or any other context. However, to accurately answer this question, one would need to be familiar with Hugging Face's ecosystem and specifically the Hub platform.
"
"# If the answer is not fully inside the context, label is (0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # Otherwise it's the start and end token positions
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python out
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

Let's take a look at a few results to verify that our approach is correct. For the first feature we find `(83, 85)` as labels, so let's compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):

```py
idx = 0
sample_idx = inputs[""overflow_to_sample_mapping""][idx]
answer = answers[sample_idx][""text""][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs[""input_ids""][idx][start : end + 1])

print(f""Theoretical answer: {answer}, labels give: {labeled_answer}"")
```

```python out
'Theoretical answer: the Main Building, labels give: the Main Building'
```

So that's a match! Now let's check index 4, where we set the labels to `(0, 0)`, which means the answer is not in the context chunk of that feature:

```py
idx = 4
sample_idx = inputs[""overflow_to_sample_mapping""][idx]
answer = answers[sample_idx][""text""][0]

decoded_example = tokenizer.decode(inputs[""input_ids""][idx])
print(f""Theoretical answer: {answer}, decoded example: {decoded_example}"")
```","What is the value of start_positions[idx - 1] when idx is set to the context_start index?
",83,huggingface/course/blob/main/chapters/en/chapter7/7.mdx,5.0,"The context snippet shows that `start_positions[idx - 1]` is assigned a value when the loop increments `idx` until it exceeds `context_end` or `offset[idx][0] > start_char`. This indicates that `start_positions[idx - 1]` will be set to the token position just before exceeding the context start character, which corresponds to the index `context_start`.

",4.0,"This question seems straightforward and directly related to understanding how to access specific indices in a list, which is a fundamental concept in programming. The mention of `context_start` suggests that it's part of a larger NLP task or context, such as Named Entity Recognition (NER) where `start_positions` likely stores the start positions of entities in the text.

However, without more context about the specific problem being solved, the model being used, and what `idx`, `start_positions`, and `context_start` represent, it's challenging to fully assess how useful this question is. Nonetheless, for a machine learning developer familiar with the Hugging Face ecosystem and its common practices in NLP tasks like token classification or sequence labeling, this question should be relatively easy to understand.

",5.0,"The question refers to specific variables (start_positions, idx, context_start) without providing any additional context. However, it's clear that these variables are related to indexing and positions within a sequence or list.

"
"Let's round out our exploration of dataset streaming with a common application: combining multiple datasets together to create a single corpus. ğŸ¤— Datasets provides an `interleave_datasets()` function that converts a list of `IterableDataset` objects into a single `IterableDataset`, where the elements of the new dataset are obtained by alternating among the source examples. This function is especially useful when you're trying to combine large datasets, so as an example let's stream the FreeLaw subset of the Pile, which is a 51 GB dataset of legal opinions from US courts:

```py
law_dataset_streamed = load_dataset(
    ""json"",
    data_files=""https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst"",
    split=""train"",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

This dataset is large enough to stress the RAM of most laptops, yet we've been able to load and access it without breaking a sweat! Let's now combine the examples from the FreeLaw and PubMed Abstracts datasets with the `interleave_datasets()` function:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```","What is the name of the function provided by Datasets for converting a list of IterableDataset objects into a single IterableDataset?

",interleave_datasets,huggingface/course/blob/main/chapters/en/chapter5/4.mdx,5.0,"The question is clearly answerable with the given context because it directly asks about a specific function provided by Datasets, and the context explicitly mentions this function (`interleave_datasets()`) as an example of how to combine multiple datasets together. Furthermore, the code snippet in the context shows how to use this function to interleave two datasets.
",4.0,"This question is relatively specific to the Hugging Face ecosystem, but it still requires some knowledge about the Datasets library. A machine learning developer working with NLP applications might be able to find the answer through documentation or trial-and-error, but having a clear understanding of this function could save time and effort in certain workflows.
",5.0,"The question clearly references a specific library (Datasets) and mentions a particular functionality, but it does not rely on any external context or information to be understood. It can be answered directly with knowledge of the Datasets library.
"
"In terms of the metric itself, the accuracy of AUPR has been debated because its estimates are quite noisy and because of the fact that reducing the Precision-Recall Curve to a single number ignores the fact that it is about the tradeoffs between the different systems or performance points plotted and not the performance of an individual system. Reporting the original F1 and exact match scores is therefore useful to ensure a more complete representation of system performance.


## Citation

```bibtex
@article{hendrycks2021cuad,
      title={CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review},
      author={Dan Hendrycks and Collin Burns and Anya Chen and Spencer Ball},
      journal={arXiv preprint arXiv:2103.06268},
      year={2021}
}
```

## Further References

- [CUAD dataset homepage](https://www.atticusprojectai.org/cuad-v1-performance-announcements)","What is the name of the NLP dataset for legal contract review?
",CUAD.,huggingface/evaluate/blob/main/metrics/cuad/README.md,5.0,"The context provides a clear reference to the CUAD dataset, its description as an NLP dataset for legal contract review, and even includes a citation and further references to the dataset. This provides all the necessary information to unambiguously answer the question about the name of the NLP dataset for legal contract review.

",4.0,"This question is useful because it indicates a need for information about specific datasets that are relevant to NLP applications, particularly those focused on a specialized domain like legal contract review. Providing the correct answer could help clarify which datasets are suitable for tasks such as text analysis or classification in this area.

",4.0,"The question implies familiarity with the domain of Natural Language Processing (NLP) and its applications, particularly in the area of legal contract review. It doesn't provide any context or specific details about the dataset being referred to, suggesting it's a general query that could apply across various datasets used for similar purposes.

"
"To use this pipeline, you need to:
1. Install [IPEX](https://github.com/intel/intel-extension-for-pytorch)

**Note:** For each PyTorch release, there is a corresponding release of the IPEX. Here is the mapping relationship. It is recommended to install Pytorch/IPEX2.0 to get the best performance.

|PyTorch Version|IPEX Version|
|--|--|
|[v2.0.\*](https://github.com/pytorch/pytorch/tree/v2.0.1 ""v2.0.1"")|[v2.0.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.100+cpu)|
|[v1.13.\*](https://github.com/pytorch/pytorch/tree/v1.13.0 ""v1.13.0"")|[v1.13.\*](https://github.com/intel/intel-extension-for-pytorch/tree/v1.13.100+cpu)|

You can simply use pip to install IPEX with the latest version.
```python
python -m pip install intel_extension_for_pytorch
```
**Note:** To install a specific version, run with the following command:
```
python -m pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
```

2. After pipeline initialization, `prepare_for_ipex()` should be called to enable IPEX accelaration. Supported inference datatypes are Float32 and BFloat16.

**Note:** The setting of generated image height/width for `prepare_for_ipex()` should be same as the setting of pipeline inference.
```python
pipe = DiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", custom_pipeline=""stable_diffusion_ipex"")
# For Float32
pipe.prepare_for_ipex(prompt, dtype=torch.float32, height=512, width=512) #value of image height/width should be consistent with the pipeline inference
# For BFloat16
pipe.prepare_for_ipex(prompt, dtype=torch.bfloat16, height=512, width=512) #value of image height/width should be consistent with the pipeline inference
```","What are the supported inference datatypes for IPEX acceleration?
",Float32 and BFloat16.,huggingface/diffusers/blob/main/examples/community/README.md,5.0,"The context clearly states that the supported inference datatypes for IPEX acceleration are Float32 and BFloat16. This information is explicitly mentioned in point 2 of the instructions, making it easy to determine the correct answer.

",4.0,"This question seems to be relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as it specifically asks about a feature (IPEX acceleration) that is part of the Transformers library. The answer would provide valuable information on how to optimize inference performance for their models.
",4.0,"The question clearly specifies what is being asked, and it refers to a specific technology (IPEX acceleration), which should be understood by anyone familiar with the topic. However, without additional context or information about IPEX, it's not entirely clear what specific inference datatypes are being referred to, implying that there might be multiple possible answers.
"
"But, with policy-based methods, we want to optimize the policy directlyÂ **without having an intermediate step of learning a value function.**

So today,Â **we'll study our first Policy-Based method**: Reinforce. And we'll implement it from scratch using PyTorch. Before testing its robustness using CartPole-v1, PixelCopter, and Pong.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/envs.gif"" alt=""Environments""/>
</figure>

Let's get started,

- [What are Policy-Gradient Methods?](#what-are-policy-gradient-methods)
  - [An Overview of Policy Gradients](#an-overview-of-policy-gradients)
  - [The Advantages of Policy-Gradient Methods](#the-advantages-of-policy-gradient-methods)
  - [The Disadvantages of Policy-Gradient Methods](#the-disadvantages-of-policy-gradient-methods)
- [Reinforce (Monte Carlo Policy Gradient)](#reinforce-monte-carlo-policy-gradient)


## What are Policy-Gradient Methods?
Policy-Gradient is a subclass of Policy-Based Methods, a category of algorithms thatÂ **aims to optimize the policy directly without using a value function using different techniques.** The difference with Policy-Based Methods is that Policy-Gradient methods are a series of algorithms that aim to optimize the policy directlyÂ **by estimating the weights of the optimal policy using Gradient Ascent.**

### An Overview of Policy Gradients
Why do we optimize the policy directly by estimating the weights of an optimal policy using Gradient Ascent in Policy Gradients Methods?

Remember that reinforcement learning aimsÂ **to find an optimal behavior strategy (policy) to maximize its expected cumulative reward.**

We also need to remember that a policy is a function that **given a state, outputs, a distribution over actions** (in our case using a stochastic policy).

<figure class=""image table text-center m-0 w-full"">
  <img src=""https://huggingface.co/blog/assets/63_deep_rl_intro/pbm_2.jpg"" alt=""Stochastic Policy""/>
</figure>","What category of algorithms aim to optimize the policy directly without using a value function?
",Policy-Based Methods,huggingface/blog/blob/main/deep-rl-pg.md,5.0,"The context clearly states that the category of algorithms aiming to optimize the policy directly without using a value function are ""Policy-Based Methods"". Additionally, it further specifies that within these methods, Policy-Gradient is a subclass. This implies that to find the answer to the question, one would need to look for any mention of categories or subclasses of algorithms in the given context.

",4.0,"This question appears to be related to reinforcement learning, which is a subfield of machine learning. The Hugging Face ecosystem provides tools and models for NLP tasks, but it also has extensions for other areas like reinforcement learning (e.g., the Transformers library's support for RLHF). A question about direct policy optimization without using a value function could be useful for developers working on NLP applications that involve decision-making or control, such as dialogue systems or text-based games.
",5.0,"The question appears to be independent from any specific context, as it asks about a general concept in reinforcement learning. It does not require additional information or setting to understand what is being asked.
"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# LoHa

Low-Rank Hadamard Product ([LoHa](https://huggingface.co/papers/2108.06098)), is similar to LoRA except it approximates the large weight matrix with more low-rank matrices and combines them with the Hadamard product. This method is even more parameter-efficient than LoRA and achieves comparable performance.

The abstract from the paper is:","What is the name of the method that approximates the large weight matrix with more low-rank matrices and combines them with the Hadamard product?
",LoHa,huggingface/peft/blob/main/docs/source/package_reference/loha.md,5.0,"The context provides a clear description of LoHa (Low-Rank Hadamard Product), which explains its key characteristics, including approximating large weight matrices with low-rank matrices combined using the Hadamard product. The text also contrasts it with LoRA and highlights its increased parameter efficiency while achieving comparable performance.

",5.0,"This question appears to be asking about a specific technique used in natural language processing, likely related to neural networks or other machine learning models. The mention of ""large weight matrix"" and ""low-rank matrices"" suggests that it's related to matrix factorization or low-rank approximations.

",5.0,"The question asks about a specific technique for reducing the dimensionality of a large weight matrix, which involves approximating it with lower-rank matrices using the Hadamard product. This is a well-defined mathematical concept that can be understood without any additional context.

"
"The abstract from the paper is the following:

*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks.
However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*

![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif)

This model was contributed by [ybelkada](https://huggingface.co/ybelkada).
The original code can be found [here](https://github.com/salesforce/BLIP).

## Resources

- [Jupyter notebook](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb) on how to fine-tune BLIP for image captioning on a custom dataset

## BlipConfig

[[autodoc]] BlipConfig
    - from_text_vision_configs

## BlipTextConfig

[[autodoc]] BlipTextConfig

## BlipVisionConfig

[[autodoc]] BlipVisionConfig

## BlipProcessor

[[autodoc]] BlipProcessor

## BlipImageProcessor

[[autodoc]] BlipImageProcessor
    - preprocess

<frameworkcontent>
<pt>

## BlipModel

[[autodoc]] BlipModel
    - forward
    - get_text_features
    - get_image_features

## BlipTextModel","What percentage improvement in average recall@1 does BLIP achieve on image-text retrieval tasks?
",2.7%,huggingface/transformers/blob/main/docs/source/en/model_doc/blip.md,5.0,"The context provides information about the performance of BLIP on various vision-language tasks, including image-text retrieval. Specifically, it mentions that BLIP achieves a +2.7% improvement in average recall@1 on this task. However, the question asks for a percentage improvement, which is already provided in the context.

",4.0,"This question appears to be specific and relevant to a particular research topic, but it lacks context about what BLIP is or how its performance was evaluated. Additionally, the phrasing of the question doesn't specify whether the percentage improvement is compared to another model or baseline. However, for machine learning developers working with NLP applications using the Hugging Face ecosystem, having access to benchmarking data and comparisons between different models can be valuable.
",5.0,"This question appears to be independent of any specific context, as it refers to a well-defined concept (average recall@1) and a specific model (BLIP), which should be understood through its documentation or general knowledge. The question simply asks for a numerical value that can be looked up or computed based on the properties of the BLIP model.
"
"The abstract of the paper is as follows:

*Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: [this https URL](https://latent-consistency-models.github.io/).*

A demo for the [SimianLuo/LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7) checkpoint can be found [here](https://huggingface.co/spaces/SimianLuo/Latent_Consistency_Model).

The pipelines were contributed by [luosiallen](https://luosiallen.github.io/), [nagolinc](https://github.com/nagolinc), and [dg845](https://github.com/dg845).


## LatentConsistencyModelPipeline

[[autodoc]] LatentConsistencyModelPipeline
    - all
    - __call__
    - enable_freeu
    - disable_freeu
    - enable_vae_slicing
    - disable_vae_slicing
    - enable_vae_tiling
    - disable_vae_tiling

## LatentConsistencyModelImg2ImgPipeline","What is the resolution of images that can be synthesized by high-resolution image synthesis models?
",768 x 768,huggingface/diffusers/blob/main/docs/source/en/api/pipelines/latent_consistency_models.md,4.0,"The context provides information about the capabilities of high-resolution image synthesis models, specifically mentioning that Latent Diffusion models (LDMs) can synthesize images with a resolution of up to 768 x 768 pixels. However, it does not explicitly state the maximum resolution that can be achieved by these models.

",4.0,"This question is useful because it pertains to a specific application of NLP, namely high-resolution image synthesis. The Hugging Face ecosystem includes tools for text-to-image synthesis, such as DALL-E and Stable Diffusion, which are based on large language models. Understanding the capabilities of these models in terms of image resolution can help developers evaluate their suitability for various tasks.

",5.0,"This question makes sense on its own, even without knowing the specific context or model being referred to. It asks for a general characteristic (image resolution) of a class of models (high-resolution image synthesis), which is clear and unambiguous.

"
"Most ASR datasets only provide input audio samples (`audio`) and the
corresponding transcribed text (`sentence`). Common Voice contains additional
metadata information, such as `accent` and `locale`, which we can disregard for ASR.
Keeping the notebook as general as possible, we only consider the input audio and
transcribed text for fine-tuning, discarding the additional metadata information:

```python
common_voice = common_voice.remove_columns([""accent"", ""age"", ""client_id"", ""down_votes"", ""gender"", ""locale"", ""path"", ""segment"", ""up_votes""])
```

Common Voice is but one multilingual ASR dataset that we can download from the Hub -
there are plenty more available to us! To view the range of datasets available for speech recognition,
follow the link: [ASR Datasets on the Hub](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=downloads).

### Prepare Feature Extractor, Tokenizer and Data

The ASR pipeline can be de-composed into three components:
1) A feature extractor which pre-processes the raw audio-inputs
2) The model which performs the sequence-to-sequence mapping
3) A tokenizer which post-processes the model outputs to text format

In ğŸ¤— Transformers, the Whisper model has an associated feature extractor and tokenizer,
called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)
and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)
respectively.

We'll go through details of the feature extractor and tokenizer one-by-one!

### Load WhisperFeatureExtractor

Speech is represented by a 1-dimensional array that varies with time.
The value of the array at any given time step is the signal's _amplitude_
at that point. From the amplitude information alone, we can reconstruct the
frequency spectrum of the audio and recover all acoustic features.","What type of array represents speech in Whisper model?
",A 1-dimensional array.,huggingface/blog/blob/main/fine-tune-whisper.md,4.0,"The context describes how to prepare feature extractor, tokenizer, and data for an ASR pipeline using the Whisper model, including information about the feature extractor and tokenizer associated with Whisper. However, it does not explicitly state that the array representing speech is 1-dimensional or time-varying. Nevertheless, based on general knowledge about audio signals and the Whisper model's architecture, it can be inferred that the array representing speech in Whisper is likely a time-series representation of audio amplitude over time.

",4.0,"This question seems to be asking about the specific data structure used by the Whisper model to represent speech, which is an important detail for developers working with this particular NLP application. The use of arrays as a representation is likely relevant, but without more context or information about what type of array (e.g., numpy array, pandas DataFrame), it's difficult to provide a precise answer.
",4.0,"The question asks about the specific data structure used to represent speech in a particular machine learning model, Whisper. The mention of ""Whisper model"" implies that the question requires knowledge of this specific model and its implementation details.

"
"![alt text](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/reformer_benchmark/local_attention_3.png)

Note that local self-attention is implemented efficiently way so that no output is computed and subsequently ""thrown-out"" as shown here for illustration purposes by the red cross.

It's important to note here that extending the input vectors for each chunked self-attention function allows *each* single output vector \\( \mathbf{z}_{i} \\) of this self-attention function to learn better vector representations. E.g. each of the output vectors \\( \mathbf{z}_{5}^{\text{loc}}, \mathbf{z}_{6}^{\text{loc}}, \mathbf{z}_{7}^{\text{loc}}, \mathbf{z}_{8}^{\text{loc}} \\) can take into account all of the input vectors \\( \mathbf{X}_{1:8} \\) to learn better representations.

The gain in memory consumption is quite obvious: The \\( \mathcal{O}(n^2) \\) memory complexity is broken down for each segment individually so that the total asymptotic memory consumption is reduced to \\( \mathcal{O}(n_{c} * l_{c}^2) = \mathcal{O}(n * l_{c}) \\).","What is the reduced asymptotic memory consumption for local self-attention?
",O(n * lc),huggingface/blog/blob/main/reformer.md,4.0,"The context provides a clear explanation of how local self-attention reduces the asymptotic memory consumption from O(n^2) to O(n * lc), where n is the total number of input vectors and lc is the length of each chunk. The diagram also illustrates this concept, showing how output vectors can learn better representations by taking into account all input vectors within a chunk.

However, there's no explicit mention of what ""reduced asymptotic memory consumption"" means in terms of numerical values or exact expressions. Nevertheless, the explanation provides enough context to infer that it refers to the mentioned reduction from O(n^2) to O(n * lc).

",4.0,"This question is useful because it requires understanding of a specific aspect of the Hugging Face ecosystem, namely the Transformer architecture used in models like BERT and RoBERTa. The concept of local self-attention and its implications on memory consumption are relevant to developers working with these models.
",5.0,"The question asks about a specific concept within the context of attention mechanisms, but it does not require any external information or setting to understand what it's asking. It's clear and unambiguous.
"
"As ML continues to be more intertwined with different domains, collaborative and open-source ML processes that center accessibility, ethics and inclusion are a critical part of the machine learning lifecycle and a stepping stone in ML documentation.


<p align=""center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/121_model-cards/vines_idea.jpg"" width=""400""/>
  <BR/>
    <span style=""font-size:12px"">
        Today's release sits within a larger ecosystem of ML documentation work: Data and model documentation have been taken up by many tech companies, including Hugging Face ğŸ¤—. We've prioritized ""Repository Cards"" for both dataset cards and model cards, focusing on multidisciplinarity. Continuing in this line of work, the model card creation UI tool
 focuses on inclusivity, providing guidance on formatting and prompting to aid card creation for people with different backgrounds.
    </span>
        </p>

## Call to action

Let's look ahead
<p align=""center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/121_model-cards/looking_ahead.png"" width=""250""/>
</p>

This work is a ""*snapshot*"" of the current state of model cards, informed by a landscape analysis of the many ways ML documentation artefacts have been instantiated. The model book and these findings represent one perspective amongst multiple about both the current state and more aspirational visions of model cards.","What has Hugging Face prioritized for data and model documentation.
",Repository Cards.,huggingface/blog/blob/main/model-cards.md,5.0,"The context clearly mentions Hugging Face's prioritization of data and model documentation, specifically mentioning ""Repository Cards"" for datasets and models, focusing on multidisciplinary approaches. Additionally, it highlights their focus on inclusivity in the creation process through guidance on formatting and prompting. This information directly answers the question about what Hugging Face has prioritized.

",4.0,"The question seems to be asking about the priorities of Hugging Face regarding data and model documentation, which is a specific aspect of their ecosystem. This type of information is crucial for machine learning developers who rely on Hugging Face's models and datasets.

To provide an accurate answer, one would need to know about Hugging Face's policies, guidelines, or initiatives related to data and model documentation. If this information is readily available, it could be very useful for developers to understand what they can expect from Hugging Face in terms of documentation quality and availability.

However, without more context or background knowledge on the topic, it's difficult to provide a precise answer that covers all aspects of data and model documentation priorities.

",5.0,"The question seems clear on its own, asking about a specific topic (data and model documentation) related to Hugging Face. There's no implicit or explicit mention of a context that would require additional information to understand the question.
"
"| Task | Example datasets | Trainer support | ğŸ¤— Accelerate | ğŸ¤— Datasets | Colab
|---|---|:---:|:---:|:---:|:---:|
| [**`language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) | [WikiText-2](https://huggingface.co/datasets/wikitext) | âœ… | âœ… | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)
| [**`multiple-choice`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) | [SWAG](https://huggingface.co/datasets/swag) | âœ… | âœ… | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)
| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) | [SQuAD](https://huggingface.co/datasets/squad) | âœ… | âœ… | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)
| [**`summarization`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) |  [XSum](https://huggingface.co/datasets/xsum) | âœ… | âœ… | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)
| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) | [GLUE](https://huggingface.co/datasets/glue) | âœ… | âœ… | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)","Is there support for trainer in the language-modeling task?
",Yes,huggingface/transformers/blob/main/examples/pytorch/README.md,5.0,"The context provides a table with various tasks, including language-modeling, and indicates whether trainer support is available for each task. Under the ""language-modeling"" task, there is a checkmark (âœ…) in the ""Trainer support"" column, which suggests that support for the trainer is indeed available.

",4.0,"This question appears to be relevant to the Hugging Face ecosystem, particularly for developers working on NLP applications. The support for a ""trainer"" in the context of the language-modeling task could potentially relate to using tools like the Transformers library or other components within the Hugging Face suite to fine-tune models for specific tasks. Therefore, it has moderate utility because understanding this would help in determining whether certain pre-trained models can be further adapted for specialized tasks.

",5.0,"The question refers to specific components (trainer, language-modeling task) that need to be understood within a broader framework of machine learning or deep learning. However, these components are well-defined and widely recognized in the field, making it clear what is being asked without needing additional context.
"
"To work towards that goal, it is important to recognize the thoughtful, dedicated efforts that have helped model cards grow into what they are today, from the adoption of model cards as a standard practice at many large organisations to the development of sophisticated tools for hosting and generating model cards. Since model cards were proposed by Mitchell et al. (2018), the landscape of machine learning documentation has expanded and evolved. A plethora of documentation tools and templates for data, models, and ML systems have been proposed and have developed â€“ reflecting the incredible work of hundreds of researchers, impacted community members, advocates, and other stakeholders. Important discussions about the relationship between ML documentation and theories of change in responsible AI have created continued important discussions, and at times, divergence. We also recognize the challenges facing model cards, which in some ways mirror the challenges facing machine learning documentation and responsible AI efforts more generally, and we see opportunities ahead to help shape both model cards and the ecosystems in which they function positively in the months and years ahead.","Who proposed model cards?
",Mitchell et al. (2018),huggingface/hub-docs/blob/main/docs/hub/model-card-guidebook.md,4.0,"The context clearly mentions that model cards were proposed by Mitchell et al. (2018), providing a direct answer to the question. However, there is no further information about who specifically among Mitchell et al. proposed the concept. Therefore, while the authorship of the proposal can be inferred, it's not explicitly stated.

",4.0,"This question appears to be about the origin of model cards, which are used in the Hugging Face ecosystem to document and track models' performance on various datasets. The knowledge of who proposed model cards can provide context and insight into their purpose, design, and potential limitations.
",5.0,"This question does not require any additional context to understand. It directly asks about the person responsible for proposing a specific concept (model cards), which is sufficient information on its own.
"
"Parameters: 66349999
    File Size: 266850607
    Architecture:
    - 1x1 Convolution
    - Average Pooling
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Dropout
    - Inverted Residual Block
    - Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AdvProp
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tf_efficientnet_b7_ap
    LR: 0.256
    Epochs: 350
    Crop Pct: '0.949'
    Momentum: 0.9
    Batch Size: 2048
    Image Size: '600'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99
  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1405
  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ap-ddb28fec.pth
  Results:
  - Task: Image Classification
    Dataset: ImageNet
    Metrics:
      Top 1 Accuracy: 85.12%
      Top 5 Accuracy: 97.25%
- Name: tf_efficientnet_b8_ap
  In Collection: AdvProp
  Metadata:
    FLOPs: 80962956270
    Parameters: 87410000
    File Size: 351412563
    Architecture:
    - 1x1 Convolution
    - Average Pooling
    - Batch Normalization
    - Convolution
    - Dense Connections
    - Dropout
    - Inverted Residual Block
    - Squeeze-and-Excitation Block
    - Swish
    Tasks:
    - Image Classification
    Training Techniques:
    - AdvProp
    - AutoAugment
    - Label Smoothing
    - RMSProp
    - Stochastic Depth
    - Weight Decay
    Training Data:
    - ImageNet
    ID: tf_efficientnet_b8_ap
    LR: 0.128
    Epochs: 350
    Crop Pct: '0.954'
    Momentum: 0.9
    Batch Size: 2048
    Image Size: '672'
    Weight Decay: 1.0e-05
    Interpolation: bicubic
    RMSProp Decay: 0.9
    Label Smoothing: 0.1
    BatchNorm Momentum: 0.99","What is the number of parameters in tf_efficientnet_b8_ap?
",87410000,huggingface/pytorch-image-models/blob/main/docs/models/advprop.md,5.0,"The context provides information about the architecture, tasks, training techniques, and other details of a model named tf_efficientnet_b8_ap. However, it does not explicitly mention the number of parameters in this specific model. There is another entry in the context for tf_efficientnet_b7_ap which states that it has 66349999 parameters, but this information might be relevant or irrelevant to the question at hand depending on whether tf_efficientnet_b8_ap and tf_efficientnet_b7_ap are actually the same model. Luckily, we can find a more direct answer in another section of the context where it explicitly states that tf_efficientnet_b8_ap has 87410000 parameters.

",4.0,"This question appears to be related to the Hugging Face Transformers library, specifically asking about the number of parameters in a particular model architecture (`tf_efficientnet_b8_ap`). However, this specific information might not be directly available from the library's documentation or API. Nonetheless, if answered accurately, it could still provide valuable insight for users exploring different models and their characteristics.
",5.0,"This question makes sense on its own as it asks for a specific piece of information about a model (tf_efficientnet_b8_ap) without referencing any external context. The mention of 'tf_efficientnet_b8_ap' implies that the respondent should be familiar with this model, but the question itself does not rely on additional information to understand what is being asked.

"
"- Some layers were not added, *i.e.* an *activation* layer was not added, or the residual connection was forgotten
- The word embedding matrix was not tied
- The wrong positional embeddings are used because the original implementation uses on offset
- Dropout is applied during the forward pass. To fix this make sure *model.training is False* and that no dropout
  layer is falsely activated during the forward pass, *i.e.* pass *self.training* to [PyTorch's functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)

The best way to fix the problem is usually to look at the forward pass of the original implementation and the ğŸ¤—
Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out
intermediate outputs of both implementations of the forward pass to find the exact position in the network where the ğŸ¤—
Transformers implementation shows a different output than the original implementation. First, make sure that the
hard-coded `input_ids` in both scripts are identical. Next, verify that the outputs of the first transformation of
the `input_ids` (usually the word embeddings) are identical. And then work your way up to the very last layer of the
network. At some point, you will notice a difference between the two implementations, which should point you to the bug
in the ğŸ¤— Transformers implementation. From our experience, a simple and efficient way is to add many print statements
in both the original implementation and ğŸ¤— Transformers implementation, at the same positions in the network
respectively, and to successively remove print statements showing the same values for intermediate presentations.

When you're confident that both implementations yield the same output, verify the outputs with
`torch.allclose(original_output, output, atol=1e-3)`, you're done with the most difficult part! Congratulations - the
work left to be done should be a cakewalk ğŸ˜Š.","What must be set to False when making forward pass to avoid dropout?
",model.training,huggingface/transformers/blob/main/docs/source/en/add_new_model.md,5.0,"The context mentions that dropout is applied during the forward pass and provides guidance on how to fix it by setting `model.training` to False and ensuring no dropout layer is falsely activated. It explicitly states that passing `self.training` to PyTorch's functional dropout function will help avoid dropout.
",5.0,"This question requires knowledge of the Hugging Face ecosystem, specifically the use of dropout in their transformer models. The correct answer would be `training`, as setting `training` to False during inference (forward pass) is a common practice to avoid dropout and ensure consistent model behavior.

",5.0,"The question assumes basic knowledge of machine learning and neural networks, but it doesn't depend on any specific context or information from a particular document. It's clear that the question is asking about a general property of forward passes in neural networks, specifically related to dropout.
"
">>> model_type = ""distilbert""
>>> # For instance, for the ONNX export.
>>> backend = ""onnx""
>>> distilbert_tasks = list(TasksManager.get_supported_tasks_for_model_type(model_type, backend).keys())

>>> print(distilbert_tasks)
['default', 'fill-mask', 'text-classification', 'multiple-choice', 'token-classification', 'question-answering']
```

</Tip>

### PyTorch

| Task                                 | Auto Class                           |
|--------------------------------------|--------------------------------------|
| `text-generation`, `text-generation-with-past`   | `AutoModelForCausalLM`               |
| `feature-extraction`, `feature-extraction-with-past`       | `AutoModel`                          |
| `fill-mask`                          | `AutoModelForMaskedLM`               |
| `question-answering`                 | `AutoModelForQuestionAnswering`      |
| `text2text-generation`, `text2text-generation-with-past` | `AutoModelForSeq2SeqLM`              |
| `text-classification`            | `AutoModelForSequenceClassification` |
| `token-classification`               | `AutoModelForTokenClassification`    |
| `multiple-choice`                    | `AutoModelForMultipleChoice`         |
| `image-classification`               | `AutoModelForImageClassification`    |
| `object-detection`                   | `AutoModelForObjectDetection`        |
| `image-segmentation`                 | `AutoModelForImageSegmentation`      |
| `masked-im`                          | `AutoModelForMaskedImageModeling`    |
| `semantic-segmentation`              | `AutoModelForSemanticSegmentation`   |
| `automatic-speech-recognition`                      | `AutoModelForSpeechSeq2Seq`          |

### TensorFlow","What are the supported tasks for a DistilBERT model with ONNX backend?
","['default', 'fill-mask', 'text-classification', 'multiple-choice', 'token-classification', 'question-answering']",huggingface/optimum/blob/main/docs/source/exporters/task_manager.mdx,5.0,"The context provides a code snippet that lists the supported tasks for a DistilBERT model with an ONNX backend, which are 'default', 'fill-mask', 'text-classification', 'multiple-choice', 'token-classification', and 'question-answering'. This makes it clear what tasks can be performed with a DistilBERT model using the ONNX export.

",4.0,"This question is useful because it requires knowledge of specific NLP models (DistilBERT) and their integration with a particular framework (ONNX). The answer would likely involve understanding the capabilities of the model in different NLP tasks, which can be beneficial for machine learning developers building applications. However, the question is relatively narrow in scope.
",5.0,"The question is clear and directly asks about the capabilities of a specific model architecture (DistilBERT) with a particular backend (ONNX). There's no reference to external context, settings, or assumptions required to understand the question. It only depends on the knowledge of the DistilBERT model and its ONNX backend.

"
"There have been significant advances in new Transformer and Diffuser machine learning models that process and generate text, audio, and images. However, most of these popular generative AI models are not publicly available, widening the gap of machine learning capabilities between the largest tech companies and everyone else. To counter this trend, AWS and Hugging Face are partnering to contribute next-generation models to the global AI community and democratize machine learning. Through the strategic partnership, Hugging Face will leverage AWS as a preferred cloud provider so developers in Hugging Faceâ€™s community can access AWSâ€™s state-of-the-art tools (e.g., [Amazon SageMaker](https://aws.amazon.com/sagemaker), [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)) to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.

â€œThe future of AI is here, but itâ€™s not evenly distributed,â€ said Clement Delangue, CEO of Hugging Face. â€œAccessibility and transparency are the keys to sharing progress and creating tools to use these new capabilities wisely and responsibly. Amazon SageMaker and AWS-designed chips will enable our team and the larger machine learning community to convert the latest research into openly reproducible models that anyone can build on.â€

## Collaborating to scale AI in the cloud","What is the name of the preferred cloud provider for Hugging Face's community?
",AWS,huggingface/blog/blob/main/aws-partnership.md,5.0,"The context clearly states that Hugging Face has partnered with AWS, and as part of this partnership, Hugging Face will leverage AWS as a preferred cloud provider. This implies that AWS is the preferred cloud provider for Hugging Face's community. There is no ambiguity or uncertainty in this statement.

",4.0,"This question seems to be about the specific implementation details of Hugging Face, which might not directly relate to the broader topic of building NLP applications with the Hugging Face ecosystem. However, understanding the recommended infrastructure setup can still provide valuable context for developers working on large-scale projects.
",5.0,"This question can be understood without additional context, as it refers to a general fact about Hugging Face. The mention of ""preferred cloud provider"" implies that there may be alternative options, but the question itself does not require any specific knowledge beyond what is publicly available.

"
"|      |      |[TheBloke/Zarafusionex-1.1-L2-7B-GGUF](https://huggingface.co/TheBloke/Zarafusionex-1.1-L2-7B-GGUF)                                                |24          |5       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/Zarafusionex-1.1-L2-7B-GGUF/blob/main/LICENSE.txt)                        |                                                                                                    |             |
|      |      |[deerslab/llama-7b-embeddings](https://huggingface.co/deerslab/llama-7b-embeddings)                                                                |22          |5       |llama-license            |https://huggingface.co/deerslab/llama-7b-embeddings/blob/main/LICENSE              |[LICENSE](https://huggingface.co/deerslab/llama-7b-embeddings/blob/main/LICENSE)                                        |                                                                                                    |             |
|      |      |[TheBloke/NewHope-GPTQ](https://huggingface.co/TheBloke/NewHope-GPTQ)                                                                              |21          |23      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/NewHope-GPTQ/blob/main/LICENSE.txt)                                       |                                                                                                    |             |
|      |      |[localmodels/Llama-2-70B-Chat-GPTQ](https://huggingface.co/localmodels/Llama-2-70B-Chat-GPTQ)                                                      |20          |6       | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/localmodels/Llama-2-70B-Chat-GPTQ/blob/main/LICENSE.txt)                           |                                                                                                    |             |","What is the license of the model ""deerslab/llama-7b-embeddings""?
",llama-license,huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md,5.0,"The context provided includes a table listing several models, including ""deerslab/llama-7b-embeddings"". The table specifies the license for this model as ""llama-license"" and provides a link to the LICENSE file. This direct reference clearly indicates that the question about the license of the model can be answered unambiguously.

",4.0,"This question is likely to be useful because it seeks specific information about a particular model within the Hugging Face ecosystem. The license under which a model is released can have significant implications for its use and distribution, especially in applications where data privacy or intellectual property are concerns.

",5.0,"The question refers to a specific model, but it does not imply any additional context or setting. It can be understood independently of other information, and an operator with access to documentation would know what is being asked.

"
"!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# GPTSAN-japanese

## Overview

The GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama).

GPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM
in the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can
fine-tune for translation or summarization.

### Usage example

The `generate()` method can be used to generate text using GPTSAN-Japanese model.

```python
>>> from transformers import AutoModel, AutoTokenizer
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained(""Tanrei/GPTSAN-japanese"")
>>> model = AutoModel.from_pretrained(""Tanrei/GPTSAN-japanese"").cuda()
>>> x_tok = tokenizer(""ã¯ã€"", prefix_text=""ç¹”ç”°ä¿¡é•·"", return_tensors=""pt"")
>>> torch.manual_seed(0)
>>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)
>>> tokenizer.decode(gen_tok[0])
'ç¹”ç”°ä¿¡é•·ã¯ã€2004å¹´ã«ã€æˆ¦å›½BASARAã€ã®ãŸã‚ã«ã€è±Šè‡£ç§€å‰'
```

## GPTSAN Features","Who released the GPTSAN-japanese model in the repository?
",Toshiyuki Sakamoto (tanreinama),huggingface/transformers/blob/main/docs/source/en/model_doc/gptsan-japanese.md,4.0,"The context clearly mentions the author of the GPTSAN-japanese model, Toshiyuki Sakamoto (tanreinama), and also provides information about his repository (""Tanrei/GPTSAN-japanese""). However, it does not explicitly mention who released the model in the repository. Despite this ambiguity, the context suggests that Toshiyuki Sakamoto is likely the one who released the model.

",4.0,"This question is useful because it shows that the person asking is interested in understanding the origin of a specific pre-trained language model within the Hugging Face ecosystem. Knowing who released the model can be helpful for developers who want to understand the context and potential limitations of the model, as well as its possible applications.
",4.0,"The question can be understood without any additional information about the context, as it directly asks for the person or organization responsible for releasing a specific model. However, there might be an implicit assumption that the reader is familiar with the ""GPTSAN-japanese"" model and its association with a repository.
"
"In both cases, we can then test the tokenizer on a text by calling the `encode()` method:

```python
encoding = tokenizer.encode(""Let's test this tokenizer."")
print(encoding.tokens)
```

```python out
['let', ""'"", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

The `encoding` obtained is an `Encoding`, which contains all the necessary outputs of the tokenizer in its various attributes: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, and `overflowing`.

The last step in the tokenization pipeline is post-processing. We need to add the `[CLS]` token at the beginning and the `[SEP]` token at the end (or after each sentence, if we have a pair of sentences). We will use a `TemplateProcessor` for this, but first we need to know the IDs of the `[CLS]` and `[SEP]` tokens in the vocabulary:

```python
cls_token_id = tokenizer.token_to_id(""[CLS]"")
sep_token_id = tokenizer.token_to_id(""[SEP]"")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

To write the template for the `TemplateProcessor`, we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon.

The classic BERT template is thus defined as follows:

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f""[CLS]:0 $A:0 [SEP]:0"",
    pair=f""[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1"",
    special_tokens=[(""[CLS]"", cls_token_id), (""[SEP]"", sep_token_id)],
)
```

Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs.

Once this is added, going back to our previous example will give:

```python
encoding = tokenizer.encode(""Let's test this tokenizer."")
print(encoding.tokens)
```","What are the token IDs of the [CLS] and [SEP] tokens in the vocabulary?
",2 and 3.,huggingface/course/blob/main/chapters/en/chapter6/8.mdx,5.0,"The context provides a clear and unambiguous description of how to obtain the token IDs of the [CLS] and [SEP] tokens in the vocabulary using the `token_to_id()` method of the tokenizer. It also shows an example code snippet that demonstrates how to use this method to get the IDs.
",4.0,"This question is useful because it directly asks about specific details related to the Hugging Face ecosystem, specifically regarding token IDs of certain special tokens ([CLS] and [SEP]) in a vocabulary. Understanding these IDs can be crucial for model implementation and fine-tuning, particularly when dealing with pre-trained models that rely on these tokens for classification or sequence labeling tasks.

",5.0,"The question refers to specific tokens ([CLS] and [SEP]) that are commonly used in certain models or tasks, but it does not require any additional context about a specific model, document, or setting. The terms are standard and can be looked up in documentation or general knowledge about NLP.
"
"## Preprocess

<Youtube id=""ma1TrR7gE7I""/>

The next step is to load a DistilGPT2 tokenizer to process the `text` subfield:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
```

You'll notice from the example above, the `text` field is actually nested inside `answers`. This means you'll need to
extract the `text` subfield from its nested structure with the [`flatten`](https://huggingface.co/docs/datasets/process#flatten) method:","What type of tokenizer is being loaded in this example?
",DistilGPT2,huggingface/transformers/blob/main/docs/source/en/tasks/language_modeling.md,5.0,"The context clearly states that a DistilGPT2 tokenizer is being loaded, as indicated by the line `tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")`. This makes it unambiguous to determine the type of tokenizer being used.

",4.0,"The question appears to be referring to some specific code or context that loads a tokenizer, but it's not provided. However, based on general knowledge of Hugging Face and their ecosystem, I'll assume the question is about identifying the type of tokenizer being loaded.

The answer would depend on the specific library (e.g., transformers) and the function being used to load the tokenizer (e.g., AutoTokenizer, BertTokenizer). If we're assuming a common scenario where someone loads a pre-trained model using Hugging Face's AutoTokenizer, for example, the type of tokenizer would typically be a wordpiece or BPE tokenizer.

Given this context, I'll rate the question as follows:

",4.0,"The question makes sense without any additional context, as it is asking about a specific aspect (tokenizer type) within an example. The term ""this example"" implies that there might be some text or code snippet associated with the question, but it does not necessarily depend on prior knowledge of the specific content of the example.
"
"## What's New

â—Updates after Oct 10, 2022 are available in version >= 0.9â—
* Many changes since the last 0.6.x stable releases. They were previewed in 0.8.x dev releases but not everyone transitioned.
* `timm.models.layers` moved to `timm.layers`:
  * `from timm.models.layers import name` will still work via deprecation mapping (but please transition to `timm.layers`).
  * `import timm.models.layers.module` or `from timm.models.layers.module import name` needs to be changed now.
* Builder, helper, non-model modules in `timm.models` have a `_` prefix added, ie `timm.models.helpers` -> `timm.models._helpers`, there are temporary deprecation mapping files but those will be removed.
* All models now support `architecture.pretrained_tag` naming (ex `resnet50.rsb_a1`).
  * The pretrained_tag is the specific weight variant (different head) for the architecture.
  * Using only `architecture` defaults to the first weights in the default_cfgs for that model architecture.
  * In adding pretrained tags, many model names that existed to differentiate were renamed to use the tag  (ex: `vit_base_patch16_224_in21k` -> `vit_base_patch16_224.augreg_in21k`). There are deprecation mappings for these.
* A number of models had their checkpoints remaped to match architecture changes needed to better support `features_only=True`, there are `checkpoint_filter_fn` methods in any model module that was remapped. These can be passed to `timm.models.load_checkpoint(..., filter_fn=timm.models.swin_transformer_v2.checkpoint_filter_fn)` to remap your existing checkpoint.
* The Hugging Face Hub (https://huggingface.co/timm) is now the primary source for `timm` weights. Model cards include link to papers, original source, license.
* Previous 0.6.x can be cloned from [0.6.x](https://github.com/rwightman/pytorch-image-models/tree/0.6.x) branch or installed via pip with version.","What is the primary source for timm weights?
",The Hugging Face Hub (https://huggingface.co/timm).,huggingface/pytorch-image-models/blob/main/README.md,4.0,"The context mentions that the Hugging Face Hub is now the primary source for timm weights, but it does not explicitly state what ""timm weights"" refers to. However, given the mention of models and their architectures, it can be inferred that the question is referring to pre-trained model weights.
",5.0,"The question is about identifying the primary source of pre-trained model weights for the timm library, which is a common tool for building and training computer vision models. This information is crucial for machine learning developers who use timm in their NLP applications, especially when working with transfer learning or fine-tuning pre-trained models.

",5.0,"This question can be understood on its own without any additional information, as it directly asks about the primary source of timm weights. The term ""timm"" is likely referring to a specific library or framework, and the question is asking for a straightforward answer.
"
"The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.

The abstract from the paper is the following:

*State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a ""foundation"", that targets all modalities
at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.*

This model was contributed by [aps](https://huggingface.co/aps). The original code can be found [here](https://github.com/facebookresearch/multimodal/tree/main/examples/flava).

## FlavaConfig

[[autodoc]] FlavaConfig

## FlavaTextConfig

[[autodoc]] FlavaTextConfig

## FlavaImageConfig

[[autodoc]] FlavaImageConfig

## FlavaMultimodalConfig

[[autodoc]] FlavaMultimodalConfig

## FlavaImageCodebookConfig

[[autodoc]] FlavaImageCodebookConfig

## FlavaProcessor

[[autodoc]] FlavaProcessor

## FlavaFeatureExtractor

[[autodoc]] FlavaFeatureExtractor

## FlavaImageProcessor

[[autodoc]] FlavaImageProcessor
    - preprocess

## FlavaForPreTraining

[[autodoc]] FlavaForPreTraining
    - forward

## FlavaModel

[[autodoc]] FlavaModel
    - forward
    - get_text_features
    - get_image_features

## FlavaImageCodebook

[[autodoc]] FlavaImageCodebook
    - forward
    - get_codebook_indices
    - get_codebook_probs

## FlavaTextModel

[[autodoc]] FlavaTextModel
    - forward

## FlavaImageModel","How many tasks does the FLAVA model demonstrate performance on?
",35,huggingface/transformers/blob/main/docs/source/en/model_doc/flava.md,5.0,"The question is clearly answerable with the given context, as it directly mentions that FLAVA demonstrates impressive performance on a wide range of 35 tasks. The abstract from the paper provides explicit information about the number of tasks FLAVA performs well on.

",4.0,"The question seems to be about a specific model, FLAVA, which implies that it may require some background knowledge or research into the model's capabilities. However, asking for the number of tasks it demonstrates performance on is a clear and concise inquiry that can lead to a direct answer.
",5.0,"The question makes sense without any additional context, as it only requires knowledge of the FLAVA model. It's clear that the question refers to a specific machine learning model and asks about its performance across various tasks.

"
"## Resources

A list of official Hugging Face and community (indicated by ğŸŒ) resources to help you get started with GroupViT.

- The quickest way to get started with GroupViT is by checking the [example notebooks](https://github.com/xvjiarui/GroupViT/blob/main/demo/GroupViT_hf_inference_notebook.ipynb) (which showcase zero-shot segmentation inference).
- One can also check out the [HuggingFace Spaces demo](https://huggingface.co/spaces/xvjiarui/GroupViT) to play with GroupViT.

## GroupViTConfig

[[autodoc]] GroupViTConfig
    - from_text_vision_configs

## GroupViTTextConfig

[[autodoc]] GroupViTTextConfig

## GroupViTVisionConfig

[[autodoc]] GroupViTVisionConfig

<frameworkcontent>
<pt>

## GroupViTModel

[[autodoc]] GroupViTModel
    - forward
    - get_text_features
    - get_image_features

## GroupViTTextModel

[[autodoc]] GroupViTTextModel
    - forward

## GroupViTVisionModel

[[autodoc]] GroupViTVisionModel
    - forward

</pt>
<tf>

## TFGroupViTModel

[[autodoc]] TFGroupViTModel
    - call
    - get_text_features
    - get_image_features

## TFGroupViTTextModel

[[autodoc]] TFGroupViTTextModel
    - call

## TFGroupViTVisionModel

[[autodoc]] TFGroupViTVisionModel
    - call

</tf>
</frameworkcontent>","What is the quickest way to get started with GroupViT?
",By checking the example notebooks.,huggingface/transformers/blob/main/docs/source/en/model_doc/groupvit.md,5.0,"The context provides a clear and direct answer to the question by listing two ways to get started with GroupViT, specifically mentioning example notebooks and HuggingFace Spaces demo. This makes it easy for someone reading the context to quickly find the information they need.

",4.0,"This question seems straightforward, but it lacks context about what aspects of getting started with GroupViT are of interest. However, assuming it's asking for a general approach or resource (like tutorials, documentation, or code examples) to quickly begin working with the model, I would rate it as somewhat useful.

",5.0,"The question refers to a specific model, GroupViT, but does not mention any context or setting. It asks for a general instruction on how to begin working with it, making it understandable without additional information.

"
"But what is this _pass@k_ metric exactly? Simply put it measures the probability of at least one program passing the unittest of a programming problem given _k_ candidate generations from the model. Naturally, the metric increases with k as more candidates can potentially solve the coding challenge. You can use the metric with `datasets`:

```Python
from datasets import load_metric

code_eval = datasets.load_metric(""code_eval"")
test_cases = [""assert add(2,3)==5""]
candidates = [[""def add(a,b): return a*b"", ""def add(a, b): return a+b""]]
pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1, 2])

print(pass_at_k)

# -> {'pass@1': 0.5, 'pass@2': 1.0}
```



You can also load OpenAI's HumanEval dataset with `datasets`:

```Python
from datasets import load_dataset

dataset = load_dataset(""openai_humaneval"")
```

Now let's have a look at what we get after anxiously waiting for so long!

## Results

Let's start slow and check if we can prompt the model to write a function to return the size of a file given its path:

**Prompt:**
```Python
def get_files_size(filename):
```

**Completion:**
```Python
    """"""
    Returns the file size of a file.

    :param filepath: The path to the file.
    :type filepath: str
    :return: The file size.
    :rtype: int
    """"""
    return os.path.getsize(filepath)
```

So far so good, but let's take this a step further and try something a bit more interesting. One of the less exciting aspects of building production ready code is writing unittest. Let's see if we can use CodeParrot to do this for us!

**Prompt:**
```Python
def is_even(value):
    """"""Returns True if value is an even number.""""""
    return value % 2 == 0

# setup unit tests for is_even
import unittest
```

**Completion:**
```Python
class TestIsEven(unittest.TestCase):
    """"""Tests for is_even function.""""""","What does the _pass@k_ metric measure?
",The probability of at least one program passing the unittest of a programming problem given _k_ candidate generations from the model.,huggingface/blog/blob/main/codeparrot.md,5.0,"The context clearly explains what the _pass@k_ metric measures, which is the probability of at least one program passing a unittest given k candidate generations from the model. It also provides an example code snippet that computes this metric using the `code_eval` dataset.

",4.0,"The question appears to be asking about a specific evaluation metric used in NLP, which is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem. The answer should provide information on what the _pass@k_ metric measures, such as its purpose, calculation method, and application scope.
",5.0,"The question makes sense by itself without any additional context. It is asking about a specific metric, which can be looked up or understood through general knowledge of evaluation metrics in machine learning.
"
"Keywords: MLOps

## [FederatedScope](https://github.com/alibaba/FederatedScope)

[FederatedScope](https://github.com/alibaba/FederatedScope) is a comprehensive federated learning platform that provides convenient usage and flexible customization for various federated learning tasks in both academia and industry. Based on an event-driven architecture, [FederatedScope](https://github.com/alibaba/FederatedScope) integrates rich collections of functionalities to satisfy the burgeoning demands from federated learning, and aims to build up an easy-to-use platform for promoting learning safely and effectively.

Keywords: Federated learning, Event-driven

## [pythainlp](https://github.com/PyThaiNLP/pythainlp)

PyThaiNLP is a Python package for text processing and linguistic analysis, similar to NLTK with focus on Thai language.

Keywords: Thai, NLP, NLTK

## [FlagAI](https://github.com/FlagAI-Open/FlagAI)

[FlagAI](https://github.com/FlagAI-Open/FlagAI) (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.

Keywords: Large models, Training, Fine-tuning, Deployment, Multi-modal

## [pyserini](https://github.com/castorini/pyserini)

[pyserini](https://github.com/castorini/pyserini) is a Python toolkit for reproducible information retrieval research with sparse and dense representations. Retrieval using sparse representations is provided via integration with the group's Anserini IR toolkit. Retrieval using dense representations is provided via integration with Facebook's Faiss library.

Keywords: IR, Information Retrieval, Dense, Sparse

## [baal](https://github.com/baal-org/baal)","What is the name of a fast and easy-to-use toolkit for large-scale model training, fine-tuning, and deployment?
",FlagAI,huggingface/transformers/blob/main/awesome-transformers.md,5.0,"The context provides information about several tools related to machine learning and natural language processing. However, the question specifically asks for a fast and easy-to-use toolkit for large-scale model training, fine-tuning, and deployment. FlagAI is highlighted as a toolkit that meets this requirement, emphasizing its speed, ease of use, and support for large-scale models on various downstream tasks with multi-modality.

",4.0,"This question appears to be asking about a popular library or framework within the Hugging Face ecosystem. Given that the question mentions ""large-scale model training, fine-tuning, and deployment"", I'm guessing it might be related to the Transformers library, which is widely used for such tasks.
",5.0,"The question asks for the name of a general-purpose toolkit used for various tasks related to machine learning models. It does not mention any specific context or project, making it self-contained.
"
"Note that *chat* not only requires the LLM to handle long text inputs, but it also necessitates that the LLM is able to efficiently handle the back-and-forth dialogue between user and assistant (such as ChatGPT).

Once trained, the fundamental LLM architecture is difficult to change, so it is important to make considerations about the LLM's tasks beforehand and accordingly optimize the model's architecture.
There are two important components of the model architecture that quickly become memory and/or performance bottlenecks for large input sequences.

-   The positional embeddings
-   The key-value cache

Let's go over each component in more detail

### 3.1 Improving positional embeddings of LLMs

Self-attention puts each token in relation to each other's tokens.
As an example, the \\( \text{Softmax}(\mathbf{QK}^T) \\) matrix of the text input sequence *""Hello"", ""I"", ""love"", ""you""* could look as follows:

![](/blog/assets/163_optimize_llm/self_attn_tokens.png)

Each word token is given a probability mass at which it attends all other word tokens and, therefore is put into relation with all other word tokens. E.g. the word *""love""* attends to the word *""Hello""* with 5%, to *""I""* with 30%, and to itself with 65%.

A LLM based on self-attention, but without position embeddings would have great difficulties in understanding the positions of the text inputs to each other.
This is because the probability score computed by \\( \mathbf{QK}^T \\) relates each word token to each other word token in \\( O(1) \\) computations regardless of their relative positional distance to each other.
Therefore, for the LLM without position embeddings each token appears to have the same distance to all other tokens, *e.g.* differentiating between *""Hello I love you""* and *""You love I hello""* would be very challenging.","What are two important components of a model architecture that become memory and/or performance bottlenecks for large input sequences in LLMs?
",The positional embeddings and the key-value cache.,huggingface/transformers/blob/main/docs/source/en/llm_tutorial_optimization.md,5.0,"The context clearly states that two important components of a model architecture that become memory and/or performance bottlenecks for large input sequences in LLMs are the positional embeddings and the key-value cache. It also explains in detail why positional embeddings can lead to difficulties in understanding text input positions, making it clear how these components relate to the question.

",4.0,"This question is useful because it highlights a specific challenge that developers may encounter when working with large language models (LLMs) in the Hugging Face ecosystem. The two components mentioned, ""memory"" and ""performance"", are relevant to model architecture design, and understanding how they become bottlenecks for large input sequences can help developers optimize their models for better scalability.

",5.0,"The question clearly asks about specific aspects of Large Language Model (LLM) architecture, without requiring any additional context or information. It mentions the general concept of LLMs, but the components mentioned are universally applicable and can be understood with basic knowledge of model architectures.

"
"The docstring should give a minimal, clear example of how the respective model
is to be used in inference and also include the expected (ideally sensible)
output.
Often, readers will try out the example before even going through the function
or class definitions. Therefore, it is of utmost importance that the example
works as expected.","What should the docstring of a model include?
","A minimal, clear example of how to use the model in inference and its expected output.",huggingface/huggingface_hub/blob/main/docs/README.md,5.0,"The context suggests that a good docstring should provide an example use case and an explanation of what output to expect, implying that it should be concise, clear, and demonstrate the model's functionality.
",4.0,"This question is useful because it touches on an important aspect of software development, specifically documentation. In the context of the Hugging Face ecosystem, where models and libraries are used by developers to build NLP applications, having well-written docstrings can greatly facilitate understanding and usage of these tools.

",5.0,"The question is clear and concise, but it does not require any additional context or information to be understood. It is asking for general guidance on what a docstring should include in a broad sense, without referencing any specific setting, model, or document.

"
"The saving/sharing format is engine agnostic and using a graphic industry standard.

Let's do a quick exploration together.

```
import simulate as sm

scene = sm.Scene()
```

### Project Structure

The Python API is located in src/simulate. It allows creation and loading of scenes, and sending commands to the backend.

We provide several backends to render and/or run the scene.
The default backend requires no specific installation and is based on [pyvista](https://docs.pyvista.org/user-guide/index.html). It allows one to quick render/explored scene but doesn't handle physics simulation.
To allow physic simulations, the Unity backend can for instance be used by setting `engine=""unity""` (and soon the Godot and Blender Engines backend as well). A Unity build will be automatically downloaded (if not already) and spawed to run simulations. Alternatively, one can download and use the Unity editor themself, which must then be opened with Unity version 2021.3.2f1.

### Loading a scene from the Hub or a local file

Loading a scene from a local file or the Hub is done with `Scene.create_from()`, saving locally or pushing to the Hub with `scene.save()` or `scene.push_to_hub()`:

```
from simulate import Scene

scene = Scene.create_from('tests/test_assets/fixtures/Box.gltf')  # either local (priority) or on the Hub with full path to file
scene = Scene.create_from('simulate-tests/Box/glTF/Box.gltf', is_local=False)  # Set priority to the Hub file

scene.save('local_dir/file.gltf')  # Save to a local file
scene.push_to_hub('simulate-tests/Debug/glTF/Box.gltf')  # Save to the Hub - use a token if necessary

scene.show()
```
<p align=""center"">
    <br>
    <img src=""https://user-images.githubusercontent.com/10695622/191554717-acba4764-a4f4-4609-834a-39ddb50b844a.png"" width=""400""/>
    <br>
<p>

### Creating a Scene and adding/managing Objects in the scene

Basic example of creating a scene with a plane and a sphere above it:
```
import simulate as sm","Where is the Python API located?
",In src/simulate.,huggingface/simulate/blob/main/README.md,4.0,"The context provided is about an API called ""simulate"" that seems to be related to 3D simulation, but there's no specific information about its location or structure. However, the question asks for the location of the Python API, which is mentioned in the context as being located in src/simulate.

",4.0,"This question seems straightforward and relevant to developers who are new to the Hugging Face ecosystem. It likely indicates a lack of familiarity with the ecosystem, but asking for the location of the Python API shows an interest in using it for NLP applications.

",5.0,"This question can be understood without any additional context or information about a specific project, document, or setting. The question asks for a general location of the Python API, which implies it should be able to be found somewhere public and accessible.

"
"The `padding` argument controls padding. It can be a boolean or a string:

  - `True` or `'longest'`: pad to the longest sequence in the batch (no padding is applied if you only provide
    a single sequence).
  - `'max_length'`: pad to a length specified by the `max_length` argument or the maximum length accepted
    by the model if no `max_length` is provided (`max_length=None`). Padding will still be applied if you only provide a single sequence.
  - `False` or `'do_not_pad'`: no padding is applied. This is the default behavior.

The `truncation` argument controls truncation. It can be a boolean or a string:

  - `True` or `'longest_first'`: truncate to a maximum length specified by the `max_length` argument or
    the maximum length accepted by the model if no `max_length` is provided (`max_length=None`). This will
    truncate token by token, removing a token from the longest sequence in the pair until the proper length is
    reached.
  - `'only_second'`: truncate to a maximum length specified by the `max_length` argument or the maximum
    length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate
    the second sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.
  - `'only_first'`: truncate to a maximum length specified by the `max_length` argument or the maximum
    length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate
    the first sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.
  - `False` or `'do_not_truncate'`: no truncation is applied. This is the default behavior.

The `max_length` argument controls the length of the padding and truncation. It can be an integer or `None`, in which case it will default to the maximum length the model can accept. If the model has no specific maximum input length, truncation or padding to `max_length` is deactivated.","What happens when you only provide a single sequence if the `padding` argument is set?
",No padding is applied.,huggingface/transformers/blob/main/docs/source/en/pad_truncation.md,5.0,"The context clearly states that if only a single sequence is provided and the `padding` argument is set, no padding will be applied when `padding=True` or `'longest'`. This is explicitly mentioned in the description of the `True` or `'longest'` value for the `padding` argument.

",4.0,"This question appears to be specific to the Hugging Face Transformers library, particularly related to tokenization and padding. It's asking about the behavior of a specific argument (`padding`) in a particular scenario (when only providing a single sequence). The answer would likely depend on how the library handles edge cases like this.
",5.0,"The question makes sense by itself, as it refers to a specific scenario involving a model or function that has a `padding` argument. It's clear what's being asked without needing additional context.

"
"```py
training_args = TrainingArguments(""test-trainer"", evaluation_strategy=""epoch"")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets[""train""],
    eval_dataset=tokenized_datasets[""validation""],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

Note that we create a new `TrainingArguments` with its `evaluation_strategy` set to `""epoch""` and a new model â€” otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute:

```py
trainer.train()
```

This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark.

The `Trainer` will work out of the box on multiple GPUs or TPUs and provides lots of options, like mixed-precision training (use `fp16 = True` in your training arguments). We will go over everything it supports in Chapter 10.

This concludes the introduction to fine-tuning using the `Trainer` API. An example of doing this for most common NLP tasks will be given in [Chapter 7](/course/chapter7), but for now let's look at how to do the same thing in pure PyTorch.

<Tip>

âœï¸ **Try it out!** Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2.

</Tip>","What is the evaluation_strategy set to when creating a new TrainingArguments object?
",epoch,huggingface/course/blob/main/chapters/en/chapter3/3.mdx,5.0,"The question is clearly answerable with the given context. The code snippet explicitly sets the `evaluation_strategy` to `""epoch""` when creating a new `TrainingArguments` object.
",4.0,"This question demonstrates a clear understanding of the Hugging Face ecosystem, specifically the TrainingArguments class. It also shows that the user is familiar with the process of setting up hyperparameters for training NLP models using this library. The evaluation strategy is an important aspect of model training, and knowing its default value can help developers troubleshoot or fine-tune their models.
",5.0,"The question asks about a specific attribute of a particular class (TrainingArguments) without referencing any external context, making it clear and understandable on its own.
"
"Decision Transformers

The Decision Transformer model was introduced by [""Decision Transformer: Reinforcement Learning via Sequence Modelingâ€ by Chen L. et al](https://arxiv.org/abs/2106.01345). It abstracts Reinforcement Learning as a conditional-sequence modeling problem.

The main idea is that instead of training a policy using RL methods, such as fitting a value function, that will tell us what action to take to maximize the return (cumulative reward), **we use a sequence modeling algorithm (Transformer) that, given a desired return, past states, and actions, will generate future actions to achieve this desired return**.
Itâ€™s an autoregressive model conditioned on the desired return, past states, and actions to generate future actions that achieve the desired return.

This is a complete shift in the Reinforcement Learning paradigm since we use generative trajectory modeling (modeling the joint distribution of the sequence of states, actions, and rewards) to replace conventional RL algorithms. This means that in Decision Transformers, we donâ€™t maximize the return but rather generate a series of future actions that achieve the desired return.

The ğŸ¤— Transformers team integrated the Decision Transformer, an Offline Reinforcement Learning method, into the library as well as the Hugging Face Hub.

## Learn about Decision Transformers

To learn more about Decision Transformers, you should read the blogpost we wrote about it [Introducing Decision Transformers on Hugging Face](https://huggingface.co/blog/decision-transformers)

## Train your first Decision Transformers

Now that you understand how Decision Transformers work thanks to [Introducing Decision Transformers on Hugging Face](https://huggingface.co/blog/decision-transformers), youâ€™re ready to learn to train your first Offline Decision Transformer model from scratch to make a half-cheetah run.

Start the tutorial here ğŸ‘‰ https://huggingface.co/blog/train-decision-transformers

## Further reading","Who introduced the Decision Transformer model?
",Chen L.,huggingface/deep-rl-class/blob/main/units/en/unitbonus3/decision-transformers.mdx,5.0,"The context provides explicit information about the authors of the paper ""Decision Transformer: Reinforcement Learning via Sequence Modelingâ€, which is the paper that introduced the Decision Transformer model. Additionally, it mentions that Chen L. et al are the authors, without specifying further names.

",4.0,"This question appears to be related to NLP models and their authors, which is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem. However, without more context, it's hard to say whether this specific question will provide valuable insights or information that can be applied directly in practice.

",5.0,"The question is clear and unambiguous, requiring no additional information to understand. It mentions a specific model (Decision Transformer) and asks for the person who introduced it.
"
"```diff
-from transformers import Seq2SeqTrainingArguments
+from optimum.onnxruntime import ORTSeq2SeqTrainingArguments

-training_args = Seq2SeqTrainingArguments(
+training_args =  ORTSeq2SeqTrainingArguments(
    output_dir=""path/to/save/folder/"",
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=""path/to/save/folder/"",
-   optim = ""adamw_hf"",
+   optim=""adamw_ort_fused"",  # Fused Adam optimizer implemented by ORT
)
```

<Tip warning={false}>

DeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the moment).
You can find some [DeepSpeed configuration examples](https://github.com/huggingface/optimum/tree/main/tests/onnxruntime/ds_configs)
in the Optimum repository.

</Tip>

## ORTModule+StableDiffusion

Optimum supports accelerating Hugging Face Diffusers with ONNX Runtime in [this example](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training/stable-diffusion/text-to-image).
The core changes required to enable ONNX Runtime Training are summarized below:

```diff
import torch
from diffusers import AutoencoderKL, UNet2DConditionModel
from transformers import CLIPTextModel

+from onnxruntime.training.ortmodule import ORTModule
+from onnxruntime.training.optim.fp16_optimizer import FP16_Optimizer as ORT_FP16_Optimizer

unet = UNet2DConditionModel.from_pretrained(
    ""CompVis/stable-diffusion-v1-4"",
    subfolder=""unet"",
    ...
)
text_encoder = CLIPTextModel.from_pretrained(
    ""CompVis/stable-diffusion-v1-4"",
    subfolder=""text_encoder"",
    ...
)
vae = AutoencoderKL.from_pretrained(
    ""CompVis/stable-diffusion-v1-4"",
    subfolder=""vae"",
    ...
)

optimizer = torch.optim.AdamW(
    unet.parameters(),
    ...
)

+vae = ORTModule(vae)
+text_encoder = ORTModule(text_encoder)
+unet = ORTModule(unet)

+optimizer = ORT_FP16_Optimizer(optimizer)
```

## Other Resources","What is the optimizer used in training when ONNX Runtime is enabled?
",adamw_ort_fused,huggingface/optimum/blob/main/docs/source/onnxruntime/usage_guides/trainer.mdx,5.0,"The context provides clear information about the optimizer used when ONNX Runtime is enabled. In both examples, the `optim` parameter is set to `""adamw_ort_fused""`, which suggests that this is the optimizer used with ONNX Runtime.

Additionally, the description of ORTModule and its usage in the Stable Diffusion example explicitly mentions that it uses an FP16 optimizer, which is further confirmed by the import statement for `FP16_Optimizer`.

",4.0,"The question seems straightforward and directly related to a specific use case of the Hugging Face ecosystem, which involves integrating with ONNX Runtime. This suggests that it might be relevant for someone working on deploying models trained with Hugging Face's Transformers library onto systems optimized by ONNX Runtime.

",4.0,"The question makes sense on its own, asking about a specific optimization algorithm used during model training. However, it assumes prior knowledge of the context in which ONNX Runtime is being used and its typical configuration.

"
"As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.

Most likely, you will have to fine-tune a pretrained model for your own use case, but it is worth checking whether the hub already has suitable fine-tuned models. In this case, you might reach a higher performance by just continuing to fine-tune such a model on your dataset.","What security mechanisms are offered by Hugging Face Hub?
",Highest security mechanisms.,huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md,4.0,"The context explicitly mentions ""Highest security mechanisms"" as one of the features offered by Hugging Face Hub, which directly implies that it provides some form of security measures. However, it doesn't specify what those mechanisms are or provide any further details. Therefore, while we know that security mechanisms exist, we can't determine their exact nature or scope.

",4.0,"The question appears to be about understanding the security features provided by Hugging Face Hub, which is a platform for discovering, using, and deploying NLP models. This information could be relevant to machine learning developers working with the Hugging Face ecosystem who want to ensure their models are secure when deployed.
",5.0,"This question is clear and concise, asking about specific security features provided by a well-known platform. The only potential ambiguity could arise from the assumption that the user is familiar with what the Hugging Face Hub is and its purpose; however, this does not significantly impact the context-independence of the question itself.

"
"Inference Endpoints

Inference Endpoints provides a secure production solution to easily deploy any `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models).
In this guide, we will learn how to programmatically manage Inference Endpoints with `huggingface_hub`. For more information about the Inference Endpoints product itself, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index).

This guide assumes `huggingface_hub` is correctly installed and that your machine is logged in. Check out the [Quick Start guide](https://huggingface.co/docs/huggingface_hub/quick-start#quickstart) if that's not the case yet. The minimal version supporting Inference Endpoints API is `v0.19.0`.


## Create an Inference Endpoint

The first step is to create an Inference Endpoint using [`create_inference_endpoint`]:

```py
>>> from huggingface_hub import create_inference_endpoint

>>> endpoint = create_inference_endpoint(
...     ""my-endpoint-name"",
...     repository=""gpt2"",
...     framework=""pytorch"",
...     task=""text-generation"",
...     accelerator=""cpu"",
...     vendor=""aws"",
...     region=""us-east-1"",
...     type=""protected"",
...     instance_size=""medium"",
...     instance_type=""c6i""
... )
```","What is the minimal version of `huggingface_hub` required to support Inference Endpoints API?
",v0.19.0,huggingface/huggingface_hub/blob/main/docs/source/en/guides/inference_endpoints.md,4.0,"The context explicitly states that the minimal version of `huggingface_hub` required to support Inference Endpoints API is `v0.19.0`, but it does not provide any further information on how to use this version or what features are included in it. Additionally, the code snippet provided creates an Inference Endpoint using the `create_inference_endpoint` function from `huggingface_hub`, which suggests that the library supports Inference Endpoints API, but does not directly answer the question about the minimal required version.

",4.0,"This question is useful because it specifically asks about a particular feature (Inference Endpoints API) and its requirements in terms of a specific library (`huggingface_hub`). It requires knowledge about the Hugging Face ecosystem, particularly the `huggingface_hub` library, which makes it relevant to machine learning developers building NLP applications with this ecosystem. The question is also precise and unambiguous, making it easy for experts to provide an accurate answer.
",5.0,"This question requires knowledge of a specific library (`huggingface_hub`) and a particular feature (Inference Endpoints API), but it does not depend on any additional context or information. The term ""minimal version"" suggests that the answer should be a clear numerical value, making it a self-contained query.

"
"Choosing a metric for your task

**So you've trained your model and want to see how well itâ€™s doing on a dataset of your choice. Where do you start?**

There is no â€œone size fits allâ€ approach to choosing an evaluation metric, but some good guidelines to keep in mind are:

## Categories of metrics

There are 3 high-level categories of metrics:

1. *Generic metrics*, which can be applied to a variety of situations and datasets, such as precision and accuracy.
2. *Task-specific metrics*, which are limited to a given task, such as Machine Translation (often evaluated using metrics [BLEU](https://huggingface.co/metrics/bleu) or [ROUGE](https://huggingface.co/metrics/rouge)) or Named Entity Recognition (often evaluated with [seqeval](https://huggingface.co/metrics/seqeval)).
3. *Dataset-specific metrics*, which aim to measure model performance on specific benchmarks: for instance, the [GLUE benchmark](https://huggingface.co/datasets/glue) has a dedicated [evaluation metric](https://huggingface.co/metrics/glue).

Let's look at each of these three cases:

### Generic metrics

Many of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets.

This is the case for metrics like [accuracy](https://huggingface.co/metrics/accuracy) and [precision](https://huggingface.co/metrics/precision), which can be used for evaluating labeled (supervised) datasets, as well as [perplexity](https://huggingface.co/metrics/perplexity), which can be used for evaluating different kinds of (unsupervised) generative tasks.

To see the input structure of a given metric, you can look at its metric card. For example, in the case of [precision](https://huggingface.co/metrics/precision), the format is:
```
>>> precision_metric = evaluate.load(""precision"")
>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
>>> print(results)
{'precision': 1.0}
```

### Task-specific metrics","What are the three high-level categories of evaluation metrics for a model?
","Generic metrics, task-specific metrics, and dataset-specific metrics.",huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx,5.0,"The context explicitly mentions three high-level categories of evaluation metrics for a model: Generic metrics, Task-specific metrics, and Dataset-specific metrics.

",4.0,"This question seems straightforward and relevant to machine learning developers building NLP applications with the Hugging Face ecosystem, as understanding evaluation metrics is crucial for assessing model performance. The question is specific enough to elicit a clear and concise answer, which would be useful in guiding the development of NLP models.

",5.0,"This question can be understood without any additional context or information about the specific setting, model, or task. It asks for a general classification of evaluation metrics, which makes it context-independent.
"
"pred_str = processor.batch_decode(pred_ids)
    # we do not want to group tokens when computing the metrics
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)

    return {""wer"": wer}
```

Now, we can load the pretrained checkpoint of
[Wav2Vec2-XLS-R-300M](https://huggingface.co/facebook/wav2vec2-xls-r-300m).
The tokenizer\'s `pad_token_id` must be to define the model\'s
`pad_token_id` or in the case of `Wav2Vec2ForCTC` also CTC\'s *blank
token* \\( {}^2 \\). To save GPU memory, we enable PyTorch\'s [gradient
checkpointing](https://pytorch.org/docs/stable/checkpoint.html) and also
set the loss reduction to \""*mean*\"".

Because the dataset is quite small (\~6h of training data) and because
Common Voice is quite noisy, fine-tuning Facebook\'s
[wav2vec2-xls-r-300m checkpoint](FILL%20ME) seems to require some
hyper-parameter tuning. Therefore, I had to play around a bit with
different values for dropout,
[SpecAugment](https://arxiv.org/abs/1904.08779)\'s masking dropout rate,
layer dropout, and the learning rate until training seemed to be stable
enough.

**Note**: When using this notebook to train XLS-R on another language of
Common Voice those hyper-parameter settings might not work very well.
Feel free to adapt those depending on your use case.

```python
from transformers import Wav2Vec2ForCTC

model = Wav2Vec2ForCTC.from_pretrained(
    ""facebook/wav2vec2-xls-r-300m"",
    attention_dropout=0.0,
    hidden_dropout=0.0,
    feat_proj_dropout=0.0,
    mask_time_prob=0.05,
    layerdrop=0.0,
    ctc_loss_reduction=""mean"",
    pad_token_id=processor.tokenizer.pad_token_id,
    vocab_size=len(processor.tokenizer),
)
```","What is the default value of `ctc_loss_reduction` when loading a pretrained Wav2Vec2 model?
",mean,huggingface/blog/blob/main/fine-tune-xlsr-wav2vec2.md,4.0,"The context does not explicitly mention the default value of `ctc_loss_reduction` when loading a pretrained Wav2Vec2 model. However, it is mentioned that the loss reduction is set to `""mean""` when fine-tuning the model on the Common Voice dataset. This suggests that the default value might be `""mean""`, but it's not explicitly stated.

",4.0,"This question is useful because it seeks to clarify the default behavior of a specific hyperparameter in a widely used Hugging Face model (Wav2Vec2). The answer will help users understand how the model's loss function is calculated by default, which is crucial for fine-tuning or adapting the model for their specific use case. It also shows that the user is interested in using the model correctly and efficiently.
",4.0,"The question refers to specific technical concepts (Wav2Vec2, ctc_loss_reduction) and settings (loading a pre-trained model), but it is clear that it's asking for a default value of a hyperparameter. This information can typically be found in documentation or by inspecting the code of the Wav2Vec2 implementation.

"
"The example script uses the ğŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.

Let's start by creating a model repository to save the trained model and logs.
Here we call the model `""english-roberta-base-dummy""`, but you can change the model name as you like.

You can do this either directly on [huggingface.co](https://huggingface.co/new) (assuming that
you are logged in) or via the command line:

```
huggingface-cli repo create english-roberta-base-dummy
```

Next we clone the model repository to add the tokenizer and model files.

```
git clone https://huggingface.co/<your-username>/english-roberta-base-dummy
```

To ensure that all tensorboard traces will be uploaded correctly, we need to
track them. You can run the following command inside your model repo to do so.

```
cd english-roberta-base-dummy
git lfs track ""*tfevents*""
```

Great, we have set up our model repository. During training, we will automatically
push the training logs and model weights to the repo.

Next, let's add a symbolic link to the `run_mlm_flax.py`.

```bash
export MODEL_DIR=""./english-roberta-base-dummy""
ln -s ~/transformers/examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py ./
```

### Copy config and tokenizer of existing model

In this example, we will simply copy an existing config and tokenizer in English.
You can run the following code in a Python shell to do so.

```python
from transformers import RobertaTokenizerFast, RobertaConfig

model_dir = ""./english-roberta-base-dummy""

tokenizer = RobertaTokenizerFast.from_pretrained(""roberta-base"")
config = RobertaConfig.from_pretrained(""roberta-base"")

tokenizer.save_pretrained(model_dir)
config.save_pretrained(model_dir)
```

### Train model","How do I track tensorboard traces in the model repository.
","You can run the command `git lfs track ""*tfevents*""` inside your model repo.",huggingface/transformers/blob/main/examples/research_projects/jax-projects/dataset-streaming/README.md,4.0,"The context provides specific instructions on how to track tensorboard traces in the model repository, including the exact command to use (`git lfs track ""*tfevents*""`) and explains its purpose. However, it does not elaborate on what tensorboard traces are or why they need to be tracked.

",4.0,"This question is useful because it shows an interest in optimizing the performance of models, which is a common goal for machine learning developers. However, the question could be more specific about what it means to ""track tensorboard traces"" and how they are related to the model repository.

",4.0,"The question mentions ""tensorboard traces"" which implies that the context of a project or system using TensorBoard is already assumed. However, the specific action to be taken (tracking tensorboard traces) can still be understood without additional information about the project or system.

"
"```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

To see how the random masking works, let's feed a few examples to the data collator. Since it expects a list of `dict`s, where each `dict` represents a single chunk of contiguous text, we first iterate over the dataset before feeding the batch to the collator. We remove the `""word_ids""` key for this data collator as it does not expect it:

```python
samples = [lm_datasets[""train""][i] for i in range(2)]
for sample in samples:
    _ = sample.pop(""word_ids"")

for chunk in data_collator(samples)[""input_ids""]:
    print(f""\n'>>> {tokenizer.decode(chunk)}'"")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as "" teachers "". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is "" teachers "". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george å®‡in stated )å…¬ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```","What is the value of mlm_probability in DataCollatorForLanguageModeling?
",0.15,huggingface/course/blob/main/chapters/en/chapter7/3.mdx,5.0,"The context clearly shows a line of code where `DataCollatorForLanguageModeling` is instantiated with `mlm_probability=0.15`. This indicates that the value of `mlm_probability` in this specific instance is set to 0.15.

",4.0,"This question appears to be quite specific and is likely relevant to developers working with the Hugging Face library, particularly those who are using the `DataCollatorForLanguageModeling` class. Understanding the meaning of this parameter can help developers configure their models effectively.
",5.0,"The question refers to a specific parameter within a particular data collation function, but it does not require any external context to understand what ""mlm_probability"" is or its significance in the context of language modeling.

"
"1. Shell environment variable (default): `HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.
2. Shell environment variable: `HF_HOME`.
3. Shell environment variable: `XDG_CACHE_HOME` + `/huggingface`.

<Tip>

ğŸ¤— Transformers will use the shell environment variables `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE` if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable `TRANSFORMERS_CACHE`.

</Tip>

## Offline mode

Run ğŸ¤— Transformers in a firewalled or offline environment with locally cached files by setting the environment variable `TRANSFORMERS_OFFLINE=1`.

<Tip>

Add [ğŸ¤— Datasets](https://huggingface.co/docs/datasets/) to your offline training workflow with the environment variable `HF_DATASETS_OFFLINE=1`.

</Tip>

```bash
HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...
```

This script should run without hanging or waiting to timeout because it won't attempt to download the model from the Hub.

You can also bypass loading a model from the Hub from each [`~PreTrainedModel.from_pretrained`] call with the [`local_files_only`] parameter. When set to `True`, only local files are loaded:

```py
from transformers import T5Model

model = T5Model.from_pretrained(""./path/to/local/directory"", local_files_only=True)
```

### Fetch models and tokenizers to use offline

Another option for using ğŸ¤— Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:

* Download a file through the user interface on the [Model Hub](https://huggingface.co/models) by clicking on the â†“ icon.

    ![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)","What are the default shell environment variables for caching used by ğŸ¤— Transformers?
",`HUGGINGFACE_HUB_CACHE` or `TRANSFORMERS_CACHE`.,huggingface/transformers/blob/main/docs/source/en/installation.md,5.0,"The context provides information about shell environment variables related to ğŸ¤— Transformers, but it does not explicitly mention any default caching variables. However, one of the tips mentions that if you are coming from an earlier iteration of this library and have set `PYTORCH_TRANSFORMERS_CACHE` or `PYTORCH_PRETRAINED_BERT_CACHE`, it will be used unless `TRANSFORMERS_CACHE` is specified. But the question specifically asks about the default shell environment variables for caching, which implies we need to look at the context's first point. The context states that ""Shell environment variable (default): HUGGINGFACE_HUB_CACHE or TRANSFORMERS_CACHE."" This implies that there are two possible default caching variables: `HUGGINGFACE_HUB_CACHE` and `TRANSFORMERS_CACHE`.

",4.0,"The question asks about specific implementation details of the Hugging Face ecosystem, which is a very technical aspect. Knowing the default shell environment variables for caching can be useful for fine-tuning and customizing the behavior of the ğŸ¤— Transformers library, but it's not essential knowledge for most users.
",5.0,"This question is clear and self-contained, requiring only knowledge of the ğŸ¤— Transformers library to understand what is being asked. It does not rely on any external context or information.

"