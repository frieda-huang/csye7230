This is a wrapper on [colpali](https://github.com/illuin-tech/colpali/tree/main): Code used for training the vision retrievers in the ColPali: Efficient Document Retrieval with Vision Language Models paper

## ViDoRe: The Visual Document Retrieval Benchmark

-   https://huggingface.co/spaces/vidore/vidore-leaderboard

## Relevant Readings

-   [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT
    ](https://arxiv.org/abs/2004.12832)
-   [ColPali: Efficient Document Retrieval with Vision Language Models
    ](https://arxiv.org/abs/2407.01449)
-   [What is ColBERT and Late Interaction and Why They Matter in Search?](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/)
-   [Scaling ColPali to billions of PDFs with Vespa](https://blog.vespa.ai/scaling-colpali-to-billions/)
-   [A notebook utilizing Vespa to store embeddings generated by ColPali](https://pyvespa.readthedocs.io/en/latest/examples/colpali-document-retrieval-vision-language-models-cloud.html)
-   [Scalar and binary quantization for pgvector vector search and storage](https://jkatz05.com/post/postgres/pgvector-scalar-binary-quantization/)
-   [Best practices for querying vector data for gen AI apps in PostgreSQL](https://www.youtube.com/watch?v=PhIC4JlYg7A)
-   [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/)
-   [Why separating compute from storage is a bad idea for late interaction models](https://x.com/jobergum/status/1846945345646821533)

## Relevant GitHub Repos

-   [byaldi](https://github.com/AnswerDotAI/byaldi): A lightweight wrapper around the `colpali-engine`

## Improvement

-   Extract text from PDFs and use LLM to generate summary for each file
-   Use approximate search
-   Improve memory efficiency by dividing each embedding into sub-vectors.
-   Use IVFPQ (Inverted File with Product Quantization)

## Vector Database

-   We use pgvector, a PostgreSQL extension, to store embeddings (specifically multiple vectors).
-   We use the technique [here](https://github.com/pgvector/pgvector-python/blob/master/examples/colbert/exact.py) to do exact search.
-   For approximate search, we store each vector in a separate row and use the approach described in Section 3.6 of the ColBERT paper.
    -   Instead of applying MaxSim, we can use faiss. At the end of online indexing, we maintain a mapping from each embedding to its document of origin and then index all document embeddings into faiss.
    -   When serving queries, we use two-stage procedure to retrieve the top-k documents from the entire collection.
        -   Approximate stage aimed at filtering
            -   For example:
                Given a query (represented by 10 vectors, denoted by Nq = 10), each vector retrieves 50 similar vectors; then total retrieved documents will be 10 x 50 = 500 document IDs (before duplication). After duplication, we might obtain k unique documents. We have duplicates because each vector of a query can retrieve similar vectors that point to the same documents.
        -   Refinement stage
            -   re-ranking k documents based on more precise scoring mechanisms like ColBERT's scoring.

## Indexing Methods

| IVFFlat                       | HNSW                                               |
| ----------------------------- | -------------------------------------------------- |
| K-means based                 | Graph based                                        |
| Organize vectors into list    | Organize vectors into "neighborhoods"              |
| Requires prepopulated data    | Iterative insertions                               |
| Insert time bounded by #lists | Insertion time increase as data in graph increases |

## Which search method to choose?

-   Exact nearest neighbors (AKA 100% recall): No index
-   Fast indexing: IVFFlat
-   Easy to manage: HNSW
-   High performance/recall: HNSW

## Best practices for pgvector

### Storage strategies

-   TOAST (The Oversized-Attribute Storage Technique) is a mechanism for storing data larger than 8KB
-   By default, PostgreSQL "TOASTs" values over 2KB
-   510-dim 4-byte float vector
-   PostgreSQL Column storage types:
    -   PLAIN: Data stored inline with table
    -   EXTENDED: Data stored/compressed in TOAST table when threshold exceeded (pgvector default)
    -   EXTERNAL: Data stored in TOAST table when threshold exceeded
    -   MAIN: Data stored compressed inline with table
-   Strategies for pgvector and TOAST
    -   Use PLAIN storage
        -   ALTER TABLE ... ALTER COLUMN ... SET STORAGE PLAIN
        -   Requires table rewrite(VACUUM FULL) if data already exists
        -   Limits vector sizes to 2,000 dimensions
    -   Use `min_parallel_table_scan_size` to induce more parallel workers

### HNSW strategies

-   `m`
    -   Maximum number of bidirectional links between indexed vectors
    -   Default: 16
-   `ef_construction`
    -   Number of vectors to maintain in "nearest neighbor" list
    -   Default: 64
-   HNSW query parameters
    -   `hnsw.ef_search`
        -   Number of vectors to maintain in "nearest neighbor" list
        -   Must be greater than or equal to LIMIT
    -   Querying is very quick
-   HNSW does most upfront work in constructing index
-   This increases probability that searches are in best neighborhood for nearest neighbors
-   Best practices for building HNSW indexes
    -   Default values `(M=16, ef_construction=64)` usually work
    -   (pgvector 0.5.1) Start with empty index and use concurrent writes to accelerate builds
        -   `INSERT` or `COPY`
    -   Choosing `m` and `ef_construction`
        -   `m` and `ef_construction` impact relevancy
        -   Before increasing or decreasing `m`, play with `ef_construction` to see if it increases relevancy
        -   As we increase `m`, it impacts recall
    -   Performance strategies for HNSW queries
        -   Index building has biggest impact on performance/recall
        -   Increasing `hnsw.ef_search` increases recall, decreases performance

### IVFFlat strategies

-   IVFFlat index building parameters
    -   `lists`
        -   Number of "buckets" for organizing vectors
        -   Tradeoff between number of vectors in bucket and relevancy
-   Querying an IVFFlat index
    ```
    SET ivfflat.probes TO 1
    SELECT id FROM products ORDER BY $1 <-> embedding LIMIT 3
    ```
-   Performance strategies for IVFFlat queries
    -   Increasing ivfflat.probes increases recall, decreases performance
    -   Lowering `random_page_cost` on a per-query basis can induce index usage
    -   Set shared_buffers to a value that keeps data (table) in memory
    -   Increase `work_mem` on a per-query basis
-   Best practices for building IVFFlat indexes
    -   Choose value of lists to maximize recall but minimize effort of search
        -   < 1MM vectors: # vectors / 1000
        -   \> 1MM vectors: square root of # vectors
    -   May be necessary to rebuild when adding/modifying vectors in index
    -   Use parallelism to accelerate build times
-   How parallelism works with pgvector IVFFlat
    -   Find centers
    -   Assign vectors to centers
    -   Sort vectors
    -   Save to disk

### Filtering

-   How filtering impacts ANN queries

    -   PostgreSQL may choose to not use the index
    -   Uses an index, but does not return enough results
    -   Filering occurs after using the index

-   Do I need an HNSW/IVFFlat index for a filter?
    -   Does the filter use a B-Tree (or other index) to reduce the dataset?
        -   Depends on how many rows you have, e.g., if it's only 100 rows, it will be fast even without a B-Tree. If it's 50k to 100k rows, ANN search will help.
    -   How many rows does the filter remove?
    -   Do I want exact results or approximate results?
        -   If you need exact results, make sure the filter is indexed.
        -   If you need approximate results, apply several strategies:
            -   Use partial index
            -   Use partition
-   Filtering strategies

    -   Partial index
        ```
        CREATE INDEX ON docs
        USING hnsw(embedding vector_l2_ops)
        WHERE category_id = 7;
        ```
    -   Partition

        ```
        CREATE TABLE docs_cat7
        PARTITION OF docs
        FOR VALUES IN (7)

        CREATE INDEX ON docs_cat7
        USING hnsw(embedding vector_l2_ops);
        ```

    -   Filtering with existing embeddings

        ```
        SELECT *
        FROM (
            (SELECT id,
            embedding <=> (SELECT embedding FROM documents WHERE id = 1 LIMIT 1) AS dist
            FROM documents
            ORDER BY dist LIMIT 5)
            UNION
            (SELECT id,
            embedding <=> (SELECT embedding FROM documents WHERE id = 2 LIMIT 1) AS dist
            FROM documents
            ORDER BY dist LIMIT 5)
        ) x
        WHERE x.id NOT IN (1, 2)
        ORDER BY x.dist LIMIT 5;
        ```

### Conclusion

-   Primary design decision: **query performance** and **recall**
-   Determine where to invest: **storage**, **compute**, **indexing strategy**

## Common Issues Dealing With pgvector and Multiple Vectors

-   https://github.com/pgvector/pgvector/issues/640
-   https://github.com/pgvector/pgvector-python/issues/96
-   https://github.com/pgvector/pgvector-python/issues/50
-   https://github.com/pgvector/pgvector-python/issues/4
