This is a wrapper on [colpali](https://github.com/illuin-tech/colpali/tree/main): Code used for training the vision retrievers in the ColPali: Efficient Document Retrieval with Vision Language Models paper

## ViDoRe: The Visual Document Retrieval Benchmark

-   https://huggingface.co/spaces/vidore/vidore-leaderboard

## Relevant Readings

-   [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT
    ](https://arxiv.org/abs/2004.12832)
-   [ColPali: Efficient Document Retrieval with Vision Language Models
    ](https://arxiv.org/abs/2407.01449)
-   [What is ColBERT and Late Interaction and Why They Matter in Search?](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/)
-   [Scaling ColPali to billions of PDFs with Vespa](https://blog.vespa.ai/scaling-colpali-to-billions/)
-   [A notebook utilizing Vespa to store embeddings generated by ColPali](https://pyvespa.readthedocs.io/en/latest/examples/colpali-document-retrieval-vision-language-models-cloud.html)
-   [Scalar and binary quantization for pgvector vector search and storage](https://jkatz05.com/post/postgres/pgvector-scalar-binary-quantization/)
-   [Best practices for querying vector data for gen AI apps in PostgreSQL](https://www.youtube.com/watch?v=PhIC4JlYg7A)
-   [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/)
-   [Why separating compute from storage is a bad idea for late interaction models](https://x.com/jobergum/status/1846945345646821533)
-   [Hugging Face Transformer Inference Under 1 Millisecond Latency](https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c)
-   [Use PgTune to set initial values for Postgres server parameters](https://pgtune.leopard.in.ua/)
-   [How to Use ANALYZE Command in PostgreSQL](https://www.commandprompt.com/education/how-to-use-analyze-command-in-postgresql/)
-   [PostgreSQL performance tips](https://www.postgresql.org/docs/current/populate.html#DISABLE-AUTOCOMMIT)
-   [Understanding pgvector's HNSW Index Storage in Postgres](https://lantern.dev/blog/pgvector-storage)
-   [How to measure execution time in PyTorch](https://discuss.pytorch.org/t/how-to-measure-execution-time-in-pytorch/111458)
-   [Profiling in Python](https://realpython.com/python-profiling/)
-   [Retrieval Evaluations](https://myscale.com/blog/ultimate-guide-to-evaluate-rag-system/)

## Relevant GitHub Repos

-   [byaldi](https://github.com/AnswerDotAI/byaldi): A lightweight wrapper around the `colpali-engine`

## Limitations

Updated: 10/20/2024

1. Currently, we are adding new folder, files, and pages sequentially. We need to bulk insert into multiple tables parallelly.
2. We don't have a mechanism to dynamically detect new folders/files to embed yet. `embed_images()` assumes all PDF images need to be processed. This might be solved through LLM agents.
3. We can only process one directory. We need to be able to process multiple directories simultaneously.
4. When user first searches, the system needs to know whether embeddings already exist in vector DB or the file simply doesn't exist.
    - Have an efficient way of finding the relevant embedding clusters; if they don't exist, know what action to take.

## Improvement

-   Loading the model takes quite some time
-   Extract text from PDFs and use LLM to generate summary for each file
-   Use approximate search
-   Improve memory efficiency by dividing each embedding into sub-vectors.
-   Use IVFPQ (Inverted File with Product Quantization)

## Vector Database

-   We use pgvector, a PostgreSQL extension, to store embeddings (specifically multiple vectors).
-   We use the technique [here](https://github.com/pgvector/pgvector-python/blob/master/examples/colbert/exact.py) to do exact search.
-   For approximate search, we store each vector in a separate row and use the approach described in Section 3.6 of the ColBERT paper.
    -   Instead of applying MaxSim, we can use faiss. At the end of online indexing, we maintain a mapping from each embedding to its document of origin and then index all document embeddings into faiss.
    -   When serving queries, we use two-stage procedure to retrieve the top-k documents from the entire collection.
        -   Approximate stage aimed at filtering
            -   For example:
                Given a query (represented by 10 vectors, denoted by Nq = 10), each vector retrieves 50 similar vectors; then total retrieved documents will be 10 x 50 = 500 document IDs (before duplication). After duplication, we might obtain k unique documents. We have duplicates because each vector of a query can retrieve similar vectors that point to the same documents.
        -   Refinement stage
            -   re-ranking k documents based on more precise scoring mechanisms like ColBERT's scoring.

## Indexing Methods

| IVFFlat                       | HNSW                                                |
| ----------------------------- | --------------------------------------------------- |
| K-means based                 | Graph based                                         |
| Organize vectors into list    | Organize vectors into "neighborhoods"               |
| Requires prepopulated data    | Iterative insertions                                |
| Insert time bounded by #lists | Insertion time increases as data in graph increases |

## Which search method to choose?

-   Exact nearest neighbors (AKA 100% recall): No index
-   Fast indexing: IVFFlat
-   Easy to manage: HNSW
-   High performance/recall: HNSW

## Best practices for pgvector

### Storage strategies

-   TOAST (The Oversized-Attribute Storage Technique) is a mechanism for storing data larger than 8KB
-   By default, PostgreSQL "TOASTs" values over 2KB
-   510-dim 4-byte float vector
-   PostgreSQL Column storage types:
    -   PLAIN: Data stored inline with table
    -   EXTENDED: Data stored/compressed in TOAST table when threshold exceeded (pgvector default)
    -   EXTERNAL: Data stored in TOAST table when threshold exceeded
    -   MAIN: Data stored compressed inline with table
-   Strategies for pgvector and TOAST
    -   Use PLAIN storage
        -   ALTER TABLE ... ALTER COLUMN ... SET STORAGE PLAIN
        -   Requires table rewrite(VACUUM FULL) if data already exists
        -   Limits vector sizes to 2,000 dimensions
    -   Use `min_parallel_table_scan_size` to induce more parallel workers

### HNSW strategies

-   `m`
    -   Maximum number of bidirectional links between indexed vectors
    -   Default: 16
-   `ef_construction`
    -   Number of vectors to maintain in "nearest neighbor" list
    -   Default: 64
-   HNSW query parameters
    -   `hnsw.ef_search`
        -   Number of vectors to maintain in "nearest neighbor" list
        -   Must be greater than or equal to LIMIT
    -   Querying is very quick
-   HNSW does most upfront work in constructing index
-   This increases probability that searches are in best neighborhood for nearest neighbors
-   Best practices for building HNSW indexes
    -   Default values `(M=16, ef_construction=64)` usually work
    -   (pgvector 0.5.1) Start with empty index and use concurrent writes to accelerate builds
        -   `INSERT` or `COPY`
    -   Choosing `m` and `ef_construction`
        -   `m` and `ef_construction` impact relevancy
        -   Before increasing or decreasing `m`, play with `ef_construction` to see if it increases relevancy
        -   As we increase `m`, it impacts recall
    -   Performance strategies for HNSW queries
        -   Index building has biggest impact on performance/recall
        -   Increasing `hnsw.ef_search` increases recall, decreases performance

### IVFFlat strategies

-   IVFFlat index building parameters
    -   `lists`
        -   Number of "buckets" for organizing vectors
        -   Tradeoff between number of vectors in bucket and relevancy
-   Querying an IVFFlat index
    ```
    SET ivfflat.probes TO 1
    SELECT id FROM products ORDER BY $1 <-> embedding LIMIT 3
    ```
-   Performance strategies for IVFFlat queries
    -   Increasing ivfflat.probes increases recall, decreases performance
    -   Lowering `random_page_cost` on a per-query basis can induce index usage
    -   Set shared_buffers to a value that keeps data (table) in memory
    -   Increase `work_mem` on a per-query basis
-   Best practices for building IVFFlat indexes
    -   Choose value of lists to maximize recall but minimize effort of search
        -   < 1MM vectors: # vectors / 1000
        -   \> 1MM vectors: square root of # vectors
    -   May be necessary to rebuild when adding/modifying vectors in index
    -   Use parallelism to accelerate build times
-   How parallelism works with pgvector IVFFlat
    -   Find centers
    -   Assign vectors to centers
    -   Sort vectors
    -   Save to disk

### Filtering

-   How filtering impacts ANN queries

    -   PostgreSQL may choose to not use the index
    -   Uses an index, but does not return enough results
    -   Filering occurs after using the index

-   Do I need an HNSW/IVFFlat index for a filter?
    -   Does the filter use a B-Tree (or other index) to reduce the dataset?
        -   Depends on how many rows you have, e.g., if it's only 100 rows, it will be fast even without a B-Tree. If it's 50k to 100k rows, ANN search will help.
    -   How many rows does the filter remove?
    -   Do I want exact results or approximate results?
        -   If you need exact results, make sure the filter is indexed.
        -   If you need approximate results, apply several strategies:
            -   Use partial index
            -   Use partition
-   Filtering strategies

    -   Partial index
        ```
        CREATE INDEX ON docs
        USING hnsw(embedding vector_l2_ops)
        WHERE category_id = 7;
        ```
    -   Partition

        ```
        CREATE TABLE docs_cat7
        PARTITION OF docs
        FOR VALUES IN (7)

        CREATE INDEX ON docs_cat7
        USING hnsw(embedding vector_l2_ops);
        ```

    -   Filtering with existing embeddings

        ```
        SELECT *
        FROM (
            (SELECT id,
            embedding <=> (SELECT embedding FROM documents WHERE id = 1 LIMIT 1) AS dist
            FROM documents
            ORDER BY dist LIMIT 5)
            UNION
            (SELECT id,
            embedding <=> (SELECT embedding FROM documents WHERE id = 2 LIMIT 1) AS dist
            FROM documents
            ORDER BY dist LIMIT 5)
        ) x
        WHERE x.id NOT IN (1, 2)
        ORDER BY x.dist LIMIT 5;
        ```

### Optimization

Model Loading Optimization

-   [ ] Use model quantization (int8 or fp16) to reduce memory footprint
-   [ ] Implement model weight sharding for distributed reference
-   [ ] Use ONNX Runtime and TensorRT for inference optimization

Database Optimization

-   [x] Optimizing Vector Storage in PostgreSQL with pgvector's Halfvec
-   [x] Update the planner statistics to optimize query performance

Deployment Architecture

-   [ ] Use Kubernetes with horizontal pod autoscaling, aka. HPA
-   [ ] Use [Nvidia Triton inference server](https://github.com/triton-inference-server/server) for model serving

### Conclusion

-   Primary design decision: **query performance** and **recall**
-   Determine where to invest: **storage**, **compute**, **indexing strategy**

## Common Issues Dealing With pgvector and Multiple Vectors

-   https://github.com/pgvector/pgvector/issues/640
-   https://github.com/pgvector/pgvector-python/issues/96
-   https://github.com/pgvector/pgvector-python/issues/50
-   https://github.com/pgvector/pgvector-python/issues/4
